nohup: ignoring input
Using device: cuda
GPU: NVIDIA A40
CUDA Version: 11.8
Starting comprehensive benchmark comparison

Testing with 3 voting methods: borda, plurality, copeland
Testing with 3 encoding types: pairwise, pairwise_per_voter, onehot
Testing with 7 dataset sizes: 1,000, 5,000, 15,000, 50,000, 150,000, 500,000, 1,000,000


################################################################################
# Testing with BORDA voting method
################################################################################

--------------------------------------------------------------------------------
Dataset size: 1,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 3.56s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1392, LR: 0.000905
Step 200/3000, Loss: 0.0324, LR: 0.000655
Step 300/3000, Loss: 0.0159, LR: 0.000346
Step 400/3000, Loss: 0.0150, LR: 0.000096
Step 500/3000, Loss: 0.0082, LR: 0.001000
Step 600/3000, Loss: 0.0073, LR: 0.000976
Step 700/3000, Loss: 0.0155, LR: 0.000905
Step 800/3000, Loss: 0.0098, LR: 0.000794
Step 900/3000, Loss: 0.0060, LR: 0.000655
Step 1000/3000, Loss: 0.0086, LR: 0.000501
Step 1100/3000, Loss: 0.0117, LR: 0.000346
Step 1200/3000, Loss: 0.0088, LR: 0.000207
Step 1300/3000, Loss: 0.0118, LR: 0.000096
Step 1400/3000, Loss: 0.0117, LR: 0.000025
Step 1500/3000, Loss: 0.0116, LR: 0.001000
Step 1600/3000, Loss: 0.0039, LR: 0.000994
Step 1700/3000, Loss: 0.0060, LR: 0.000976
Step 1800/3000, Loss: 0.0171, LR: 0.000946
Step 1900/3000, Loss: 0.0056, LR: 0.000905
Step 2000/3000, Loss: 0.0125, LR: 0.000854
Step 2100/3000, Loss: 0.0118, LR: 0.000794
Step 2200/3000, Loss: 0.0204, LR: 0.000727
Step 2300/3000, Loss: 0.0056, LR: 0.000655
Step 2400/3000, Loss: 0.0113, LR: 0.000579
Step 2500/3000, Loss: 0.0117, LR: 0.000501
Step 2600/3000, Loss: 0.0088, LR: 0.000422
Step 2700/3000, Loss: 0.0116, LR: 0.000346
Step 2800/3000, Loss: 0.0055, LR: 0.000274
Step 2900/3000, Loss: 0.0022, LR: 0.000207
Step 3000/3000, Loss: 0.0113, LR: 0.000147
Training time: 8.35s (2.78ms per step)
Evaluating model...
Hard Accuracy: 0.923
Soft Accuracy: 0.9968
Evaluation time: 0.23s
Checking axiom satisfaction...
4615
Axiom (anonymity) Satisfaction Rate: 0.923
4536
Axiom (neutrality) Satisfaction Rate: 0.9072
3567
Axiom (condorcet) Satisfaction Rate: 0.7134
233
Axiom (pareto) Satisfaction Rate: 0.0466
Measuring inference time...
Average inference time (single sample): 0.2380ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9230
  Soft Accuracy: 0.9968
  Training Time: 8.35s
  Inference Time: 0.2380ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 0.43s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.0186, LR: 0.000905
Step 200/3000, Loss: 0.0051, LR: 0.000655
Step 300/3000, Loss: 0.0028, LR: 0.000346
Step 400/3000, Loss: 0.0024, LR: 0.000096
Step 500/3000, Loss: 0.0024, LR: 0.001000
Step 600/3000, Loss: 0.0012, LR: 0.000976
Step 700/3000, Loss: 0.0005, LR: 0.000905
Step 800/3000, Loss: 0.0004, LR: 0.000794
Step 900/3000, Loss: 0.0003, LR: 0.000655
Step 1000/3000, Loss: 0.0003, LR: 0.000501
Step 1100/3000, Loss: 0.0002, LR: 0.000346
Step 1200/3000, Loss: 0.0002, LR: 0.000207
Step 1300/3000, Loss: 0.0002, LR: 0.000096
Step 1400/3000, Loss: 0.0002, LR: 0.000025
Step 1500/3000, Loss: 0.0002, LR: 0.001000
Step 1600/3000, Loss: 0.0001, LR: 0.000994
Step 1700/3000, Loss: 0.0001, LR: 0.000976
Step 1800/3000, Loss: 0.0001, LR: 0.000946
Step 1900/3000, Loss: 0.0001, LR: 0.000905
Step 2000/3000, Loss: 0.0001, LR: 0.000854
Step 2100/3000, Loss: 0.0000, LR: 0.000794
Step 2200/3000, Loss: 0.0000, LR: 0.000727
Step 2300/3000, Loss: 0.0000, LR: 0.000655
Step 2400/3000, Loss: 0.0000, LR: 0.000579
Step 2500/3000, Loss: 0.0000, LR: 0.000501
Step 2600/3000, Loss: 0.0000, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0000, LR: 0.000274
Step 2900/3000, Loss: 0.0000, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 11.14s (3.71ms per step)
Evaluating model...
Hard Accuracy: 0.732
Soft Accuracy: 0.807
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.732
Axiom (neutrality) Satisfaction Rate: 0.7252
Axiom (condorcet) Satisfaction Rate: 0.724
Axiom (pareto) Satisfaction Rate: 0.0498
Measuring inference time...
Average inference time (single sample): 0.2761ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.7320
  Soft Accuracy: 0.8070
  Training Time: 11.14s
  Inference Time: 0.2761ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 1,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 0.38s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.0664, LR: 0.000905
Step 200/3000, Loss: 0.0053, LR: 0.000655
Step 300/3000, Loss: 0.0037, LR: 0.000346
Step 400/3000, Loss: 0.0009, LR: 0.000096
Step 500/3000, Loss: 0.0033, LR: 0.001000
Step 600/3000, Loss: 0.0005, LR: 0.000976
Step 700/3000, Loss: 0.0001, LR: 0.000905
Step 800/3000, Loss: 0.0001, LR: 0.000794
Step 900/3000, Loss: 0.0001, LR: 0.000655
Step 1000/3000, Loss: 0.0001, LR: 0.000501
Step 1100/3000, Loss: 0.0000, LR: 0.000346
Step 1200/3000, Loss: 0.0000, LR: 0.000207
Step 1300/3000, Loss: 0.0000, LR: 0.000096
Step 1400/3000, Loss: 0.0000, LR: 0.000025
Step 1500/3000, Loss: 0.0000, LR: 0.001000
Step 1600/3000, Loss: 0.0000, LR: 0.000994
Step 1700/3000, Loss: 0.0000, LR: 0.000976
Step 1800/3000, Loss: 0.0000, LR: 0.000946
Step 1900/3000, Loss: 0.0000, LR: 0.000905
Step 2000/3000, Loss: 0.0000, LR: 0.000854
Step 2100/3000, Loss: 0.0000, LR: 0.000794
Step 2200/3000, Loss: 0.0000, LR: 0.000727
Step 2300/3000, Loss: 0.0000, LR: 0.000655
Step 2400/3000, Loss: 0.0000, LR: 0.000579
Step 2500/3000, Loss: 0.0000, LR: 0.000501
Step 2600/3000, Loss: 0.0000, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0000, LR: 0.000274
Step 2900/3000, Loss: 0.0000, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 8.53s (2.84ms per step)
Evaluating model...
Hard Accuracy: 0.592
Soft Accuracy: 0.7786
Evaluation time: 0.22s
Checking axiom satisfaction...
2960
Axiom (anonymity) Satisfaction Rate: 0.592
2909
Axiom (neutrality) Satisfaction Rate: 0.5818
3585
Axiom (condorcet) Satisfaction Rate: 0.717
219
Axiom (pareto) Satisfaction Rate: 0.0438
Measuring inference time...
Average inference time (single sample): 0.2358ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.5920
  Soft Accuracy: 0.7786
  Training Time: 8.53s
  Inference Time: 0.2358ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 5,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 4.09s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1291, LR: 0.000905
Step 200/3000, Loss: 0.0468, LR: 0.000655
Step 300/3000, Loss: 0.0253, LR: 0.000346
Step 400/3000, Loss: 0.0435, LR: 0.000096
Step 500/3000, Loss: 0.0290, LR: 0.001000
Step 600/3000, Loss: 0.0371, LR: 0.000976
Step 700/3000, Loss: 0.0286, LR: 0.000905
Step 800/3000, Loss: 0.0203, LR: 0.000794
Step 900/3000, Loss: 0.0136, LR: 0.000655
Step 1000/3000, Loss: 0.0178, LR: 0.000501
Step 1100/3000, Loss: 0.0096, LR: 0.000346
Step 1200/3000, Loss: 0.0138, LR: 0.000207
Step 1300/3000, Loss: 0.0184, LR: 0.000096
Step 1400/3000, Loss: 0.0184, LR: 0.000025
Step 1500/3000, Loss: 0.0230, LR: 0.001000
Step 1600/3000, Loss: 0.0597, LR: 0.000994
Step 1700/3000, Loss: 0.0166, LR: 0.000976
Step 1800/3000, Loss: 0.0126, LR: 0.000946
Step 1900/3000, Loss: 0.0149, LR: 0.000905
Step 2000/3000, Loss: 0.0142, LR: 0.000854
Step 2100/3000, Loss: 0.0225, LR: 0.000794
Step 2200/3000, Loss: 0.0128, LR: 0.000727
Step 2300/3000, Loss: 0.0123, LR: 0.000655
Step 2400/3000, Loss: 0.0122, LR: 0.000579
Step 2500/3000, Loss: 0.0143, LR: 0.000501
Step 2600/3000, Loss: 0.0135, LR: 0.000422
Step 2700/3000, Loss: 0.0130, LR: 0.000346
Step 2800/3000, Loss: 0.0091, LR: 0.000274
Step 2900/3000, Loss: 0.0117, LR: 0.000207
Step 3000/3000, Loss: 0.0148, LR: 0.000147
Training time: 7.54s (2.51ms per step)
Evaluating model...
Hard Accuracy: 0.9666
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4833
Axiom (anonymity) Satisfaction Rate: 0.9666
4703
Axiom (neutrality) Satisfaction Rate: 0.9406
3551
Axiom (condorcet) Satisfaction Rate: 0.7102
229
Axiom (pareto) Satisfaction Rate: 0.0458
Measuring inference time...
Average inference time (single sample): 0.2338ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9666
  Soft Accuracy: 1.0000
  Training Time: 7.54s
  Inference Time: 0.2338ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 2.12s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.0623, LR: 0.000905
Step 200/3000, Loss: 0.0232, LR: 0.000655
Step 300/3000, Loss: 0.0118, LR: 0.000346
Step 400/3000, Loss: 0.0087, LR: 0.000096
Step 500/3000, Loss: 0.0067, LR: 0.001000
Step 600/3000, Loss: 0.0297, LR: 0.000976
Step 700/3000, Loss: 0.0174, LR: 0.000905
Step 800/3000, Loss: 0.0041, LR: 0.000794
Step 900/3000, Loss: 0.0022, LR: 0.000655
Step 1000/3000, Loss: 0.0008, LR: 0.000501
Step 1100/3000, Loss: 0.0007, LR: 0.000346
Step 1200/3000, Loss: 0.0007, LR: 0.000207
Step 1300/3000, Loss: 0.0006, LR: 0.000096
Step 1400/3000, Loss: 0.0005, LR: 0.000025
Step 1500/3000, Loss: 0.0005, LR: 0.001000
Step 1600/3000, Loss: 0.0004, LR: 0.000994
Step 1700/3000, Loss: 0.0003, LR: 0.000976
Step 1800/3000, Loss: 0.0002, LR: 0.000946
Step 1900/3000, Loss: 0.0002, LR: 0.000905
Step 2000/3000, Loss: 0.0002, LR: 0.000854
Step 2100/3000, Loss: 0.0001, LR: 0.000794
Step 2200/3000, Loss: 0.0001, LR: 0.000727
Step 2300/3000, Loss: 0.0001, LR: 0.000655
Step 2400/3000, Loss: 0.0001, LR: 0.000579
Step 2500/3000, Loss: 0.0001, LR: 0.000501
Step 2600/3000, Loss: 0.0001, LR: 0.000422
Step 2700/3000, Loss: 0.0001, LR: 0.000346
Step 2800/3000, Loss: 0.0001, LR: 0.000274
Step 2900/3000, Loss: 0.0001, LR: 0.000207
Step 3000/3000, Loss: 0.0001, LR: 0.000147
Training time: 8.48s (2.83ms per step)
Evaluating model...
Hard Accuracy: 0.8444
Soft Accuracy: 0.9166
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8444
Axiom (neutrality) Satisfaction Rate: 0.832
Axiom (condorcet) Satisfaction Rate: 0.7214
Axiom (pareto) Satisfaction Rate: 0.0512
Measuring inference time...
Average inference time (single sample): 0.2908ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.8444
  Soft Accuracy: 0.9166
  Training Time: 8.48s
  Inference Time: 0.2908ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 5,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 2.02s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2354, LR: 0.000905
Step 200/3000, Loss: 0.0876, LR: 0.000655
Step 300/3000, Loss: 0.0444, LR: 0.000346
Step 400/3000, Loss: 0.0171, LR: 0.000096
Step 500/3000, Loss: 0.0199, LR: 0.001000
Step 600/3000, Loss: 0.0358, LR: 0.000976
Step 700/3000, Loss: 0.0079, LR: 0.000905
Step 800/3000, Loss: 0.0028, LR: 0.000794
Step 900/3000, Loss: 0.0025, LR: 0.000655
Step 1000/3000, Loss: 0.0012, LR: 0.000501
Step 1100/3000, Loss: 0.0012, LR: 0.000346
Step 1200/3000, Loss: 0.0006, LR: 0.000207
Step 1300/3000, Loss: 0.0020, LR: 0.000096
Step 1400/3000, Loss: 0.0009, LR: 0.000025
Step 1500/3000, Loss: 0.0010, LR: 0.001000
Step 1600/3000, Loss: 0.0011, LR: 0.000994
Step 1700/3000, Loss: 0.0253, LR: 0.000976
Step 1800/3000, Loss: 0.0012, LR: 0.000946
Step 1900/3000, Loss: 0.0006, LR: 0.000905
Step 2000/3000, Loss: 0.0004, LR: 0.000854
Step 2100/3000, Loss: 0.0054, LR: 0.000794
Step 2200/3000, Loss: 0.0004, LR: 0.000727
Step 2300/3000, Loss: 0.0002, LR: 0.000655
Step 2400/3000, Loss: 0.0001, LR: 0.000579
Step 2500/3000, Loss: 0.0001, LR: 0.000501
Step 2600/3000, Loss: 0.0001, LR: 0.000422
Step 2700/3000, Loss: 0.0001, LR: 0.000346
Step 2800/3000, Loss: 0.0040, LR: 0.000274
Step 2900/3000, Loss: 0.0001, LR: 0.000207
Step 3000/3000, Loss: 0.0001, LR: 0.000147
Training time: 10.89s (3.63ms per step)
Evaluating model...
Hard Accuracy: 0.7422
Soft Accuracy: 0.915
Evaluation time: 0.22s
Checking axiom satisfaction...
3711
Axiom (anonymity) Satisfaction Rate: 0.7422
3610
Axiom (neutrality) Satisfaction Rate: 0.722
3560
Axiom (condorcet) Satisfaction Rate: 0.712
242
Axiom (pareto) Satisfaction Rate: 0.0484
Measuring inference time...
Average inference time (single sample): 0.2298ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.7422
  Soft Accuracy: 0.9150
  Training Time: 10.89s
  Inference Time: 0.2298ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 15,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 11.63s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1468, LR: 0.000905
Step 200/3000, Loss: 0.0671, LR: 0.000655
Step 300/3000, Loss: 0.0385, LR: 0.000346
Step 400/3000, Loss: 0.0332, LR: 0.000096
Step 500/3000, Loss: 0.0325, LR: 0.001000
Step 600/3000, Loss: 0.0466, LR: 0.000976
Step 700/3000, Loss: 0.0409, LR: 0.000905
Step 800/3000, Loss: 0.0250, LR: 0.000794
Step 900/3000, Loss: 0.0290, LR: 0.000655
Step 1000/3000, Loss: 0.0110, LR: 0.000501
Step 1100/3000, Loss: 0.0213, LR: 0.000346
Step 1200/3000, Loss: 0.0184, LR: 0.000207
Step 1300/3000, Loss: 0.0165, LR: 0.000096
Step 1400/3000, Loss: 0.0085, LR: 0.000025
Step 1500/3000, Loss: 0.0225, LR: 0.001000
Step 1600/3000, Loss: 0.0434, LR: 0.000994
Step 1700/3000, Loss: 0.0351, LR: 0.000976
Step 1800/3000, Loss: 0.0205, LR: 0.000946
Step 1900/3000, Loss: 0.0159, LR: 0.000905
Step 2000/3000, Loss: 0.0388, LR: 0.000854
Step 2100/3000, Loss: 0.0230, LR: 0.000794
Step 2200/3000, Loss: 0.0215, LR: 0.000727
Step 2300/3000, Loss: 0.0134, LR: 0.000655
Step 2400/3000, Loss: 0.0152, LR: 0.000579
Step 2500/3000, Loss: 0.0130, LR: 0.000501
Step 2600/3000, Loss: 0.0245, LR: 0.000422
Step 2700/3000, Loss: 0.0419, LR: 0.000346
Step 2800/3000, Loss: 0.0091, LR: 0.000274
Step 2900/3000, Loss: 0.0109, LR: 0.000207
Step 3000/3000, Loss: 0.0231, LR: 0.000147
Training time: 7.70s (2.57ms per step)
Evaluating model...
Hard Accuracy: 0.975
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4875
Axiom (anonymity) Satisfaction Rate: 0.975
4774
Axiom (neutrality) Satisfaction Rate: 0.9548
3561
Axiom (condorcet) Satisfaction Rate: 0.7122
254
Axiom (pareto) Satisfaction Rate: 0.0508
Measuring inference time...
Average inference time (single sample): 0.2315ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9750
  Soft Accuracy: 1.0000
  Training Time: 7.70s
  Inference Time: 0.2315ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 6.38s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.0945, LR: 0.000905
Step 200/3000, Loss: 0.0467, LR: 0.000655
Step 300/3000, Loss: 0.0400, LR: 0.000346
Step 400/3000, Loss: 0.0159, LR: 0.000096
Step 500/3000, Loss: 0.0158, LR: 0.001000
Step 600/3000, Loss: 0.0875, LR: 0.000976
Step 700/3000, Loss: 0.0362, LR: 0.000905
Step 800/3000, Loss: 0.0151, LR: 0.000794
Step 900/3000, Loss: 0.0153, LR: 0.000655
Step 1000/3000, Loss: 0.0089, LR: 0.000501
Step 1100/3000, Loss: 0.0050, LR: 0.000346
Step 1200/3000, Loss: 0.0039, LR: 0.000207
Step 1300/3000, Loss: 0.0030, LR: 0.000096
Step 1400/3000, Loss: 0.0023, LR: 0.000025
Step 1500/3000, Loss: 0.0027, LR: 0.001000
Step 1600/3000, Loss: 0.0294, LR: 0.000994
Step 1700/3000, Loss: 0.0268, LR: 0.000976
Step 1800/3000, Loss: 0.0204, LR: 0.000946
Step 1900/3000, Loss: 0.0161, LR: 0.000905
Step 2000/3000, Loss: 0.0162, LR: 0.000854
Step 2100/3000, Loss: 0.0102, LR: 0.000794
Step 2200/3000, Loss: 0.0030, LR: 0.000727
Step 2300/3000, Loss: 0.0217, LR: 0.000655
Step 2400/3000, Loss: 0.0033, LR: 0.000579
Step 2500/3000, Loss: 0.0024, LR: 0.000501
Step 2600/3000, Loss: 0.0006, LR: 0.000422
Step 2700/3000, Loss: 0.0010, LR: 0.000346
Step 2800/3000, Loss: 0.0004, LR: 0.000274
Step 2900/3000, Loss: 0.0004, LR: 0.000207
Step 3000/3000, Loss: 0.0004, LR: 0.000147
Training time: 10.22s (3.41ms per step)
Evaluating model...
Hard Accuracy: 0.9058
Soft Accuracy: 0.9716
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9058
Axiom (neutrality) Satisfaction Rate: 0.8892
Axiom (condorcet) Satisfaction Rate: 0.722
Axiom (pareto) Satisfaction Rate: 0.0422
Measuring inference time...
Average inference time (single sample): 0.2719ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9058
  Soft Accuracy: 0.9716
  Training Time: 10.22s
  Inference Time: 0.2719ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 15,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 5.45s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2814, LR: 0.000905
Step 200/3000, Loss: 0.1492, LR: 0.000655
Step 300/3000, Loss: 0.0881, LR: 0.000346
Step 400/3000, Loss: 0.0611, LR: 0.000096
Step 500/3000, Loss: 0.0737, LR: 0.001000
Step 600/3000, Loss: 0.0679, LR: 0.000976
Step 700/3000, Loss: 0.0401, LR: 0.000905
Step 800/3000, Loss: 0.0259, LR: 0.000794
Step 900/3000, Loss: 0.0146, LR: 0.000655
Step 1000/3000, Loss: 0.0140, LR: 0.000501
Step 1100/3000, Loss: 0.0140, LR: 0.000346
Step 1200/3000, Loss: 0.0054, LR: 0.000207
Step 1300/3000, Loss: 0.0027, LR: 0.000096
Step 1400/3000, Loss: 0.0041, LR: 0.000025
Step 1500/3000, Loss: 0.0052, LR: 0.001000
Step 1600/3000, Loss: 0.0376, LR: 0.000994
Step 1700/3000, Loss: 0.0555, LR: 0.000976
Step 1800/3000, Loss: 0.0405, LR: 0.000946
Step 1900/3000, Loss: 0.0072, LR: 0.000905
Step 2000/3000, Loss: 0.0335, LR: 0.000854
Step 2100/3000, Loss: 0.0063, LR: 0.000794
Step 2200/3000, Loss: 0.0020, LR: 0.000727
Step 2300/3000, Loss: 0.0026, LR: 0.000655
Step 2400/3000, Loss: 0.0006, LR: 0.000579
Step 2500/3000, Loss: 0.0012, LR: 0.000501
Step 2600/3000, Loss: 0.0007, LR: 0.000422
Step 2700/3000, Loss: 0.0004, LR: 0.000346
Step 2800/3000, Loss: 0.0005, LR: 0.000274
Step 2900/3000, Loss: 0.0005, LR: 0.000207
Step 3000/3000, Loss: 0.0003, LR: 0.000147
Training time: 10.49s (3.50ms per step)
Evaluating model...
Hard Accuracy: 0.8302
Soft Accuracy: 0.969
Evaluation time: 0.22s
Checking axiom satisfaction...
4151
Axiom (anonymity) Satisfaction Rate: 0.8302
4036
Axiom (neutrality) Satisfaction Rate: 0.8072
3635
Axiom (condorcet) Satisfaction Rate: 0.727
225
Axiom (pareto) Satisfaction Rate: 0.045
Measuring inference time...
Average inference time (single sample): 0.2320ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.8302
  Soft Accuracy: 0.9690
  Training Time: 10.49s
  Inference Time: 0.2320ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 50,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 39.48s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1312, LR: 0.000905
Step 200/3000, Loss: 0.0320, LR: 0.000655
Step 300/3000, Loss: 0.0347, LR: 0.000346
Step 400/3000, Loss: 0.0362, LR: 0.000096
Step 500/3000, Loss: 0.0407, LR: 0.001000
Step 600/3000, Loss: 0.0213, LR: 0.000976
Step 700/3000, Loss: 0.0354, LR: 0.000905
Step 800/3000, Loss: 0.0312, LR: 0.000794
Step 900/3000, Loss: 0.0323, LR: 0.000655
Step 1000/3000, Loss: 0.0154, LR: 0.000501
Step 1100/3000, Loss: 0.0132, LR: 0.000346
Step 1200/3000, Loss: 0.0226, LR: 0.000207
Step 1300/3000, Loss: 0.0126, LR: 0.000096
Step 1400/3000, Loss: 0.0293, LR: 0.000025
Step 1500/3000, Loss: 0.0192, LR: 0.001000
Step 1600/3000, Loss: 0.0335, LR: 0.000994
Step 1700/3000, Loss: 0.0172, LR: 0.000976
Step 1800/3000, Loss: 0.0257, LR: 0.000946
Step 1900/3000, Loss: 0.0201, LR: 0.000905
Step 2000/3000, Loss: 0.0162, LR: 0.000854
Step 2100/3000, Loss: 0.0149, LR: 0.000794
Step 2200/3000, Loss: 0.0218, LR: 0.000727
Step 2300/3000, Loss: 0.0273, LR: 0.000655
Step 2400/3000, Loss: 0.0134, LR: 0.000579
Step 2500/3000, Loss: 0.0321, LR: 0.000501
Step 2600/3000, Loss: 0.0322, LR: 0.000422
Step 2700/3000, Loss: 0.0148, LR: 0.000346
Step 2800/3000, Loss: 0.0125, LR: 0.000274
Step 2900/3000, Loss: 0.0374, LR: 0.000207
Step 3000/3000, Loss: 0.0077, LR: 0.000147
Training time: 7.57s (2.52ms per step)
Evaluating model...
Hard Accuracy: 0.9766
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4883
Axiom (anonymity) Satisfaction Rate: 0.9766
4760
Axiom (neutrality) Satisfaction Rate: 0.952
3602
Axiom (condorcet) Satisfaction Rate: 0.7204
250
Axiom (pareto) Satisfaction Rate: 0.05
Measuring inference time...
Average inference time (single sample): 0.2304ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9766
  Soft Accuracy: 1.0000
  Training Time: 7.57s
  Inference Time: 0.2304ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 21.67s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1035, LR: 0.000905
Step 200/3000, Loss: 0.0916, LR: 0.000655
Step 300/3000, Loss: 0.0288, LR: 0.000346
Step 400/3000, Loss: 0.0328, LR: 0.000096
Step 500/3000, Loss: 0.0305, LR: 0.001000
Step 600/3000, Loss: 0.0736, LR: 0.000976
Step 700/3000, Loss: 0.0442, LR: 0.000905
Step 800/3000, Loss: 0.0305, LR: 0.000794
Step 900/3000, Loss: 0.0343, LR: 0.000655
Step 1000/3000, Loss: 0.0261, LR: 0.000501
Step 1100/3000, Loss: 0.0188, LR: 0.000346
Step 1200/3000, Loss: 0.0184, LR: 0.000207
Step 1300/3000, Loss: 0.0089, LR: 0.000096
Step 1400/3000, Loss: 0.0100, LR: 0.000025
Step 1500/3000, Loss: 0.0075, LR: 0.001000
Step 1600/3000, Loss: 0.0284, LR: 0.000994
Step 1700/3000, Loss: 0.0561, LR: 0.000976
Step 1800/3000, Loss: 0.0314, LR: 0.000946
Step 1900/3000, Loss: 0.0309, LR: 0.000905
Step 2000/3000, Loss: 0.0210, LR: 0.000854
Step 2100/3000, Loss: 0.0264, LR: 0.000794
Step 2200/3000, Loss: 0.0160, LR: 0.000727
Step 2300/3000, Loss: 0.0240, LR: 0.000655
Step 2400/3000, Loss: 0.0115, LR: 0.000579
Step 2500/3000, Loss: 0.0069, LR: 0.000501
Step 2600/3000, Loss: 0.0178, LR: 0.000422
Step 2700/3000, Loss: 0.0087, LR: 0.000346
Step 2800/3000, Loss: 0.0021, LR: 0.000274
Step 2900/3000, Loss: 0.0057, LR: 0.000207
Step 3000/3000, Loss: 0.0063, LR: 0.000147
Training time: 11.14s (3.71ms per step)
Evaluating model...
Hard Accuracy: 0.9602
Soft Accuracy: 0.9952
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9602
Axiom (neutrality) Satisfaction Rate: 0.9292
Axiom (condorcet) Satisfaction Rate: 0.7208
Axiom (pareto) Satisfaction Rate: 0.0488
Measuring inference time...
Average inference time (single sample): 0.2707ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9602
  Soft Accuracy: 0.9952
  Training Time: 11.14s
  Inference Time: 0.2707ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 50,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 18.24s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2917, LR: 0.000905
Step 200/3000, Loss: 0.1870, LR: 0.000655
Step 300/3000, Loss: 0.1330, LR: 0.000346
Step 400/3000, Loss: 0.0804, LR: 0.000096
Step 500/3000, Loss: 0.1017, LR: 0.001000
Step 600/3000, Loss: 0.1211, LR: 0.000976
Step 700/3000, Loss: 0.0723, LR: 0.000905
Step 800/3000, Loss: 0.0465, LR: 0.000794
Step 900/3000, Loss: 0.0540, LR: 0.000655
Step 1000/3000, Loss: 0.0546, LR: 0.000501
Step 1100/3000, Loss: 0.0228, LR: 0.000346
Step 1200/3000, Loss: 0.0175, LR: 0.000207
Step 1300/3000, Loss: 0.0184, LR: 0.000096
Step 1400/3000, Loss: 0.0098, LR: 0.000025
Step 1500/3000, Loss: 0.0162, LR: 0.001000
Step 1600/3000, Loss: 0.0526, LR: 0.000994
Step 1700/3000, Loss: 0.0274, LR: 0.000976
Step 1800/3000, Loss: 0.0244, LR: 0.000946
Step 1900/3000, Loss: 0.0468, LR: 0.000905
Step 2000/3000, Loss: 0.0262, LR: 0.000854
Step 2100/3000, Loss: 0.0138, LR: 0.000794
Step 2200/3000, Loss: 0.0150, LR: 0.000727
Step 2300/3000, Loss: 0.0110, LR: 0.000655
Step 2400/3000, Loss: 0.0137, LR: 0.000579
Step 2500/3000, Loss: 0.0063, LR: 0.000501
Step 2600/3000, Loss: 0.0077, LR: 0.000422
Step 2700/3000, Loss: 0.0053, LR: 0.000346
Step 2800/3000, Loss: 0.0027, LR: 0.000274
Step 2900/3000, Loss: 0.0032, LR: 0.000207
Step 3000/3000, Loss: 0.0041, LR: 0.000147
Training time: 10.57s (3.52ms per step)
Evaluating model...
Hard Accuracy: 0.9264
Soft Accuracy: 0.9892
Evaluation time: 0.22s
Checking axiom satisfaction...
4632
Axiom (anonymity) Satisfaction Rate: 0.9264
4486
Axiom (neutrality) Satisfaction Rate: 0.8972
3593
Axiom (condorcet) Satisfaction Rate: 0.7186
212
Axiom (pareto) Satisfaction Rate: 0.0424
Measuring inference time...
Average inference time (single sample): 0.2303ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9264
  Soft Accuracy: 0.9892
  Training Time: 10.57s
  Inference Time: 0.2303ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 150,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 118.18s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1150, LR: 0.000905
Step 200/3000, Loss: 0.0608, LR: 0.000655
Step 300/3000, Loss: 0.0309, LR: 0.000346
Step 400/3000, Loss: 0.0450, LR: 0.000096
Step 500/3000, Loss: 0.0247, LR: 0.001000
Step 600/3000, Loss: 0.0313, LR: 0.000976
Step 700/3000, Loss: 0.0313, LR: 0.000905
Step 800/3000, Loss: 0.0379, LR: 0.000794
Step 900/3000, Loss: 0.0347, LR: 0.000655
Step 1000/3000, Loss: 0.0326, LR: 0.000501
Step 1100/3000, Loss: 0.0224, LR: 0.000346
Step 1200/3000, Loss: 0.0276, LR: 0.000207
Step 1300/3000, Loss: 0.0234, LR: 0.000096
Step 1400/3000, Loss: 0.0130, LR: 0.000025
Step 1500/3000, Loss: 0.0120, LR: 0.001000
Step 1600/3000, Loss: 0.0356, LR: 0.000994
Step 1700/3000, Loss: 0.0274, LR: 0.000976
Step 1800/3000, Loss: 0.0184, LR: 0.000946
Step 1900/3000, Loss: 0.0254, LR: 0.000905
Step 2000/3000, Loss: 0.0372, LR: 0.000854
Step 2100/3000, Loss: 0.0134, LR: 0.000794
Step 2200/3000, Loss: 0.0247, LR: 0.000727
Step 2300/3000, Loss: 0.0246, LR: 0.000655
Step 2400/3000, Loss: 0.0180, LR: 0.000579
Step 2500/3000, Loss: 0.0301, LR: 0.000501
Step 2600/3000, Loss: 0.0207, LR: 0.000422
Step 2700/3000, Loss: 0.0169, LR: 0.000346
Step 2800/3000, Loss: 0.0336, LR: 0.000274
Step 2900/3000, Loss: 0.0154, LR: 0.000207
Step 3000/3000, Loss: 0.0149, LR: 0.000147
Training time: 16.27s (5.42ms per step)
Evaluating model...
Hard Accuracy: 0.9772
Soft Accuracy: 1.0
Evaluation time: 1.86s
Checking axiom satisfaction...
4886
Axiom (anonymity) Satisfaction Rate: 0.9772
4759
Axiom (neutrality) Satisfaction Rate: 0.9518
3669
Axiom (condorcet) Satisfaction Rate: 0.7338
240
Axiom (pareto) Satisfaction Rate: 0.048
Measuring inference time...
Average inference time (single sample): 0.2485ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9772
  Soft Accuracy: 1.0000
  Training Time: 16.27s
  Inference Time: 0.2485ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 65.89s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1207, LR: 0.000905
Step 200/3000, Loss: 0.0889, LR: 0.000655
Step 300/3000, Loss: 0.0646, LR: 0.000346
Step 400/3000, Loss: 0.0329, LR: 0.000096
Step 500/3000, Loss: 0.0434, LR: 0.001000
Step 600/3000, Loss: 0.0639, LR: 0.000976
Step 700/3000, Loss: 0.0411, LR: 0.000905
Step 800/3000, Loss: 0.0383, LR: 0.000794
Step 900/3000, Loss: 0.0252, LR: 0.000655
Step 1000/3000, Loss: 0.0298, LR: 0.000501
Step 1100/3000, Loss: 0.0248, LR: 0.000346
Step 1200/3000, Loss: 0.0182, LR: 0.000207
Step 1300/3000, Loss: 0.0136, LR: 0.000096
Step 1400/3000, Loss: 0.0265, LR: 0.000025
Step 1500/3000, Loss: 0.0190, LR: 0.001000
Step 1600/3000, Loss: 0.0437, LR: 0.000994
Step 1700/3000, Loss: 0.0544, LR: 0.000976
Step 1800/3000, Loss: 0.0460, LR: 0.000946
Step 1900/3000, Loss: 0.0268, LR: 0.000905
Step 2000/3000, Loss: 0.0353, LR: 0.000854
Step 2100/3000, Loss: 0.0405, LR: 0.000794
Step 2200/3000, Loss: 0.0227, LR: 0.000727
Step 2300/3000, Loss: 0.0229, LR: 0.000655
Step 2400/3000, Loss: 0.0125, LR: 0.000579
Step 2500/3000, Loss: 0.0196, LR: 0.000501
Step 2600/3000, Loss: 0.0192, LR: 0.000422
Step 2700/3000, Loss: 0.0152, LR: 0.000346
Step 2800/3000, Loss: 0.0109, LR: 0.000274
Step 2900/3000, Loss: 0.0111, LR: 0.000207
Step 3000/3000, Loss: 0.0113, LR: 0.000147
Training time: 13.82s (4.61ms per step)
Evaluating model...
Hard Accuracy: 0.9798
Soft Accuracy: 0.9996
Evaluation time: 2.61s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9798
Axiom (neutrality) Satisfaction Rate: 0.9494
Axiom (condorcet) Satisfaction Rate: 0.7208
Axiom (pareto) Satisfaction Rate: 0.0434
Measuring inference time...
Average inference time (single sample): 0.2712ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9798
  Soft Accuracy: 0.9996
  Training Time: 13.82s
  Inference Time: 0.2712ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 150,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 57.27s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2777, LR: 0.000905
Step 200/3000, Loss: 0.1900, LR: 0.000655
Step 300/3000, Loss: 0.1489, LR: 0.000346
Step 400/3000, Loss: 0.1406, LR: 0.000096
Step 500/3000, Loss: 0.1299, LR: 0.001000
Step 600/3000, Loss: 0.1148, LR: 0.000976
Step 700/3000, Loss: 0.0566, LR: 0.000905
Step 800/3000, Loss: 0.0705, LR: 0.000794
Step 900/3000, Loss: 0.0769, LR: 0.000655
Step 1000/3000, Loss: 0.0557, LR: 0.000501
Step 1100/3000, Loss: 0.0470, LR: 0.000346
Step 1200/3000, Loss: 0.0272, LR: 0.000207
Step 1300/3000, Loss: 0.0272, LR: 0.000096
Step 1400/3000, Loss: 0.0242, LR: 0.000025
Step 1500/3000, Loss: 0.0306, LR: 0.001000
Step 1600/3000, Loss: 0.0396, LR: 0.000994
Step 1700/3000, Loss: 0.0428, LR: 0.000976
Step 1800/3000, Loss: 0.0449, LR: 0.000946
Step 1900/3000, Loss: 0.0747, LR: 0.000905
Step 2000/3000, Loss: 0.0379, LR: 0.000854
Step 2100/3000, Loss: 0.0636, LR: 0.000794
Step 2200/3000, Loss: 0.0440, LR: 0.000727
Step 2300/3000, Loss: 0.0238, LR: 0.000655
Step 2400/3000, Loss: 0.0137, LR: 0.000579
Step 2500/3000, Loss: 0.0210, LR: 0.000501
Step 2600/3000, Loss: 0.0171, LR: 0.000422
Step 2700/3000, Loss: 0.0152, LR: 0.000346
Step 2800/3000, Loss: 0.0108, LR: 0.000274
Step 2900/3000, Loss: 0.0132, LR: 0.000207
Step 3000/3000, Loss: 0.0172, LR: 0.000147
Training time: 14.90s (4.97ms per step)
Evaluating model...
Hard Accuracy: 0.9618
Soft Accuracy: 0.9948
Evaluation time: 10.18s
Checking axiom satisfaction...
4809
Axiom (anonymity) Satisfaction Rate: 0.9618
4670
Axiom (neutrality) Satisfaction Rate: 0.934
3518
Axiom (condorcet) Satisfaction Rate: 0.7036
249
Axiom (pareto) Satisfaction Rate: 0.0498
Measuring inference time...
Average inference time (single sample): 0.3164ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9618
  Soft Accuracy: 0.9948
  Training Time: 14.90s
  Inference Time: 0.3164ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 500,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 422.08s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1063, LR: 0.000905
Step 200/3000, Loss: 0.0596, LR: 0.000655
Step 300/3000, Loss: 0.0349, LR: 0.000346
Step 400/3000, Loss: 0.0257, LR: 0.000096
Step 500/3000, Loss: 0.0485, LR: 0.001000
Step 600/3000, Loss: 0.0353, LR: 0.000976
Step 700/3000, Loss: 0.0355, LR: 0.000905
Step 800/3000, Loss: 0.0136, LR: 0.000794
Step 900/3000, Loss: 0.0269, LR: 0.000655
Step 1000/3000, Loss: 0.0316, LR: 0.000501
Step 1100/3000, Loss: 0.0218, LR: 0.000346
Step 1200/3000, Loss: 0.0120, LR: 0.000207
Step 1300/3000, Loss: 0.0235, LR: 0.000096
Step 1400/3000, Loss: 0.0147, LR: 0.000025
Step 1500/3000, Loss: 0.0229, LR: 0.001000
Step 1600/3000, Loss: 0.0216, LR: 0.000994
Step 1700/3000, Loss: 0.0165, LR: 0.000976
Step 1800/3000, Loss: 0.0163, LR: 0.000946
Step 1900/3000, Loss: 0.0324, LR: 0.000905
Step 2000/3000, Loss: 0.0252, LR: 0.000854
Step 2100/3000, Loss: 0.0168, LR: 0.000794
Step 2200/3000, Loss: 0.0233, LR: 0.000727
Step 2300/3000, Loss: 0.0174, LR: 0.000655
Step 2400/3000, Loss: 0.0196, LR: 0.000579
Step 2500/3000, Loss: 0.0140, LR: 0.000501
Step 2600/3000, Loss: 0.0173, LR: 0.000422
Step 2700/3000, Loss: 0.0147, LR: 0.000346
Step 2800/3000, Loss: 0.0115, LR: 0.000274
Step 2900/3000, Loss: 0.0133, LR: 0.000207
Step 3000/3000, Loss: 0.0154, LR: 0.000147
Training time: 21.65s (7.22ms per step)
Evaluating model...
Hard Accuracy: 0.9782
Soft Accuracy: 1.0
Evaluation time: 11.98s
Checking axiom satisfaction...
4891
Axiom (anonymity) Satisfaction Rate: 0.9782
4760
Axiom (neutrality) Satisfaction Rate: 0.952
3651
Axiom (condorcet) Satisfaction Rate: 0.7302
247
Axiom (pareto) Satisfaction Rate: 0.0494
Measuring inference time...
Average inference time (single sample): 0.2550ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9782
  Soft Accuracy: 1.0000
  Training Time: 21.65s
  Inference Time: 0.2550ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 221.65s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1292, LR: 0.000905
Step 200/3000, Loss: 0.0722, LR: 0.000655
Step 300/3000, Loss: 0.0333, LR: 0.000346
Step 400/3000, Loss: 0.0377, LR: 0.000096
Step 500/3000, Loss: 0.0452, LR: 0.001000
Step 600/3000, Loss: 0.0763, LR: 0.000976
Step 700/3000, Loss: 0.0570, LR: 0.000905
Step 800/3000, Loss: 0.0518, LR: 0.000794
Step 900/3000, Loss: 0.0471, LR: 0.000655
Step 1000/3000, Loss: 0.0294, LR: 0.000501
Step 1100/3000, Loss: 0.0195, LR: 0.000346
Step 1200/3000, Loss: 0.0230, LR: 0.000207
Step 1300/3000, Loss: 0.0208, LR: 0.000096
Step 1400/3000, Loss: 0.0139, LR: 0.000025
Step 1500/3000, Loss: 0.0094, LR: 0.001000
Step 1600/3000, Loss: 0.0508, LR: 0.000994
Step 1700/3000, Loss: 0.0281, LR: 0.000976
Step 1800/3000, Loss: 0.0391, LR: 0.000946
Step 1900/3000, Loss: 0.0395, LR: 0.000905
Step 2000/3000, Loss: 0.0276, LR: 0.000854
Step 2100/3000, Loss: 0.0378, LR: 0.000794
Step 2200/3000, Loss: 0.0215, LR: 0.000727
Step 2300/3000, Loss: 0.0303, LR: 0.000655
Step 2400/3000, Loss: 0.0208, LR: 0.000579
Step 2500/3000, Loss: 0.0256, LR: 0.000501
Step 2600/3000, Loss: 0.0212, LR: 0.000422
Step 2700/3000, Loss: 0.0119, LR: 0.000346
Step 2800/3000, Loss: 0.0100, LR: 0.000274
Step 2900/3000, Loss: 0.0061, LR: 0.000207
Step 3000/3000, Loss: 0.0092, LR: 0.000147
Training time: 12.03s (4.01ms per step)
Evaluating model...
Hard Accuracy: 0.9856
Soft Accuracy: 0.9994
Evaluation time: 4.58s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9856
Axiom (neutrality) Satisfaction Rate: 0.9554
Axiom (condorcet) Satisfaction Rate: 0.7176
Axiom (pareto) Satisfaction Rate: 0.045
Measuring inference time...
Average inference time (single sample): 0.2663ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9856
  Soft Accuracy: 0.9994
  Training Time: 12.03s
  Inference Time: 0.2663ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 500,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 194.01s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2311, LR: 0.000905
Step 200/3000, Loss: 0.1472, LR: 0.000655
Step 300/3000, Loss: 0.1514, LR: 0.000346
Step 400/3000, Loss: 0.0749, LR: 0.000096
Step 500/3000, Loss: 0.0854, LR: 0.001000
Step 600/3000, Loss: 0.0912, LR: 0.000976
Step 700/3000, Loss: 0.1018, LR: 0.000905
Step 800/3000, Loss: 0.0598, LR: 0.000794
Step 900/3000, Loss: 0.0643, LR: 0.000655
Step 1000/3000, Loss: 0.0586, LR: 0.000501
Step 1100/3000, Loss: 0.0335, LR: 0.000346
Step 1200/3000, Loss: 0.0437, LR: 0.000207
Step 1300/3000, Loss: 0.0331, LR: 0.000096
Step 1400/3000, Loss: 0.0297, LR: 0.000025
Step 1500/3000, Loss: 0.0287, LR: 0.001000
Step 1600/3000, Loss: 0.0531, LR: 0.000994
Step 1700/3000, Loss: 0.0519, LR: 0.000976
Step 1800/3000, Loss: 0.0438, LR: 0.000946
Step 1900/3000, Loss: 0.0522, LR: 0.000905
Step 2000/3000, Loss: 0.0565, LR: 0.000854
Step 2100/3000, Loss: 0.0292, LR: 0.000794
Step 2200/3000, Loss: 0.0366, LR: 0.000727
Step 2300/3000, Loss: 0.0409, LR: 0.000655
Step 2400/3000, Loss: 0.0216, LR: 0.000579
Step 2500/3000, Loss: 0.0299, LR: 0.000501
Step 2600/3000, Loss: 0.0118, LR: 0.000422
Step 2700/3000, Loss: 0.0227, LR: 0.000346
Step 2800/3000, Loss: 0.0234, LR: 0.000274
Step 2900/3000, Loss: 0.0149, LR: 0.000207
Step 3000/3000, Loss: 0.0189, LR: 0.000147
Training time: 11.73s (3.91ms per step)
Evaluating model...
Hard Accuracy: 0.9698
Soft Accuracy: 0.9962
Evaluation time: 3.06s
Checking axiom satisfaction...
4849
Axiom (anonymity) Satisfaction Rate: 0.9698
4716
Axiom (neutrality) Satisfaction Rate: 0.9432
3634
Axiom (condorcet) Satisfaction Rate: 0.7268
263
Axiom (pareto) Satisfaction Rate: 0.0526
Measuring inference time...
Average inference time (single sample): 0.2360ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9698
  Soft Accuracy: 0.9962
  Training Time: 11.73s
  Inference Time: 0.2360ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 1,000,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 810.23s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1235, LR: 0.000905
Step 200/3000, Loss: 0.0459, LR: 0.000655
Step 300/3000, Loss: 0.0297, LR: 0.000346
Step 400/3000, Loss: 0.0362, LR: 0.000096
Step 500/3000, Loss: 0.0413, LR: 0.001000
Step 600/3000, Loss: 0.0570, LR: 0.000976
Step 700/3000, Loss: 0.0293, LR: 0.000905
Step 800/3000, Loss: 0.0175, LR: 0.000794
Step 900/3000, Loss: 0.0329, LR: 0.000655
Step 1000/3000, Loss: 0.0195, LR: 0.000501
Step 1100/3000, Loss: 0.0233, LR: 0.000346
Step 1200/3000, Loss: 0.0074, LR: 0.000207
Step 1300/3000, Loss: 0.0144, LR: 0.000096
Step 1400/3000, Loss: 0.0128, LR: 0.000025
Step 1500/3000, Loss: 0.0197, LR: 0.001000
Step 1600/3000, Loss: 0.0265, LR: 0.000994
Step 1700/3000, Loss: 0.0163, LR: 0.000976
Step 1800/3000, Loss: 0.0252, LR: 0.000946
Step 1900/3000, Loss: 0.0105, LR: 0.000905
Step 2000/3000, Loss: 0.0196, LR: 0.000854
Step 2100/3000, Loss: 0.0227, LR: 0.000794
Step 2200/3000, Loss: 0.0137, LR: 0.000727
Step 2300/3000, Loss: 0.0268, LR: 0.000655
Step 2400/3000, Loss: 0.0101, LR: 0.000579
Step 2500/3000, Loss: 0.0137, LR: 0.000501
Step 2600/3000, Loss: 0.0163, LR: 0.000422
Step 2700/3000, Loss: 0.0190, LR: 0.000346
Step 2800/3000, Loss: 0.0418, LR: 0.000274
Step 2900/3000, Loss: 0.0233, LR: 0.000207
Step 3000/3000, Loss: 0.0156, LR: 0.000147
Training time: 7.84s (2.61ms per step)
Evaluating model...
Hard Accuracy: 0.9806
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4903
Axiom (anonymity) Satisfaction Rate: 0.9806
4788
Axiom (neutrality) Satisfaction Rate: 0.9576
3585
Axiom (condorcet) Satisfaction Rate: 0.717
216
Axiom (pareto) Satisfaction Rate: 0.0432
Measuring inference time...
Average inference time (single sample): 0.2376ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9806
  Soft Accuracy: 1.0000
  Training Time: 7.84s
  Inference Time: 0.2376ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 447.08s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1297, LR: 0.000905
Step 200/3000, Loss: 0.0604, LR: 0.000655
Step 300/3000, Loss: 0.0412, LR: 0.000346
Step 400/3000, Loss: 0.0376, LR: 0.000096
Step 500/3000, Loss: 0.0230, LR: 0.001000
Step 600/3000, Loss: 0.0443, LR: 0.000976
Step 700/3000, Loss: 0.0735, LR: 0.000905
Step 800/3000, Loss: 0.0502, LR: 0.000794
Step 900/3000, Loss: 0.0474, LR: 0.000655
Step 1000/3000, Loss: 0.0406, LR: 0.000501
Step 1100/3000, Loss: 0.0288, LR: 0.000346
Step 1200/3000, Loss: 0.0239, LR: 0.000207
Step 1300/3000, Loss: 0.0209, LR: 0.000096
Step 1400/3000, Loss: 0.0098, LR: 0.000025
Step 1500/3000, Loss: 0.0171, LR: 0.001000
Step 1600/3000, Loss: 0.0473, LR: 0.000994
Step 1700/3000, Loss: 0.0350, LR: 0.000976
Step 1800/3000, Loss: 0.0510, LR: 0.000946
Step 1900/3000, Loss: 0.0326, LR: 0.000905
Step 2000/3000, Loss: 0.0325, LR: 0.000854
Step 2100/3000, Loss: 0.0288, LR: 0.000794
Step 2200/3000, Loss: 0.0317, LR: 0.000727
Step 2300/3000, Loss: 0.0349, LR: 0.000655
Step 2400/3000, Loss: 0.0245, LR: 0.000579
Step 2500/3000, Loss: 0.0116, LR: 0.000501
Step 2600/3000, Loss: 0.0169, LR: 0.000422
Step 2700/3000, Loss: 0.0201, LR: 0.000346
Step 2800/3000, Loss: 0.0162, LR: 0.000274
Step 2900/3000, Loss: 0.0094, LR: 0.000207
Step 3000/3000, Loss: 0.0067, LR: 0.000147
Training time: 11.31s (3.77ms per step)
Evaluating model...
Hard Accuracy: 0.9852
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9852
Axiom (neutrality) Satisfaction Rate: 0.9534
Axiom (condorcet) Satisfaction Rate: 0.7194
Axiom (pareto) Satisfaction Rate: 0.049
Measuring inference time...
Average inference time (single sample): 0.2676ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9852
  Soft Accuracy: 1.0000
  Training Time: 11.31s
  Inference Time: 0.2676ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 1,000,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 379.90s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2320, LR: 0.000905
Step 200/3000, Loss: 0.1838, LR: 0.000655
Step 300/3000, Loss: 0.1170, LR: 0.000346
Step 400/3000, Loss: 0.1034, LR: 0.000096
Step 500/3000, Loss: 0.0907, LR: 0.001000
Step 600/3000, Loss: 0.1054, LR: 0.000976
Step 700/3000, Loss: 0.1056, LR: 0.000905
Step 800/3000, Loss: 0.0910, LR: 0.000794
Step 900/3000, Loss: 0.0686, LR: 0.000655
Step 1000/3000, Loss: 0.0554, LR: 0.000501
Step 1100/3000, Loss: 0.0511, LR: 0.000346
Step 1200/3000, Loss: 0.0534, LR: 0.000207
Step 1300/3000, Loss: 0.0323, LR: 0.000096
Step 1400/3000, Loss: 0.0268, LR: 0.000025
Step 1500/3000, Loss: 0.0398, LR: 0.001000
Step 1600/3000, Loss: 0.0787, LR: 0.000994
Step 1700/3000, Loss: 0.0659, LR: 0.000976
Step 1800/3000, Loss: 0.0386, LR: 0.000946
Step 1900/3000, Loss: 0.0324, LR: 0.000905
Step 2000/3000, Loss: 0.0324, LR: 0.000854
Step 2100/3000, Loss: 0.0515, LR: 0.000794
Step 2200/3000, Loss: 0.0345, LR: 0.000727
Step 2300/3000, Loss: 0.0395, LR: 0.000655
Step 2400/3000, Loss: 0.0317, LR: 0.000579
Step 2500/3000, Loss: 0.0249, LR: 0.000501
Step 2600/3000, Loss: 0.0246, LR: 0.000422
Step 2700/3000, Loss: 0.0240, LR: 0.000346
Step 2800/3000, Loss: 0.0177, LR: 0.000274
Step 2900/3000, Loss: 0.0208, LR: 0.000207
Step 3000/3000, Loss: 0.0192, LR: 0.000147
Training time: 11.01s (3.67ms per step)
Evaluating model...
Hard Accuracy: 0.9734
Soft Accuracy: 0.9968
Evaluation time: 0.22s
Checking axiom satisfaction...
4867
Axiom (anonymity) Satisfaction Rate: 0.9734
4707
Axiom (neutrality) Satisfaction Rate: 0.9414
3548
Axiom (condorcet) Satisfaction Rate: 0.7096
233
Axiom (pareto) Satisfaction Rate: 0.0466
Measuring inference time...
Average inference time (single sample): 0.2311ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9734
  Soft Accuracy: 0.9968
  Training Time: 11.01s
  Inference Time: 0.2311ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

################################################################################
# Testing with PLURALITY voting method
################################################################################

--------------------------------------------------------------------------------
Dataset size: 1,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 1.13s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1880, LR: 0.000905
Step 200/3000, Loss: 0.2228, LR: 0.000655
Step 300/3000, Loss: 0.1770, LR: 0.000346
Step 400/3000, Loss: 0.1751, LR: 0.000096
Step 500/3000, Loss: 0.1641, LR: 0.001000
Step 600/3000, Loss: 0.1773, LR: 0.000976
Step 700/3000, Loss: 0.1142, LR: 0.000905
Step 800/3000, Loss: 0.1130, LR: 0.000794
Step 900/3000, Loss: 0.0678, LR: 0.000655
Step 1000/3000, Loss: 0.0678, LR: 0.000501
Step 1100/3000, Loss: 0.0568, LR: 0.000346
Step 1200/3000, Loss: 0.0585, LR: 0.000207
Step 1300/3000, Loss: 0.0467, LR: 0.000096
Step 1400/3000, Loss: 0.0417, LR: 0.000025
Step 1500/3000, Loss: 0.0437, LR: 0.001000
Step 1600/3000, Loss: 0.0646, LR: 0.000994
Step 1700/3000, Loss: 0.0527, LR: 0.000976
Step 1800/3000, Loss: 0.0451, LR: 0.000946
Step 1900/3000, Loss: 0.0282, LR: 0.000905
Step 2000/3000, Loss: 0.0462, LR: 0.000854
Step 2100/3000, Loss: 0.0225, LR: 0.000794
Step 2200/3000, Loss: 0.0397, LR: 0.000727
Step 2300/3000, Loss: 0.0217, LR: 0.000655
Step 2400/3000, Loss: 0.0320, LR: 0.000579
Step 2500/3000, Loss: 0.0296, LR: 0.000501
Step 2600/3000, Loss: 0.0262, LR: 0.000422
Step 2700/3000, Loss: 0.0242, LR: 0.000346
Step 2800/3000, Loss: 0.0213, LR: 0.000274
Step 2900/3000, Loss: 0.0176, LR: 0.000207
Step 3000/3000, Loss: 0.0258, LR: 0.000147
Training time: 7.63s (2.54ms per step)
Evaluating model...
Hard Accuracy: 0.6668
Soft Accuracy: 0.8398
Evaluation time: 0.22s
Checking axiom satisfaction...
3334
Axiom (anonymity) Satisfaction Rate: 0.6668
3301
Axiom (neutrality) Satisfaction Rate: 0.6602
2703
Axiom (condorcet) Satisfaction Rate: 0.5406
238
Axiom (pareto) Satisfaction Rate: 0.0476
Measuring inference time...
Average inference time (single sample): 0.2332ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.6668
  Soft Accuracy: 0.8398
  Training Time: 7.63s
  Inference Time: 0.2332ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 0.43s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.0279, LR: 0.000905
Step 200/3000, Loss: 0.0065, LR: 0.000655
Step 300/3000, Loss: 0.0039, LR: 0.000346
Step 400/3000, Loss: 0.0032, LR: 0.000096
Step 500/3000, Loss: 0.0031, LR: 0.001000
Step 600/3000, Loss: 0.0013, LR: 0.000976
Step 700/3000, Loss: 0.0008, LR: 0.000905
Step 800/3000, Loss: 0.0005, LR: 0.000794
Step 900/3000, Loss: 0.0004, LR: 0.000655
Step 1000/3000, Loss: 0.0003, LR: 0.000501
Step 1100/3000, Loss: 0.0002, LR: 0.000346
Step 1200/3000, Loss: 0.0002, LR: 0.000207
Step 1300/3000, Loss: 0.0003, LR: 0.000096
Step 1400/3000, Loss: 0.0002, LR: 0.000025
Step 1500/3000, Loss: 0.0002, LR: 0.001000
Step 1600/3000, Loss: 0.0002, LR: 0.000994
Step 1700/3000, Loss: 0.0002, LR: 0.000976
Step 1800/3000, Loss: 0.0001, LR: 0.000946
Step 1900/3000, Loss: 0.0001, LR: 0.000905
Step 2000/3000, Loss: 0.0001, LR: 0.000854
Step 2100/3000, Loss: 0.0000, LR: 0.000794
Step 2200/3000, Loss: 0.0001, LR: 0.000727
Step 2300/3000, Loss: 0.0000, LR: 0.000655
Step 2400/3000, Loss: 0.0000, LR: 0.000579
Step 2500/3000, Loss: 0.0000, LR: 0.000501
Step 2600/3000, Loss: 0.0000, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0000, LR: 0.000274
Step 2900/3000, Loss: 0.0000, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 8.80s (2.93ms per step)
Evaluating model...
Hard Accuracy: 0.6066
Soft Accuracy: 0.7516
Evaluation time: 0.24s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.6066
Axiom (neutrality) Satisfaction Rate: 0.601
Axiom (condorcet) Satisfaction Rate: 0.5302
Axiom (pareto) Satisfaction Rate: 0.0534
Measuring inference time...
Average inference time (single sample): 0.2679ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.6066
  Soft Accuracy: 0.7516
  Training Time: 8.80s
  Inference Time: 0.2679ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 1,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 0.35s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.0934, LR: 0.000905
Step 200/3000, Loss: 0.0123, LR: 0.000655
Step 300/3000, Loss: 0.0049, LR: 0.000346
Step 400/3000, Loss: 0.0030, LR: 0.000096
Step 500/3000, Loss: 0.0014, LR: 0.001000
Step 600/3000, Loss: 0.0054, LR: 0.000976
Step 700/3000, Loss: 0.0006, LR: 0.000905
Step 800/3000, Loss: 0.0002, LR: 0.000794
Step 900/3000, Loss: 0.0001, LR: 0.000655
Step 1000/3000, Loss: 0.0001, LR: 0.000501
Step 1100/3000, Loss: 0.0001, LR: 0.000346
Step 1200/3000, Loss: 0.0000, LR: 0.000207
Step 1300/3000, Loss: 0.0000, LR: 0.000096
Step 1400/3000, Loss: 0.0000, LR: 0.000025
Step 1500/3000, Loss: 0.0001, LR: 0.001000
Step 1600/3000, Loss: 0.0000, LR: 0.000994
Step 1700/3000, Loss: 0.0000, LR: 0.000976
Step 1800/3000, Loss: 0.0000, LR: 0.000946
Step 1900/3000, Loss: 0.0000, LR: 0.000905
Step 2000/3000, Loss: 0.0000, LR: 0.000854
Step 2100/3000, Loss: 0.0000, LR: 0.000794
Step 2200/3000, Loss: 0.0000, LR: 0.000727
Step 2300/3000, Loss: 0.0000, LR: 0.000655
Step 2400/3000, Loss: 0.0000, LR: 0.000579
Step 2500/3000, Loss: 0.0000, LR: 0.000501
Step 2600/3000, Loss: 0.0000, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0000, LR: 0.000274
Step 2900/3000, Loss: 0.0000, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 10.06s (3.35ms per step)
Evaluating model...
Hard Accuracy: 0.5936
Soft Accuracy: 0.8866
Evaluation time: 0.23s
Checking axiom satisfaction...
2968
Axiom (anonymity) Satisfaction Rate: 0.5936
2864
Axiom (neutrality) Satisfaction Rate: 0.5728
2827
Axiom (condorcet) Satisfaction Rate: 0.5654
212
Axiom (pareto) Satisfaction Rate: 0.0424
Measuring inference time...
Average inference time (single sample): 0.2330ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.5936
  Soft Accuracy: 0.8866
  Training Time: 10.06s
  Inference Time: 0.2330ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 5,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 3.83s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.2404, LR: 0.000905
Step 200/3000, Loss: 0.2444, LR: 0.000655
Step 300/3000, Loss: 0.2022, LR: 0.000346
Step 400/3000, Loss: 0.1996, LR: 0.000096
Step 500/3000, Loss: 0.1805, LR: 0.001000
Step 600/3000, Loss: 0.2319, LR: 0.000976
Step 700/3000, Loss: 0.1993, LR: 0.000905
Step 800/3000, Loss: 0.1822, LR: 0.000794
Step 900/3000, Loss: 0.1842, LR: 0.000655
Step 1000/3000, Loss: 0.1873, LR: 0.000501
Step 1100/3000, Loss: 0.1916, LR: 0.000346
Step 1200/3000, Loss: 0.1819, LR: 0.000207
Step 1300/3000, Loss: 0.1876, LR: 0.000096
Step 1400/3000, Loss: 0.1960, LR: 0.000025
Step 1500/3000, Loss: 0.1566, LR: 0.001000
Step 1600/3000, Loss: 0.1972, LR: 0.000994
Step 1700/3000, Loss: 0.2195, LR: 0.000976
Step 1800/3000, Loss: 0.1315, LR: 0.000946
Step 1900/3000, Loss: 0.1608, LR: 0.000905
Step 2000/3000, Loss: 0.1483, LR: 0.000854
Step 2100/3000, Loss: 0.1614, LR: 0.000794
Step 2200/3000, Loss: 0.1427, LR: 0.000727
Step 2300/3000, Loss: 0.1251, LR: 0.000655
Step 2400/3000, Loss: 0.1173, LR: 0.000579
Step 2500/3000, Loss: 0.1258, LR: 0.000501
Step 2600/3000, Loss: 0.1388, LR: 0.000422
Step 2700/3000, Loss: 0.1042, LR: 0.000346
Step 2800/3000, Loss: 0.1289, LR: 0.000274
Step 2900/3000, Loss: 0.1370, LR: 0.000207
Step 3000/3000, Loss: 0.1068, LR: 0.000147
Training time: 7.53s (2.51ms per step)
Evaluating model...
Hard Accuracy: 0.6944
Soft Accuracy: 0.8438
Evaluation time: 0.22s
Checking axiom satisfaction...
3472
Axiom (anonymity) Satisfaction Rate: 0.6944
3447
Axiom (neutrality) Satisfaction Rate: 0.6894
2684
Axiom (condorcet) Satisfaction Rate: 0.5368
208
Axiom (pareto) Satisfaction Rate: 0.0416
Measuring inference time...
Average inference time (single sample): 0.2382ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.6944
  Soft Accuracy: 0.8438
  Training Time: 7.53s
  Inference Time: 0.2382ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 2.11s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1215, LR: 0.000905
Step 200/3000, Loss: 0.0701, LR: 0.000655
Step 300/3000, Loss: 0.0220, LR: 0.000346
Step 400/3000, Loss: 0.0114, LR: 0.000096
Step 500/3000, Loss: 0.0104, LR: 0.001000
Step 600/3000, Loss: 0.0639, LR: 0.000976
Step 700/3000, Loss: 0.0081, LR: 0.000905
Step 800/3000, Loss: 0.0049, LR: 0.000794
Step 900/3000, Loss: 0.0022, LR: 0.000655
Step 1000/3000, Loss: 0.0012, LR: 0.000501
Step 1100/3000, Loss: 0.0009, LR: 0.000346
Step 1200/3000, Loss: 0.0010, LR: 0.000207
Step 1300/3000, Loss: 0.0009, LR: 0.000096
Step 1400/3000, Loss: 0.0008, LR: 0.000025
Step 1500/3000, Loss: 0.0007, LR: 0.001000
Step 1600/3000, Loss: 0.0007, LR: 0.000994
Step 1700/3000, Loss: 0.0322, LR: 0.000976
Step 1800/3000, Loss: 0.0134, LR: 0.000946
Step 1900/3000, Loss: 0.0009, LR: 0.000905
Step 2000/3000, Loss: 0.0007, LR: 0.000854
Step 2100/3000, Loss: 0.0004, LR: 0.000794
Step 2200/3000, Loss: 0.0003, LR: 0.000727
Step 2300/3000, Loss: 0.0003, LR: 0.000655
Step 2400/3000, Loss: 0.0002, LR: 0.000579
Step 2500/3000, Loss: 0.0001, LR: 0.000501
Step 2600/3000, Loss: 0.0002, LR: 0.000422
Step 2700/3000, Loss: 0.0002, LR: 0.000346
Step 2800/3000, Loss: 0.0002, LR: 0.000274
Step 2900/3000, Loss: 0.0002, LR: 0.000207
Step 3000/3000, Loss: 0.0001, LR: 0.000147
Training time: 8.43s (2.81ms per step)
Evaluating model...
Hard Accuracy: 0.6708
Soft Accuracy: 0.818
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.6708
Axiom (neutrality) Satisfaction Rate: 0.6562
Axiom (condorcet) Satisfaction Rate: 0.5566
Axiom (pareto) Satisfaction Rate: 0.0472
Measuring inference time...
Average inference time (single sample): 0.2707ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.6708
  Soft Accuracy: 0.8180
  Training Time: 8.43s
  Inference Time: 0.2707ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 5,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 1.74s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2290, LR: 0.000905
Step 200/3000, Loss: 0.1095, LR: 0.000655
Step 300/3000, Loss: 0.0364, LR: 0.000346
Step 400/3000, Loss: 0.0248, LR: 0.000096
Step 500/3000, Loss: 0.0185, LR: 0.001000
Step 600/3000, Loss: 0.0205, LR: 0.000976
Step 700/3000, Loss: 0.0052, LR: 0.000905
Step 800/3000, Loss: 0.0027, LR: 0.000794
Step 900/3000, Loss: 0.0006, LR: 0.000655
Step 1000/3000, Loss: 0.0002, LR: 0.000501
Step 1100/3000, Loss: 0.0003, LR: 0.000346
Step 1200/3000, Loss: 0.0002, LR: 0.000207
Step 1300/3000, Loss: 0.0002, LR: 0.000096
Step 1400/3000, Loss: 0.0002, LR: 0.000025
Step 1500/3000, Loss: 0.0005, LR: 0.001000
Step 1600/3000, Loss: 0.0001, LR: 0.000994
Step 1700/3000, Loss: 0.0005, LR: 0.000976
Step 1800/3000, Loss: 0.0374, LR: 0.000946
Step 1900/3000, Loss: 0.0041, LR: 0.000905
Step 2000/3000, Loss: 0.0045, LR: 0.000854
Step 2100/3000, Loss: 0.0001, LR: 0.000794
Step 2200/3000, Loss: 0.0001, LR: 0.000727
Step 2300/3000, Loss: 0.0000, LR: 0.000655
Step 2400/3000, Loss: 0.0001, LR: 0.000579
Step 2500/3000, Loss: 0.0001, LR: 0.000501
Step 2600/3000, Loss: 0.0001, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0001, LR: 0.000274
Step 2900/3000, Loss: 0.0001, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 9.06s (3.02ms per step)
Evaluating model...
Hard Accuracy: 0.7724
Soft Accuracy: 0.963
Evaluation time: 0.22s
Checking axiom satisfaction...
3862
Axiom (anonymity) Satisfaction Rate: 0.7724
3653
Axiom (neutrality) Satisfaction Rate: 0.7306
2726
Axiom (condorcet) Satisfaction Rate: 0.5452
211
Axiom (pareto) Satisfaction Rate: 0.0422
Measuring inference time...
Average inference time (single sample): 0.2320ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.7724
  Soft Accuracy: 0.9630
  Training Time: 9.06s
  Inference Time: 0.2320ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 15,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 11.61s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.2626, LR: 0.000905
Step 200/3000, Loss: 0.2608, LR: 0.000655
Step 300/3000, Loss: 0.2242, LR: 0.000346
Step 400/3000, Loss: 0.2071, LR: 0.000096
Step 500/3000, Loss: 0.1792, LR: 0.001000
Step 600/3000, Loss: 0.2139, LR: 0.000976
Step 700/3000, Loss: 0.2094, LR: 0.000905
Step 800/3000, Loss: 0.1804, LR: 0.000794
Step 900/3000, Loss: 0.1731, LR: 0.000655
Step 1000/3000, Loss: 0.2048, LR: 0.000501
Step 1100/3000, Loss: 0.1836, LR: 0.000346
Step 1200/3000, Loss: 0.2345, LR: 0.000207
Step 1300/3000, Loss: 0.2114, LR: 0.000096
Step 1400/3000, Loss: 0.1790, LR: 0.000025
Step 1500/3000, Loss: 0.1798, LR: 0.001000
Step 1600/3000, Loss: 0.1621, LR: 0.000994
Step 1700/3000, Loss: 0.1813, LR: 0.000976
Step 1800/3000, Loss: 0.2647, LR: 0.000946
Step 1900/3000, Loss: 0.1837, LR: 0.000905
Step 2000/3000, Loss: 0.1894, LR: 0.000854
Step 2100/3000, Loss: 0.1526, LR: 0.000794
Step 2200/3000, Loss: 0.2081, LR: 0.000727
Step 2300/3000, Loss: 0.1802, LR: 0.000655
Step 2400/3000, Loss: 0.1831, LR: 0.000579
Step 2500/3000, Loss: 0.1783, LR: 0.000501
Step 2600/3000, Loss: 0.2110, LR: 0.000422
Step 2700/3000, Loss: 0.1758, LR: 0.000346
Step 2800/3000, Loss: 0.1614, LR: 0.000274
Step 2900/3000, Loss: 0.1974, LR: 0.000207
Step 3000/3000, Loss: 0.1914, LR: 0.000147
Training time: 7.50s (2.50ms per step)
Evaluating model...
Hard Accuracy: 0.7226
Soft Accuracy: 0.8628
Evaluation time: 0.22s
Checking axiom satisfaction...
3613
Axiom (anonymity) Satisfaction Rate: 0.7226
3569
Axiom (neutrality) Satisfaction Rate: 0.7138
2659
Axiom (condorcet) Satisfaction Rate: 0.5318
242
Axiom (pareto) Satisfaction Rate: 0.0484
Measuring inference time...
Average inference time (single sample): 0.2337ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7226
  Soft Accuracy: 0.8628
  Training Time: 7.50s
  Inference Time: 0.2337ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 6.32s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1818, LR: 0.000905
Step 200/3000, Loss: 0.1540, LR: 0.000655
Step 300/3000, Loss: 0.1219, LR: 0.000346
Step 400/3000, Loss: 0.1177, LR: 0.000096
Step 500/3000, Loss: 0.0772, LR: 0.001000
Step 600/3000, Loss: 0.1293, LR: 0.000976
Step 700/3000, Loss: 0.0843, LR: 0.000905
Step 800/3000, Loss: 0.0542, LR: 0.000794
Step 900/3000, Loss: 0.0630, LR: 0.000655
Step 1000/3000, Loss: 0.0276, LR: 0.000501
Step 1100/3000, Loss: 0.0177, LR: 0.000346
Step 1200/3000, Loss: 0.0073, LR: 0.000207
Step 1300/3000, Loss: 0.0065, LR: 0.000096
Step 1400/3000, Loss: 0.0055, LR: 0.000025
Step 1500/3000, Loss: 0.0042, LR: 0.001000
Step 1600/3000, Loss: 0.0488, LR: 0.000994
Step 1700/3000, Loss: 0.0559, LR: 0.000976
Step 1800/3000, Loss: 0.0512, LR: 0.000946
Step 1900/3000, Loss: 0.0287, LR: 0.000905
Step 2000/3000, Loss: 0.0171, LR: 0.000854
Step 2100/3000, Loss: 0.0090, LR: 0.000794
Step 2200/3000, Loss: 0.0051, LR: 0.000727
Step 2300/3000, Loss: 0.0039, LR: 0.000655
Step 2400/3000, Loss: 0.0034, LR: 0.000579
Step 2500/3000, Loss: 0.0013, LR: 0.000501
Step 2600/3000, Loss: 0.0006, LR: 0.000422
Step 2700/3000, Loss: 0.0008, LR: 0.000346
Step 2800/3000, Loss: 0.0006, LR: 0.000274
Step 2900/3000, Loss: 0.0005, LR: 0.000207
Step 3000/3000, Loss: 0.0007, LR: 0.000147
Training time: 10.29s (3.43ms per step)
Evaluating model...
Hard Accuracy: 0.7058
Soft Accuracy: 0.8564
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7058
Axiom (neutrality) Satisfaction Rate: 0.6894
Axiom (condorcet) Satisfaction Rate: 0.5458
Axiom (pareto) Satisfaction Rate: 0.0478
Measuring inference time...
Average inference time (single sample): 0.2701ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7058
  Soft Accuracy: 0.8564
  Training Time: 10.29s
  Inference Time: 0.2701ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 15,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 5.24s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2562, LR: 0.000905
Step 200/3000, Loss: 0.1375, LR: 0.000655
Step 300/3000, Loss: 0.1079, LR: 0.000346
Step 400/3000, Loss: 0.0824, LR: 0.000096
Step 500/3000, Loss: 0.0605, LR: 0.001000
Step 600/3000, Loss: 0.0540, LR: 0.000976
Step 700/3000, Loss: 0.0174, LR: 0.000905
Step 800/3000, Loss: 0.0197, LR: 0.000794
Step 900/3000, Loss: 0.0088, LR: 0.000655
Step 1000/3000, Loss: 0.0072, LR: 0.000501
Step 1100/3000, Loss: 0.0063, LR: 0.000346
Step 1200/3000, Loss: 0.0012, LR: 0.000207
Step 1300/3000, Loss: 0.0013, LR: 0.000096
Step 1400/3000, Loss: 0.0007, LR: 0.000025
Step 1500/3000, Loss: 0.0009, LR: 0.001000
Step 1600/3000, Loss: 0.0006, LR: 0.000994
Step 1700/3000, Loss: 0.0029, LR: 0.000976
Step 1800/3000, Loss: 0.0358, LR: 0.000946
Step 1900/3000, Loss: 0.0154, LR: 0.000905
Step 2000/3000, Loss: 0.0047, LR: 0.000854
Step 2100/3000, Loss: 0.0124, LR: 0.000794
Step 2200/3000, Loss: 0.0006, LR: 0.000727
Step 2300/3000, Loss: 0.0022, LR: 0.000655
Step 2400/3000, Loss: 0.0004, LR: 0.000579
Step 2500/3000, Loss: 0.0001, LR: 0.000501
Step 2600/3000, Loss: 0.0002, LR: 0.000422
Step 2700/3000, Loss: 0.0001, LR: 0.000346
Step 2800/3000, Loss: 0.0002, LR: 0.000274
Step 2900/3000, Loss: 0.0001, LR: 0.000207
Step 3000/3000, Loss: 0.0001, LR: 0.000147
Training time: 8.58s (2.86ms per step)
Evaluating model...
Hard Accuracy: 0.874
Soft Accuracy: 0.9938
Evaluation time: 0.22s
Checking axiom satisfaction...
4370
Axiom (anonymity) Satisfaction Rate: 0.874
4096
Axiom (neutrality) Satisfaction Rate: 0.8192
2739
Axiom (condorcet) Satisfaction Rate: 0.5478
239
Axiom (pareto) Satisfaction Rate: 0.0478
Measuring inference time...
Average inference time (single sample): 0.2324ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.8740
  Soft Accuracy: 0.9938
  Training Time: 8.58s
  Inference Time: 0.2324ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 50,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 39.74s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.3078, LR: 0.000905
Step 200/3000, Loss: 0.2652, LR: 0.000655
Step 300/3000, Loss: 0.2184, LR: 0.000346
Step 400/3000, Loss: 0.2288, LR: 0.000096
Step 500/3000, Loss: 0.2254, LR: 0.001000
Step 600/3000, Loss: 0.2371, LR: 0.000976
Step 700/3000, Loss: 0.2078, LR: 0.000905
Step 800/3000, Loss: 0.2554, LR: 0.000794
Step 900/3000, Loss: 0.1883, LR: 0.000655
Step 1000/3000, Loss: 0.1920, LR: 0.000501
Step 1100/3000, Loss: 0.1877, LR: 0.000346
Step 1200/3000, Loss: 0.1868, LR: 0.000207
Step 1300/3000, Loss: 0.2118, LR: 0.000096
Step 1400/3000, Loss: 0.2102, LR: 0.000025
Step 1500/3000, Loss: 0.1887, LR: 0.001000
Step 1600/3000, Loss: 0.2257, LR: 0.000994
Step 1700/3000, Loss: 0.2162, LR: 0.000976
Step 1800/3000, Loss: 0.2200, LR: 0.000946
Step 1900/3000, Loss: 0.2298, LR: 0.000905
Step 2000/3000, Loss: 0.2137, LR: 0.000854
Step 2100/3000, Loss: 0.2223, LR: 0.000794
Step 2200/3000, Loss: 0.2046, LR: 0.000727
Step 2300/3000, Loss: 0.2172, LR: 0.000655
Step 2400/3000, Loss: 0.2178, LR: 0.000579
Step 2500/3000, Loss: 0.1819, LR: 0.000501
Step 2600/3000, Loss: 0.1973, LR: 0.000422
Step 2700/3000, Loss: 0.2155, LR: 0.000346
Step 2800/3000, Loss: 0.2017, LR: 0.000274
Step 2900/3000, Loss: 0.1828, LR: 0.000207
Step 3000/3000, Loss: 0.2113, LR: 0.000147
Training time: 7.72s (2.57ms per step)
Evaluating model...
Hard Accuracy: 0.7296
Soft Accuracy: 0.8552
Evaluation time: 0.21s
Checking axiom satisfaction...
3648
Axiom (anonymity) Satisfaction Rate: 0.7296
3620
Axiom (neutrality) Satisfaction Rate: 0.724
2742
Axiom (condorcet) Satisfaction Rate: 0.5484
238
Axiom (pareto) Satisfaction Rate: 0.0476
Measuring inference time...
Average inference time (single sample): 0.2362ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7296
  Soft Accuracy: 0.8552
  Training Time: 7.72s
  Inference Time: 0.2362ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 21.80s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.2242, LR: 0.000905
Step 200/3000, Loss: 0.2466, LR: 0.000655
Step 300/3000, Loss: 0.1759, LR: 0.000346
Step 400/3000, Loss: 0.1781, LR: 0.000096
Step 500/3000, Loss: 0.1739, LR: 0.001000
Step 600/3000, Loss: 0.1961, LR: 0.000976
Step 700/3000, Loss: 0.2103, LR: 0.000905
Step 800/3000, Loss: 0.1696, LR: 0.000794
Step 900/3000, Loss: 0.1614, LR: 0.000655
Step 1000/3000, Loss: 0.1766, LR: 0.000501
Step 1100/3000, Loss: 0.1067, LR: 0.000346
Step 1200/3000, Loss: 0.1223, LR: 0.000207
Step 1300/3000, Loss: 0.1290, LR: 0.000096
Step 1400/3000, Loss: 0.0774, LR: 0.000025
Step 1500/3000, Loss: 0.0919, LR: 0.001000
Step 1600/3000, Loss: 0.1337, LR: 0.000994
Step 1700/3000, Loss: 0.1283, LR: 0.000976
Step 1800/3000, Loss: 0.0942, LR: 0.000946
Step 1900/3000, Loss: 0.1256, LR: 0.000905
Step 2000/3000, Loss: 0.0910, LR: 0.000854
Step 2100/3000, Loss: 0.0718, LR: 0.000794
Step 2200/3000, Loss: 0.1061, LR: 0.000727
Step 2300/3000, Loss: 0.0756, LR: 0.000655
Step 2400/3000, Loss: 0.0814, LR: 0.000579
Step 2500/3000, Loss: 0.0645, LR: 0.000501
Step 2600/3000, Loss: 0.0356, LR: 0.000422
Step 2700/3000, Loss: 0.0464, LR: 0.000346
Step 2800/3000, Loss: 0.0413, LR: 0.000274
Step 2900/3000, Loss: 0.0298, LR: 0.000207
Step 3000/3000, Loss: 0.0317, LR: 0.000147
Training time: 14.91s (4.97ms per step)
Evaluating model...
Hard Accuracy: 0.7436
Soft Accuracy: 0.8864
Evaluation time: 0.42s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7436
Axiom (neutrality) Satisfaction Rate: 0.7174
Axiom (condorcet) Satisfaction Rate: 0.5466
Axiom (pareto) Satisfaction Rate: 0.0454
Measuring inference time...
Average inference time (single sample): 0.2702ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7436
  Soft Accuracy: 0.8864
  Training Time: 14.91s
  Inference Time: 0.2702ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 50,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 21.77s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2162, LR: 0.000905
Step 200/3000, Loss: 0.1612, LR: 0.000655
Step 300/3000, Loss: 0.0967, LR: 0.000346
Step 400/3000, Loss: 0.1008, LR: 0.000096
Step 500/3000, Loss: 0.0898, LR: 0.001000
Step 600/3000, Loss: 0.0479, LR: 0.000976
Step 700/3000, Loss: 0.0561, LR: 0.000905
Step 800/3000, Loss: 0.0302, LR: 0.000794
Step 900/3000, Loss: 0.0313, LR: 0.000655
Step 1000/3000, Loss: 0.0264, LR: 0.000501
Step 1100/3000, Loss: 0.0117, LR: 0.000346
Step 1200/3000, Loss: 0.0115, LR: 0.000207
Step 1300/3000, Loss: 0.0054, LR: 0.000096
Step 1400/3000, Loss: 0.0061, LR: 0.000025
Step 1500/3000, Loss: 0.0082, LR: 0.001000
Step 1600/3000, Loss: 0.0337, LR: 0.000994
Step 1700/3000, Loss: 0.0467, LR: 0.000976
Step 1800/3000, Loss: 0.0170, LR: 0.000946
Step 1900/3000, Loss: 0.0217, LR: 0.000905
Step 2000/3000, Loss: 0.0164, LR: 0.000854
Step 2100/3000, Loss: 0.0077, LR: 0.000794
Step 2200/3000, Loss: 0.0073, LR: 0.000727
Step 2300/3000, Loss: 0.0092, LR: 0.000655
Step 2400/3000, Loss: 0.0020, LR: 0.000579
Step 2500/3000, Loss: 0.0040, LR: 0.000501
Step 2600/3000, Loss: 0.0010, LR: 0.000422
Step 2700/3000, Loss: 0.0011, LR: 0.000346
Step 2800/3000, Loss: 0.0050, LR: 0.000274
Step 2900/3000, Loss: 0.0005, LR: 0.000207
Step 3000/3000, Loss: 0.0007, LR: 0.000147
Training time: 12.57s (4.19ms per step)
Evaluating model...
Hard Accuracy: 0.9606
Soft Accuracy: 0.999
Evaluation time: 0.24s
Checking axiom satisfaction...
4803
Axiom (anonymity) Satisfaction Rate: 0.9606
4464
Axiom (neutrality) Satisfaction Rate: 0.8928
2676
Axiom (condorcet) Satisfaction Rate: 0.5352
259
Axiom (pareto) Satisfaction Rate: 0.0518
Measuring inference time...
Average inference time (single sample): 0.2339ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9606
  Soft Accuracy: 0.9990
  Training Time: 12.57s
  Inference Time: 0.2339ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 150,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 119.29s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.2559, LR: 0.000905
Step 200/3000, Loss: 0.2514, LR: 0.000655
Step 300/3000, Loss: 0.2417, LR: 0.000346
Step 400/3000, Loss: 0.2306, LR: 0.000096
Step 500/3000, Loss: 0.1926, LR: 0.001000
Step 600/3000, Loss: 0.2183, LR: 0.000976
Step 700/3000, Loss: 0.2286, LR: 0.000905
Step 800/3000, Loss: 0.1977, LR: 0.000794
Step 900/3000, Loss: 0.2392, LR: 0.000655
Step 1000/3000, Loss: 0.2054, LR: 0.000501
Step 1100/3000, Loss: 0.2055, LR: 0.000346
Step 1200/3000, Loss: 0.2295, LR: 0.000207
Step 1300/3000, Loss: 0.2139, LR: 0.000096
Step 1400/3000, Loss: 0.2073, LR: 0.000025
Step 1500/3000, Loss: 0.2259, LR: 0.001000
Step 1600/3000, Loss: 0.1743, LR: 0.000994
Step 1700/3000, Loss: 0.1973, LR: 0.000976
Step 1800/3000, Loss: 0.1768, LR: 0.000946
Step 1900/3000, Loss: 0.2088, LR: 0.000905
Step 2000/3000, Loss: 0.1704, LR: 0.000854
Step 2100/3000, Loss: 0.1998, LR: 0.000794
Step 2200/3000, Loss: 0.2155, LR: 0.000727
Step 2300/3000, Loss: 0.2260, LR: 0.000655
Step 2400/3000, Loss: 0.2122, LR: 0.000579
Step 2500/3000, Loss: 0.1926, LR: 0.000501
Step 2600/3000, Loss: 0.2381, LR: 0.000422
Step 2700/3000, Loss: 0.2059, LR: 0.000346
Step 2800/3000, Loss: 0.1913, LR: 0.000274
Step 2900/3000, Loss: 0.2808, LR: 0.000207
Step 3000/3000, Loss: 0.2050, LR: 0.000147
Training time: 9.13s (3.04ms per step)
Evaluating model...
Hard Accuracy: 0.7282
Soft Accuracy: 0.8578
Evaluation time: 0.21s
Checking axiom satisfaction...
3641
Axiom (anonymity) Satisfaction Rate: 0.7282
3618
Axiom (neutrality) Satisfaction Rate: 0.7236
2815
Axiom (condorcet) Satisfaction Rate: 0.563
240
Axiom (pareto) Satisfaction Rate: 0.048
Measuring inference time...
Average inference time (single sample): 0.2320ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7282
  Soft Accuracy: 0.8578
  Training Time: 9.13s
  Inference Time: 0.2320ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 72.22s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1919, LR: 0.000905
Step 200/3000, Loss: 0.2141, LR: 0.000655
Step 300/3000, Loss: 0.1774, LR: 0.000346
Step 400/3000, Loss: 0.1912, LR: 0.000096
Step 500/3000, Loss: 0.1916, LR: 0.001000
Step 600/3000, Loss: 0.1975, LR: 0.000976
Step 700/3000, Loss: 0.1692, LR: 0.000905
Step 800/3000, Loss: 0.1866, LR: 0.000794
Step 900/3000, Loss: 0.1600, LR: 0.000655
Step 1000/3000, Loss: 0.1732, LR: 0.000501
Step 1100/3000, Loss: 0.1866, LR: 0.000346
Step 1200/3000, Loss: 0.1620, LR: 0.000207
Step 1300/3000, Loss: 0.1666, LR: 0.000096
Step 1400/3000, Loss: 0.1556, LR: 0.000025
Step 1500/3000, Loss: 0.1473, LR: 0.001000
Step 1600/3000, Loss: 0.1798, LR: 0.000994
Step 1700/3000, Loss: 0.1958, LR: 0.000976
Step 1800/3000, Loss: 0.1785, LR: 0.000946
Step 1900/3000, Loss: 0.1668, LR: 0.000905
Step 2000/3000, Loss: 0.1653, LR: 0.000854
Step 2100/3000, Loss: 0.1762, LR: 0.000794
Step 2200/3000, Loss: 0.1669, LR: 0.000727
Step 2300/3000, Loss: 0.1407, LR: 0.000655
Step 2400/3000, Loss: 0.1502, LR: 0.000579
Step 2500/3000, Loss: 0.1352, LR: 0.000501
Step 2600/3000, Loss: 0.1556, LR: 0.000422
Step 2700/3000, Loss: 0.1244, LR: 0.000346
Step 2800/3000, Loss: 0.1313, LR: 0.000274
Step 2900/3000, Loss: 0.1086, LR: 0.000207
Step 3000/3000, Loss: 0.1359, LR: 0.000147
Training time: 8.78s (2.93ms per step)
Evaluating model...
Hard Accuracy: 0.7638
Soft Accuracy: 0.885
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7638
Axiom (neutrality) Satisfaction Rate: 0.742
Axiom (condorcet) Satisfaction Rate: 0.552
Axiom (pareto) Satisfaction Rate: 0.0418
Measuring inference time...
Average inference time (single sample): 0.3595ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7638
  Soft Accuracy: 0.8850
  Training Time: 8.78s
  Inference Time: 0.3595ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 150,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 56.17s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2369, LR: 0.000905
Step 200/3000, Loss: 0.1520, LR: 0.000655
Step 300/3000, Loss: 0.1200, LR: 0.000346
Step 400/3000, Loss: 0.0922, LR: 0.000096
Step 500/3000, Loss: 0.0895, LR: 0.001000
Step 600/3000, Loss: 0.0771, LR: 0.000976
Step 700/3000, Loss: 0.0629, LR: 0.000905
Step 800/3000, Loss: 0.0442, LR: 0.000794
Step 900/3000, Loss: 0.0259, LR: 0.000655
Step 1000/3000, Loss: 0.0379, LR: 0.000501
Step 1100/3000, Loss: 0.0237, LR: 0.000346
Step 1200/3000, Loss: 0.0221, LR: 0.000207
Step 1300/3000, Loss: 0.0129, LR: 0.000096
Step 1400/3000, Loss: 0.0085, LR: 0.000025
Step 1500/3000, Loss: 0.0085, LR: 0.001000
Step 1600/3000, Loss: 0.0511, LR: 0.000994
Step 1700/3000, Loss: 0.0380, LR: 0.000976
Step 1800/3000, Loss: 0.0254, LR: 0.000946
Step 1900/3000, Loss: 0.0141, LR: 0.000905
Step 2000/3000, Loss: 0.0312, LR: 0.000854
Step 2100/3000, Loss: 0.0229, LR: 0.000794
Step 2200/3000, Loss: 0.0204, LR: 0.000727
Step 2300/3000, Loss: 0.0220, LR: 0.000655
Step 2400/3000, Loss: 0.0097, LR: 0.000579
Step 2500/3000, Loss: 0.0085, LR: 0.000501
Step 2600/3000, Loss: 0.0074, LR: 0.000422
Step 2700/3000, Loss: 0.0018, LR: 0.000346
Step 2800/3000, Loss: 0.0058, LR: 0.000274
Step 2900/3000, Loss: 0.0085, LR: 0.000207
Step 3000/3000, Loss: 0.0056, LR: 0.000147
Training time: 11.59s (3.86ms per step)
Evaluating model...
Hard Accuracy: 0.9884
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4942
Axiom (anonymity) Satisfaction Rate: 0.9884
4590
Axiom (neutrality) Satisfaction Rate: 0.918
2640
Axiom (condorcet) Satisfaction Rate: 0.528
218
Axiom (pareto) Satisfaction Rate: 0.0436
Measuring inference time...
Average inference time (single sample): 0.2309ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9884
  Soft Accuracy: 1.0000
  Training Time: 11.59s
  Inference Time: 0.2309ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 500,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 415.64s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.3088, LR: 0.000905
Step 200/3000, Loss: 0.1913, LR: 0.000655
Step 300/3000, Loss: 0.2560, LR: 0.000346
Step 400/3000, Loss: 0.2448, LR: 0.000096
Step 500/3000, Loss: 0.2244, LR: 0.001000
Step 600/3000, Loss: 0.2376, LR: 0.000976
Step 700/3000, Loss: 0.2069, LR: 0.000905
Step 800/3000, Loss: 0.2180, LR: 0.000794
Step 900/3000, Loss: 0.2430, LR: 0.000655
Step 1000/3000, Loss: 0.2500, LR: 0.000501
Step 1100/3000, Loss: 0.1945, LR: 0.000346
Step 1200/3000, Loss: 0.1949, LR: 0.000207
Step 1300/3000, Loss: 0.2159, LR: 0.000096
Step 1400/3000, Loss: 0.2078, LR: 0.000025
Step 1500/3000, Loss: 0.2206, LR: 0.001000
Step 1600/3000, Loss: 0.2279, LR: 0.000994
Step 1700/3000, Loss: 0.2195, LR: 0.000976
Step 1800/3000, Loss: 0.1695, LR: 0.000946
Step 1900/3000, Loss: 0.1768, LR: 0.000905
Step 2000/3000, Loss: 0.1842, LR: 0.000854
Step 2100/3000, Loss: 0.1921, LR: 0.000794
Step 2200/3000, Loss: 0.1984, LR: 0.000727
Step 2300/3000, Loss: 0.1957, LR: 0.000655
Step 2400/3000, Loss: 0.2393, LR: 0.000579
Step 2500/3000, Loss: 0.1734, LR: 0.000501
Step 2600/3000, Loss: 0.1826, LR: 0.000422
Step 2700/3000, Loss: 0.2039, LR: 0.000346
Step 2800/3000, Loss: 0.1978, LR: 0.000274
Step 2900/3000, Loss: 0.2494, LR: 0.000207
Step 3000/3000, Loss: 0.1952, LR: 0.000147
Training time: 8.00s (2.67ms per step)
Evaluating model...
Hard Accuracy: 0.7152
Soft Accuracy: 0.845
Evaluation time: 1.37s
Checking axiom satisfaction...
3576
Axiom (anonymity) Satisfaction Rate: 0.7152
3537
Axiom (neutrality) Satisfaction Rate: 0.7074
2675
Axiom (condorcet) Satisfaction Rate: 0.535
246
Axiom (pareto) Satisfaction Rate: 0.0492
Measuring inference time...
Average inference time (single sample): 0.2364ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7152
  Soft Accuracy: 0.8450
  Training Time: 8.00s
  Inference Time: 0.2364ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 223.16s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.2029, LR: 0.000905
Step 200/3000, Loss: 0.2008, LR: 0.000655
Step 300/3000, Loss: 0.2094, LR: 0.000346
Step 400/3000, Loss: 0.1600, LR: 0.000096
Step 500/3000, Loss: 0.2226, LR: 0.001000
Step 600/3000, Loss: 0.2052, LR: 0.000976
Step 700/3000, Loss: 0.2003, LR: 0.000905
Step 800/3000, Loss: 0.1878, LR: 0.000794
Step 900/3000, Loss: 0.1715, LR: 0.000655
Step 1000/3000, Loss: 0.1899, LR: 0.000501
Step 1100/3000, Loss: 0.1782, LR: 0.000346
Step 1200/3000, Loss: 0.1544, LR: 0.000207
Step 1300/3000, Loss: 0.1957, LR: 0.000096
Step 1400/3000, Loss: 0.1578, LR: 0.000025
Step 1500/3000, Loss: 0.1592, LR: 0.001000
Step 1600/3000, Loss: 0.1778, LR: 0.000994
Step 1700/3000, Loss: 0.1515, LR: 0.000976
Step 1800/3000, Loss: 0.2241, LR: 0.000946
Step 1900/3000, Loss: 0.2078, LR: 0.000905
Step 2000/3000, Loss: 0.1528, LR: 0.000854
Step 2100/3000, Loss: 0.1764, LR: 0.000794
Step 2200/3000, Loss: 0.1929, LR: 0.000727
Step 2300/3000, Loss: 0.1889, LR: 0.000655
Step 2400/3000, Loss: 0.1920, LR: 0.000579
Step 2500/3000, Loss: 0.1650, LR: 0.000501
Step 2600/3000, Loss: 0.1521, LR: 0.000422
Step 2700/3000, Loss: 0.1350, LR: 0.000346
Step 2800/3000, Loss: 0.1687, LR: 0.000274
Step 2900/3000, Loss: 0.1572, LR: 0.000207
Step 3000/3000, Loss: 0.1638, LR: 0.000147
Training time: 12.45s (4.15ms per step)
Evaluating model...
Hard Accuracy: 0.7682
Soft Accuracy: 0.877
Evaluation time: 0.74s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7682
Axiom (neutrality) Satisfaction Rate: 0.746
Axiom (condorcet) Satisfaction Rate: 0.5478
Axiom (pareto) Satisfaction Rate: 0.0476
Measuring inference time...
Average inference time (single sample): 0.2692ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7682
  Soft Accuracy: 0.8770
  Training Time: 12.45s
  Inference Time: 0.2692ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 500,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 191.26s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2724, LR: 0.000905
Step 200/3000, Loss: 0.1534, LR: 0.000655
Step 300/3000, Loss: 0.0925, LR: 0.000346
Step 400/3000, Loss: 0.0833, LR: 0.000096
Step 500/3000, Loss: 0.0754, LR: 0.001000
Step 600/3000, Loss: 0.0643, LR: 0.000976
Step 700/3000, Loss: 0.0569, LR: 0.000905
Step 800/3000, Loss: 0.0501, LR: 0.000794
Step 900/3000, Loss: 0.0170, LR: 0.000655
Step 1000/3000, Loss: 0.0298, LR: 0.000501
Step 1100/3000, Loss: 0.0281, LR: 0.000346
Step 1200/3000, Loss: 0.0247, LR: 0.000207
Step 1300/3000, Loss: 0.0099, LR: 0.000096
Step 1400/3000, Loss: 0.0132, LR: 0.000025
Step 1500/3000, Loss: 0.0130, LR: 0.001000
Step 1600/3000, Loss: 0.0602, LR: 0.000994
Step 1700/3000, Loss: 0.0322, LR: 0.000976
Step 1800/3000, Loss: 0.0268, LR: 0.000946
Step 1900/3000, Loss: 0.0255, LR: 0.000905
Step 2000/3000, Loss: 0.0136, LR: 0.000854
Step 2100/3000, Loss: 0.0333, LR: 0.000794
Step 2200/3000, Loss: 0.0286, LR: 0.000727
Step 2300/3000, Loss: 0.0125, LR: 0.000655
Step 2400/3000, Loss: 0.0161, LR: 0.000579
Step 2500/3000, Loss: 0.0097, LR: 0.000501
Step 2600/3000, Loss: 0.0238, LR: 0.000422
Step 2700/3000, Loss: 0.0031, LR: 0.000346
Step 2800/3000, Loss: 0.0034, LR: 0.000274
Step 2900/3000, Loss: 0.0059, LR: 0.000207
Step 3000/3000, Loss: 0.0041, LR: 0.000147
Training time: 10.97s (3.66ms per step)
Evaluating model...
Hard Accuracy: 0.9954
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4977
Axiom (anonymity) Satisfaction Rate: 0.9954
4632
Axiom (neutrality) Satisfaction Rate: 0.9264
2730
Axiom (condorcet) Satisfaction Rate: 0.546
238
Axiom (pareto) Satisfaction Rate: 0.0476
Measuring inference time...
Average inference time (single sample): 0.2348ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9954
  Soft Accuracy: 1.0000
  Training Time: 10.97s
  Inference Time: 0.2348ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 1,000,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 788.97s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.2745, LR: 0.000905
Step 200/3000, Loss: 0.2460, LR: 0.000655
Step 300/3000, Loss: 0.2163, LR: 0.000346
Step 400/3000, Loss: 0.2005, LR: 0.000096
Step 500/3000, Loss: 0.2446, LR: 0.001000
Step 600/3000, Loss: 0.2159, LR: 0.000976
Step 700/3000, Loss: 0.2003, LR: 0.000905
Step 800/3000, Loss: 0.2069, LR: 0.000794
Step 900/3000, Loss: 0.2421, LR: 0.000655
Step 1000/3000, Loss: 0.2225, LR: 0.000501
Step 1100/3000, Loss: 0.2117, LR: 0.000346
Step 1200/3000, Loss: 0.2543, LR: 0.000207
Step 1300/3000, Loss: 0.2513, LR: 0.000096
Step 1400/3000, Loss: 0.1755, LR: 0.000025
Step 1500/3000, Loss: 0.1829, LR: 0.001000
Step 1600/3000, Loss: 0.2104, LR: 0.000994
Step 1700/3000, Loss: 0.2250, LR: 0.000976
Step 1800/3000, Loss: 0.1827, LR: 0.000946
Step 1900/3000, Loss: 0.1635, LR: 0.000905
Step 2000/3000, Loss: 0.2093, LR: 0.000854
Step 2100/3000, Loss: 0.2153, LR: 0.000794
Step 2200/3000, Loss: 0.2224, LR: 0.000727
Step 2300/3000, Loss: 0.1598, LR: 0.000655
Step 2400/3000, Loss: 0.2094, LR: 0.000579
Step 2500/3000, Loss: 0.2508, LR: 0.000501
Step 2600/3000, Loss: 0.2044, LR: 0.000422
Step 2700/3000, Loss: 0.1854, LR: 0.000346
Step 2800/3000, Loss: 0.1808, LR: 0.000274
Step 2900/3000, Loss: 0.2063, LR: 0.000207
Step 3000/3000, Loss: 0.1636, LR: 0.000147
Training time: 8.39s (2.80ms per step)
Evaluating model...
Hard Accuracy: 0.7318
Soft Accuracy: 0.8586
Evaluation time: 0.22s
Checking axiom satisfaction...
3659
Axiom (anonymity) Satisfaction Rate: 0.7318
3630
Axiom (neutrality) Satisfaction Rate: 0.726
2766
Axiom (condorcet) Satisfaction Rate: 0.5532
241
Axiom (pareto) Satisfaction Rate: 0.0482
Measuring inference time...
Average inference time (single sample): 0.2322ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7318
  Soft Accuracy: 0.8586
  Training Time: 8.39s
  Inference Time: 0.2322ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 440.75s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.2148, LR: 0.000905
Step 200/3000, Loss: 0.2132, LR: 0.000655
Step 300/3000, Loss: 0.1883, LR: 0.000346
Step 400/3000, Loss: 0.1933, LR: 0.000096
Step 500/3000, Loss: 0.1722, LR: 0.001000
Step 600/3000, Loss: 0.1873, LR: 0.000976
Step 700/3000, Loss: 0.1860, LR: 0.000905
Step 800/3000, Loss: 0.1666, LR: 0.000794
Step 900/3000, Loss: 0.1383, LR: 0.000655
Step 1000/3000, Loss: 0.1672, LR: 0.000501
Step 1100/3000, Loss: 0.1515, LR: 0.000346
Step 1200/3000, Loss: 0.1771, LR: 0.000207
Step 1300/3000, Loss: 0.1750, LR: 0.000096
Step 1400/3000, Loss: 0.1844, LR: 0.000025
Step 1500/3000, Loss: 0.1719, LR: 0.001000
Step 1600/3000, Loss: 0.1571, LR: 0.000994
Step 1700/3000, Loss: 0.1944, LR: 0.000976
Step 1800/3000, Loss: 0.1876, LR: 0.000946
Step 1900/3000, Loss: 0.1641, LR: 0.000905
Step 2000/3000, Loss: 0.1812, LR: 0.000854
Step 2100/3000, Loss: 0.1641, LR: 0.000794
Step 2200/3000, Loss: 0.1723, LR: 0.000727
Step 2300/3000, Loss: 0.1616, LR: 0.000655
Step 2400/3000, Loss: 0.1851, LR: 0.000579
Step 2500/3000, Loss: 0.1624, LR: 0.000501
Step 2600/3000, Loss: 0.1354, LR: 0.000422
Step 2700/3000, Loss: 0.1567, LR: 0.000346
Step 2800/3000, Loss: 0.1536, LR: 0.000274
Step 2900/3000, Loss: 0.1494, LR: 0.000207
Step 3000/3000, Loss: 0.1409, LR: 0.000147
Training time: 11.23s (3.74ms per step)
Evaluating model...
Hard Accuracy: 0.774
Soft Accuracy: 0.8774
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.774
Axiom (neutrality) Satisfaction Rate: 0.7466
Axiom (condorcet) Satisfaction Rate: 0.5364
Axiom (pareto) Satisfaction Rate: 0.043
Measuring inference time...
Average inference time (single sample): 0.2682ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7740
  Soft Accuracy: 0.8774
  Training Time: 11.23s
  Inference Time: 0.2682ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 1,000,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 387.62s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2193, LR: 0.000905
Step 200/3000, Loss: 0.1622, LR: 0.000655
Step 300/3000, Loss: 0.1062, LR: 0.000346
Step 400/3000, Loss: 0.0835, LR: 0.000096
Step 500/3000, Loss: 0.0962, LR: 0.001000
Step 600/3000, Loss: 0.0820, LR: 0.000976
Step 700/3000, Loss: 0.0802, LR: 0.000905
Step 800/3000, Loss: 0.0683, LR: 0.000794
Step 900/3000, Loss: 0.0406, LR: 0.000655
Step 1000/3000, Loss: 0.0284, LR: 0.000501
Step 1100/3000, Loss: 0.0170, LR: 0.000346
Step 1200/3000, Loss: 0.0202, LR: 0.000207
Step 1300/3000, Loss: 0.0167, LR: 0.000096
Step 1400/3000, Loss: 0.0190, LR: 0.000025
Step 1500/3000, Loss: 0.0119, LR: 0.001000
Step 1600/3000, Loss: 0.0453, LR: 0.000994
Step 1700/3000, Loss: 0.0260, LR: 0.000976
Step 1800/3000, Loss: 0.0279, LR: 0.000946
Step 1900/3000, Loss: 0.0125, LR: 0.000905
Step 2000/3000, Loss: 0.0107, LR: 0.000854
Step 2100/3000, Loss: 0.0241, LR: 0.000794
Step 2200/3000, Loss: 0.0201, LR: 0.000727
Step 2300/3000, Loss: 0.0341, LR: 0.000655
Step 2400/3000, Loss: 0.0072, LR: 0.000579
Step 2500/3000, Loss: 0.0173, LR: 0.000501
Step 2600/3000, Loss: 0.0065, LR: 0.000422
Step 2700/3000, Loss: 0.0103, LR: 0.000346
Step 2800/3000, Loss: 0.0041, LR: 0.000274
Step 2900/3000, Loss: 0.0015, LR: 0.000207
Step 3000/3000, Loss: 0.0044, LR: 0.000147
Training time: 11.05s (3.68ms per step)
Evaluating model...
Hard Accuracy: 0.9944
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4972
Axiom (anonymity) Satisfaction Rate: 0.9944
4601
Axiom (neutrality) Satisfaction Rate: 0.9202
2787
Axiom (condorcet) Satisfaction Rate: 0.5574
260
Axiom (pareto) Satisfaction Rate: 0.052
Measuring inference time...
Average inference time (single sample): 0.2329ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9944
  Soft Accuracy: 1.0000
  Training Time: 11.05s
  Inference Time: 0.2329ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

################################################################################
# Testing with COPELAND voting method
################################################################################

--------------------------------------------------------------------------------
Dataset size: 1,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 0.74s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1544, LR: 0.000905
Step 200/3000, Loss: 0.1242, LR: 0.000655
Step 300/3000, Loss: 0.0625, LR: 0.000346
Step 400/3000, Loss: 0.0624, LR: 0.000096
Step 500/3000, Loss: 0.0645, LR: 0.001000
Step 600/3000, Loss: 0.0402, LR: 0.000976
Step 700/3000, Loss: 0.0418, LR: 0.000905
Step 800/3000, Loss: 0.0259, LR: 0.000794
Step 900/3000, Loss: 0.0286, LR: 0.000655
Step 1000/3000, Loss: 0.0205, LR: 0.000501
Step 1100/3000, Loss: 0.0240, LR: 0.000346
Step 1200/3000, Loss: 0.0133, LR: 0.000207
Step 1300/3000, Loss: 0.0132, LR: 0.000096
Step 1400/3000, Loss: 0.0175, LR: 0.000025
Step 1500/3000, Loss: 0.0153, LR: 0.001000
Step 1600/3000, Loss: 0.0219, LR: 0.000994
Step 1700/3000, Loss: 0.0212, LR: 0.000976
Step 1800/3000, Loss: 0.0095, LR: 0.000946
Step 1900/3000, Loss: 0.0195, LR: 0.000905
Step 2000/3000, Loss: 0.0198, LR: 0.000854
Step 2100/3000, Loss: 0.0103, LR: 0.000794
Step 2200/3000, Loss: 0.0184, LR: 0.000727
Step 2300/3000, Loss: 0.0211, LR: 0.000655
Step 2400/3000, Loss: 0.0165, LR: 0.000579
Step 2500/3000, Loss: 0.0152, LR: 0.000501
Step 2600/3000, Loss: 0.0124, LR: 0.000422
Step 2700/3000, Loss: 0.0168, LR: 0.000346
Step 2800/3000, Loss: 0.0174, LR: 0.000274
Step 2900/3000, Loss: 0.0152, LR: 0.000207
Step 3000/3000, Loss: 0.0137, LR: 0.000147
Training time: 7.48s (2.49ms per step)
Evaluating model...
Hard Accuracy: 0.8468
Soft Accuracy: 0.9798
Evaluation time: 0.22s
Checking axiom satisfaction...
4234
Axiom (anonymity) Satisfaction Rate: 0.8468
4127
Axiom (neutrality) Satisfaction Rate: 0.8254
4040
Axiom (condorcet) Satisfaction Rate: 0.808
247
Axiom (pareto) Satisfaction Rate: 0.0494
Measuring inference time...
Average inference time (single sample): 0.2352ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.8468
  Soft Accuracy: 0.9798
  Training Time: 7.48s
  Inference Time: 0.2352ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 0.42s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.0209, LR: 0.000905
Step 200/3000, Loss: 0.0063, LR: 0.000655
Step 300/3000, Loss: 0.0039, LR: 0.000346
Step 400/3000, Loss: 0.0031, LR: 0.000096
Step 500/3000, Loss: 0.0030, LR: 0.001000
Step 600/3000, Loss: 0.0014, LR: 0.000976
Step 700/3000, Loss: 0.0008, LR: 0.000905
Step 800/3000, Loss: 0.0006, LR: 0.000794
Step 900/3000, Loss: 0.0004, LR: 0.000655
Step 1000/3000, Loss: 0.0004, LR: 0.000501
Step 1100/3000, Loss: 0.0003, LR: 0.000346
Step 1200/3000, Loss: 0.0003, LR: 0.000207
Step 1300/3000, Loss: 0.0003, LR: 0.000096
Step 1400/3000, Loss: 0.0003, LR: 0.000025
Step 1500/3000, Loss: 0.0003, LR: 0.001000
Step 1600/3000, Loss: 0.0002, LR: 0.000994
Step 1700/3000, Loss: 0.0002, LR: 0.000976
Step 1800/3000, Loss: 0.0001, LR: 0.000946
Step 1900/3000, Loss: 0.0001, LR: 0.000905
Step 2000/3000, Loss: 0.0001, LR: 0.000854
Step 2100/3000, Loss: 0.0001, LR: 0.000794
Step 2200/3000, Loss: 0.0001, LR: 0.000727
Step 2300/3000, Loss: 0.0001, LR: 0.000655
Step 2400/3000, Loss: 0.0001, LR: 0.000579
Step 2500/3000, Loss: 0.0000, LR: 0.000501
Step 2600/3000, Loss: 0.0000, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0000, LR: 0.000274
Step 2900/3000, Loss: 0.0000, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 10.26s (3.42ms per step)
Evaluating model...
Hard Accuracy: 0.693
Soft Accuracy: 0.822
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.693
Axiom (neutrality) Satisfaction Rate: 0.6848
Axiom (condorcet) Satisfaction Rate: 0.815
Axiom (pareto) Satisfaction Rate: 0.0498
Measuring inference time...
Average inference time (single sample): 0.2688ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.6930
  Soft Accuracy: 0.8220
  Training Time: 10.26s
  Inference Time: 0.2688ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 1,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 0.35s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.1008, LR: 0.000905
Step 200/3000, Loss: 0.0036, LR: 0.000655
Step 300/3000, Loss: 0.0005, LR: 0.000346
Step 400/3000, Loss: 0.0009, LR: 0.000096
Step 500/3000, Loss: 0.0010, LR: 0.001000
Step 600/3000, Loss: 0.0003, LR: 0.000976
Step 700/3000, Loss: 0.0002, LR: 0.000905
Step 800/3000, Loss: 0.0001, LR: 0.000794
Step 900/3000, Loss: 0.0000, LR: 0.000655
Step 1000/3000, Loss: 0.0000, LR: 0.000501
Step 1100/3000, Loss: 0.0000, LR: 0.000346
Step 1200/3000, Loss: 0.0000, LR: 0.000207
Step 1300/3000, Loss: 0.0000, LR: 0.000096
Step 1400/3000, Loss: 0.0000, LR: 0.000025
Step 1500/3000, Loss: 0.0000, LR: 0.001000
Step 1600/3000, Loss: 0.0000, LR: 0.000994
Step 1700/3000, Loss: 0.0000, LR: 0.000976
Step 1800/3000, Loss: 0.0000, LR: 0.000946
Step 1900/3000, Loss: 0.0000, LR: 0.000905
Step 2000/3000, Loss: 0.0000, LR: 0.000854
Step 2100/3000, Loss: 0.0000, LR: 0.000794
Step 2200/3000, Loss: 0.0000, LR: 0.000727
Step 2300/3000, Loss: 0.0000, LR: 0.000655
Step 2400/3000, Loss: 0.0000, LR: 0.000579
Step 2500/3000, Loss: 0.0000, LR: 0.000501
Step 2600/3000, Loss: 0.0000, LR: 0.000422
Step 2700/3000, Loss: 0.0000, LR: 0.000346
Step 2800/3000, Loss: 0.0000, LR: 0.000274
Step 2900/3000, Loss: 0.0000, LR: 0.000207
Step 3000/3000, Loss: 0.0000, LR: 0.000147
Training time: 10.34s (3.45ms per step)
Evaluating model...
Hard Accuracy: 0.575
Soft Accuracy: 0.8162
Evaluation time: 0.23s
Checking axiom satisfaction...
2875
Axiom (anonymity) Satisfaction Rate: 0.575
2806
Axiom (neutrality) Satisfaction Rate: 0.5612
4040
Axiom (condorcet) Satisfaction Rate: 0.808
227
Axiom (pareto) Satisfaction Rate: 0.0454
Measuring inference time...
Average inference time (single sample): 0.2305ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.5750
  Soft Accuracy: 0.8162
  Training Time: 10.34s
  Inference Time: 0.2305ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 5,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 3.93s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1311, LR: 0.000905
Step 200/3000, Loss: 0.1378, LR: 0.000655
Step 300/3000, Loss: 0.1136, LR: 0.000346
Step 400/3000, Loss: 0.1145, LR: 0.000096
Step 500/3000, Loss: 0.1065, LR: 0.001000
Step 600/3000, Loss: 0.1025, LR: 0.000976
Step 700/3000, Loss: 0.0747, LR: 0.000905
Step 800/3000, Loss: 0.0918, LR: 0.000794
Step 900/3000, Loss: 0.0719, LR: 0.000655
Step 1000/3000, Loss: 0.0509, LR: 0.000501
Step 1100/3000, Loss: 0.0656, LR: 0.000346
Step 1200/3000, Loss: 0.0589, LR: 0.000207
Step 1300/3000, Loss: 0.0471, LR: 0.000096
Step 1400/3000, Loss: 0.0457, LR: 0.000025
Step 1500/3000, Loss: 0.0549, LR: 0.001000
Step 1600/3000, Loss: 0.0538, LR: 0.000994
Step 1700/3000, Loss: 0.0598, LR: 0.000976
Step 1800/3000, Loss: 0.0389, LR: 0.000946
Step 1900/3000, Loss: 0.0325, LR: 0.000905
Step 2000/3000, Loss: 0.0489, LR: 0.000854
Step 2100/3000, Loss: 0.0537, LR: 0.000794
Step 2200/3000, Loss: 0.0337, LR: 0.000727
Step 2300/3000, Loss: 0.0268, LR: 0.000655
Step 2400/3000, Loss: 0.0322, LR: 0.000579
Step 2500/3000, Loss: 0.0217, LR: 0.000501
Step 2600/3000, Loss: 0.0229, LR: 0.000422
Step 2700/3000, Loss: 0.0313, LR: 0.000346
Step 2800/3000, Loss: 0.0155, LR: 0.000274
Step 2900/3000, Loss: 0.0135, LR: 0.000207
Step 3000/3000, Loss: 0.0186, LR: 0.000147
Training time: 7.48s (2.49ms per step)
Evaluating model...
Hard Accuracy: 0.8992
Soft Accuracy: 0.9942
Evaluation time: 0.22s
Checking axiom satisfaction...
4496
Axiom (anonymity) Satisfaction Rate: 0.8992
4352
Axiom (neutrality) Satisfaction Rate: 0.8704
4042
Axiom (condorcet) Satisfaction Rate: 0.8084
260
Axiom (pareto) Satisfaction Rate: 0.052
Measuring inference time...
Average inference time (single sample): 0.2321ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.8992
  Soft Accuracy: 0.9942
  Training Time: 7.48s
  Inference Time: 0.2321ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 2.12s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1208, LR: 0.000905
Step 200/3000, Loss: 0.0360, LR: 0.000655
Step 300/3000, Loss: 0.0239, LR: 0.000346
Step 400/3000, Loss: 0.0082, LR: 0.000096
Step 500/3000, Loss: 0.0078, LR: 0.001000
Step 600/3000, Loss: 0.0417, LR: 0.000976
Step 700/3000, Loss: 0.0186, LR: 0.000905
Step 800/3000, Loss: 0.0071, LR: 0.000794
Step 900/3000, Loss: 0.0055, LR: 0.000655
Step 1000/3000, Loss: 0.0018, LR: 0.000501
Step 1100/3000, Loss: 0.0010, LR: 0.000346
Step 1200/3000, Loss: 0.0009, LR: 0.000207
Step 1300/3000, Loss: 0.0007, LR: 0.000096
Step 1400/3000, Loss: 0.0008, LR: 0.000025
Step 1500/3000, Loss: 0.0008, LR: 0.001000
Step 1600/3000, Loss: 0.0006, LR: 0.000994
Step 1700/3000, Loss: 0.0004, LR: 0.000976
Step 1800/3000, Loss: 0.0002, LR: 0.000946
Step 1900/3000, Loss: 0.0003, LR: 0.000905
Step 2000/3000, Loss: 0.0002, LR: 0.000854
Step 2100/3000, Loss: 0.0002, LR: 0.000794
Step 2200/3000, Loss: 0.0002, LR: 0.000727
Step 2300/3000, Loss: 0.0002, LR: 0.000655
Step 2400/3000, Loss: 0.0001, LR: 0.000579
Step 2500/3000, Loss: 0.0002, LR: 0.000501
Step 2600/3000, Loss: 0.0001, LR: 0.000422
Step 2700/3000, Loss: 0.0001, LR: 0.000346
Step 2800/3000, Loss: 0.0001, LR: 0.000274
Step 2900/3000, Loss: 0.0001, LR: 0.000207
Step 3000/3000, Loss: 0.0001, LR: 0.000147
Training time: 10.25s (3.42ms per step)
Evaluating model...
Hard Accuracy: 0.7734
Soft Accuracy: 0.9118
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7734
Axiom (neutrality) Satisfaction Rate: 0.7592
Axiom (condorcet) Satisfaction Rate: 0.809
Axiom (pareto) Satisfaction Rate: 0.0452
Measuring inference time...
Average inference time (single sample): 0.2679ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.7734
  Soft Accuracy: 0.9118
  Training Time: 10.25s
  Inference Time: 0.2679ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 5,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 1.75s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2295, LR: 0.000905
Step 200/3000, Loss: 0.1024, LR: 0.000655
Step 300/3000, Loss: 0.0477, LR: 0.000346
Step 400/3000, Loss: 0.0303, LR: 0.000096
Step 500/3000, Loss: 0.0290, LR: 0.001000
Step 600/3000, Loss: 0.0270, LR: 0.000976
Step 700/3000, Loss: 0.0138, LR: 0.000905
Step 800/3000, Loss: 0.0053, LR: 0.000794
Step 900/3000, Loss: 0.0014, LR: 0.000655
Step 1000/3000, Loss: 0.0010, LR: 0.000501
Step 1100/3000, Loss: 0.0007, LR: 0.000346
Step 1200/3000, Loss: 0.0007, LR: 0.000207
Step 1300/3000, Loss: 0.0008, LR: 0.000096
Step 1400/3000, Loss: 0.0007, LR: 0.000025
Step 1500/3000, Loss: 0.0007, LR: 0.001000
Step 1600/3000, Loss: 0.0007, LR: 0.000994
Step 1700/3000, Loss: 0.0339, LR: 0.000976
Step 1800/3000, Loss: 0.0116, LR: 0.000946
Step 1900/3000, Loss: 0.0021, LR: 0.000905
Step 2000/3000, Loss: 0.0003, LR: 0.000854
Step 2100/3000, Loss: 0.0003, LR: 0.000794
Step 2200/3000, Loss: 0.0001, LR: 0.000727
Step 2300/3000, Loss: 0.0002, LR: 0.000655
Step 2400/3000, Loss: 0.0001, LR: 0.000579
Step 2500/3000, Loss: 0.0001, LR: 0.000501
Step 2600/3000, Loss: 0.0001, LR: 0.000422
Step 2700/3000, Loss: 0.0001, LR: 0.000346
Step 2800/3000, Loss: 0.0001, LR: 0.000274
Step 2900/3000, Loss: 0.0001, LR: 0.000207
Step 3000/3000, Loss: 0.0001, LR: 0.000147
Training time: 9.97s (3.32ms per step)
Evaluating model...
Hard Accuracy: 0.6926
Soft Accuracy: 0.915
Evaluation time: 0.22s
Checking axiom satisfaction...
3463
Axiom (anonymity) Satisfaction Rate: 0.6926
3371
Axiom (neutrality) Satisfaction Rate: 0.6742
4027
Axiom (condorcet) Satisfaction Rate: 0.8054
239
Axiom (pareto) Satisfaction Rate: 0.0478
Measuring inference time...
Average inference time (single sample): 0.2322ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.6926
  Soft Accuracy: 0.9150
  Training Time: 9.97s
  Inference Time: 0.2322ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 15,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 11.65s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1737, LR: 0.000905
Step 200/3000, Loss: 0.1421, LR: 0.000655
Step 300/3000, Loss: 0.1458, LR: 0.000346
Step 400/3000, Loss: 0.1121, LR: 0.000096
Step 500/3000, Loss: 0.1245, LR: 0.001000
Step 600/3000, Loss: 0.1193, LR: 0.000976
Step 700/3000, Loss: 0.1383, LR: 0.000905
Step 800/3000, Loss: 0.0952, LR: 0.000794
Step 900/3000, Loss: 0.0834, LR: 0.000655
Step 1000/3000, Loss: 0.0792, LR: 0.000501
Step 1100/3000, Loss: 0.0884, LR: 0.000346
Step 1200/3000, Loss: 0.0837, LR: 0.000207
Step 1300/3000, Loss: 0.0822, LR: 0.000096
Step 1400/3000, Loss: 0.0680, LR: 0.000025
Step 1500/3000, Loss: 0.1151, LR: 0.001000
Step 1600/3000, Loss: 0.0736, LR: 0.000994
Step 1700/3000, Loss: 0.0692, LR: 0.000976
Step 1800/3000, Loss: 0.0721, LR: 0.000946
Step 1900/3000, Loss: 0.0593, LR: 0.000905
Step 2000/3000, Loss: 0.0672, LR: 0.000854
Step 2100/3000, Loss: 0.0472, LR: 0.000794
Step 2200/3000, Loss: 0.0595, LR: 0.000727
Step 2300/3000, Loss: 0.0464, LR: 0.000655
Step 2400/3000, Loss: 0.0404, LR: 0.000579
Step 2500/3000, Loss: 0.0454, LR: 0.000501
Step 2600/3000, Loss: 0.0385, LR: 0.000422
Step 2700/3000, Loss: 0.0552, LR: 0.000346
Step 2800/3000, Loss: 0.0353, LR: 0.000274
Step 2900/3000, Loss: 0.0344, LR: 0.000207
Step 3000/3000, Loss: 0.0409, LR: 0.000147
Training time: 7.50s (2.50ms per step)
Evaluating model...
Hard Accuracy: 0.9184
Soft Accuracy: 0.9972
Evaluation time: 0.22s
Checking axiom satisfaction...
4592
Axiom (anonymity) Satisfaction Rate: 0.9184
4408
Axiom (neutrality) Satisfaction Rate: 0.8816
4049
Axiom (condorcet) Satisfaction Rate: 0.8098
223
Axiom (pareto) Satisfaction Rate: 0.0446
Measuring inference time...
Average inference time (single sample): 0.2336ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9184
  Soft Accuracy: 0.9972
  Training Time: 7.50s
  Inference Time: 0.2336ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 6.54s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1312, LR: 0.000905
Step 200/3000, Loss: 0.1095, LR: 0.000655
Step 300/3000, Loss: 0.0735, LR: 0.000346
Step 400/3000, Loss: 0.0521, LR: 0.000096
Step 500/3000, Loss: 0.0615, LR: 0.001000
Step 600/3000, Loss: 0.0789, LR: 0.000976
Step 700/3000, Loss: 0.0560, LR: 0.000905
Step 800/3000, Loss: 0.0601, LR: 0.000794
Step 900/3000, Loss: 0.0223, LR: 0.000655
Step 1000/3000, Loss: 0.0148, LR: 0.000501
Step 1100/3000, Loss: 0.0155, LR: 0.000346
Step 1200/3000, Loss: 0.0090, LR: 0.000207
Step 1300/3000, Loss: 0.0035, LR: 0.000096
Step 1400/3000, Loss: 0.0031, LR: 0.000025
Step 1500/3000, Loss: 0.0038, LR: 0.001000
Step 1600/3000, Loss: 0.0530, LR: 0.000994
Step 1700/3000, Loss: 0.0354, LR: 0.000976
Step 1800/3000, Loss: 0.0185, LR: 0.000946
Step 1900/3000, Loss: 0.0197, LR: 0.000905
Step 2000/3000, Loss: 0.0088, LR: 0.000854
Step 2100/3000, Loss: 0.0104, LR: 0.000794
Step 2200/3000, Loss: 0.0132, LR: 0.000727
Step 2300/3000, Loss: 0.0046, LR: 0.000655
Step 2400/3000, Loss: 0.0011, LR: 0.000579
Step 2500/3000, Loss: 0.0009, LR: 0.000501
Step 2600/3000, Loss: 0.0006, LR: 0.000422
Step 2700/3000, Loss: 0.0005, LR: 0.000346
Step 2800/3000, Loss: 0.0005, LR: 0.000274
Step 2900/3000, Loss: 0.0004, LR: 0.000207
Step 3000/3000, Loss: 0.0007, LR: 0.000147
Training time: 10.92s (3.64ms per step)
Evaluating model...
Hard Accuracy: 0.8174
Soft Accuracy: 0.9458
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8174
Axiom (neutrality) Satisfaction Rate: 0.7944
Axiom (condorcet) Satisfaction Rate: 0.805
Axiom (pareto) Satisfaction Rate: 0.0494
Measuring inference time...
Average inference time (single sample): 0.2661ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.8174
  Soft Accuracy: 0.9458
  Training Time: 10.92s
  Inference Time: 0.2661ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 15,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 5.61s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2681, LR: 0.000905
Step 200/3000, Loss: 0.1911, LR: 0.000655
Step 300/3000, Loss: 0.1532, LR: 0.000346
Step 400/3000, Loss: 0.1272, LR: 0.000096
Step 500/3000, Loss: 0.1207, LR: 0.001000
Step 600/3000, Loss: 0.1125, LR: 0.000976
Step 700/3000, Loss: 0.0597, LR: 0.000905
Step 800/3000, Loss: 0.0327, LR: 0.000794
Step 900/3000, Loss: 0.0443, LR: 0.000655
Step 1000/3000, Loss: 0.0160, LR: 0.000501
Step 1100/3000, Loss: 0.0096, LR: 0.000346
Step 1200/3000, Loss: 0.0133, LR: 0.000207
Step 1300/3000, Loss: 0.0049, LR: 0.000096
Step 1400/3000, Loss: 0.0070, LR: 0.000025
Step 1500/3000, Loss: 0.0048, LR: 0.001000
Step 1600/3000, Loss: 0.0662, LR: 0.000994
Step 1700/3000, Loss: 0.0433, LR: 0.000976
Step 1800/3000, Loss: 0.0377, LR: 0.000946
Step 1900/3000, Loss: 0.0137, LR: 0.000905
Step 2000/3000, Loss: 0.0180, LR: 0.000854
Step 2100/3000, Loss: 0.0099, LR: 0.000794
Step 2200/3000, Loss: 0.0034, LR: 0.000727
Step 2300/3000, Loss: 0.0010, LR: 0.000655
Step 2400/3000, Loss: 0.0007, LR: 0.000579
Step 2500/3000, Loss: 0.0005, LR: 0.000501
Step 2600/3000, Loss: 0.0031, LR: 0.000422
Step 2700/3000, Loss: 0.0004, LR: 0.000346
Step 2800/3000, Loss: 0.0005, LR: 0.000274
Step 2900/3000, Loss: 0.0002, LR: 0.000207
Step 3000/3000, Loss: 0.0005, LR: 0.000147
Training time: 10.98s (3.66ms per step)
Evaluating model...
Hard Accuracy: 0.7508
Soft Accuracy: 0.9636
Evaluation time: 0.22s
Checking axiom satisfaction...
3754
Axiom (anonymity) Satisfaction Rate: 0.7508
3609
Axiom (neutrality) Satisfaction Rate: 0.7218
3986
Axiom (condorcet) Satisfaction Rate: 0.7972
223
Axiom (pareto) Satisfaction Rate: 0.0446
Measuring inference time...
Average inference time (single sample): 0.2306ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.7508
  Soft Accuracy: 0.9636
  Training Time: 10.98s
  Inference Time: 0.2306ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 50,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 41.02s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1846, LR: 0.000905
Step 200/3000, Loss: 0.1173, LR: 0.000655
Step 300/3000, Loss: 0.1222, LR: 0.000346
Step 400/3000, Loss: 0.1008, LR: 0.000096
Step 500/3000, Loss: 0.1341, LR: 0.001000
Step 600/3000, Loss: 0.1141, LR: 0.000976
Step 700/3000, Loss: 0.0822, LR: 0.000905
Step 800/3000, Loss: 0.0693, LR: 0.000794
Step 900/3000, Loss: 0.0808, LR: 0.000655
Step 1000/3000, Loss: 0.0617, LR: 0.000501
Step 1100/3000, Loss: 0.0620, LR: 0.000346
Step 1200/3000, Loss: 0.0586, LR: 0.000207
Step 1300/3000, Loss: 0.0447, LR: 0.000096
Step 1400/3000, Loss: 0.0724, LR: 0.000025
Step 1500/3000, Loss: 0.0963, LR: 0.001000
Step 1600/3000, Loss: 0.0726, LR: 0.000994
Step 1700/3000, Loss: 0.0509, LR: 0.000976
Step 1800/3000, Loss: 0.0702, LR: 0.000946
Step 1900/3000, Loss: 0.0727, LR: 0.000905
Step 2000/3000, Loss: 0.0605, LR: 0.000854
Step 2100/3000, Loss: 0.0487, LR: 0.000794
Step 2200/3000, Loss: 0.0466, LR: 0.000727
Step 2300/3000, Loss: 0.0664, LR: 0.000655
Step 2400/3000, Loss: 0.0398, LR: 0.000579
Step 2500/3000, Loss: 0.0544, LR: 0.000501
Step 2600/3000, Loss: 0.0558, LR: 0.000422
Step 2700/3000, Loss: 0.0419, LR: 0.000346
Step 2800/3000, Loss: 0.0347, LR: 0.000274
Step 2900/3000, Loss: 0.0494, LR: 0.000207
Step 3000/3000, Loss: 0.0561, LR: 0.000147
Training time: 9.10s (3.03ms per step)
Evaluating model...
Hard Accuracy: 0.922
Soft Accuracy: 0.9994
Evaluation time: 0.21s
Checking axiom satisfaction...
4610
Axiom (anonymity) Satisfaction Rate: 0.922
4422
Axiom (neutrality) Satisfaction Rate: 0.8844
4013
Axiom (condorcet) Satisfaction Rate: 0.8026
225
Axiom (pareto) Satisfaction Rate: 0.045
Measuring inference time...
Average inference time (single sample): 0.2341ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9220
  Soft Accuracy: 0.9994
  Training Time: 9.10s
  Inference Time: 0.2341ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 21.61s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1649, LR: 0.000905
Step 200/3000, Loss: 0.1331, LR: 0.000655
Step 300/3000, Loss: 0.1242, LR: 0.000346
Step 400/3000, Loss: 0.0873, LR: 0.000096
Step 500/3000, Loss: 0.0918, LR: 0.001000
Step 600/3000, Loss: 0.1343, LR: 0.000976
Step 700/3000, Loss: 0.1110, LR: 0.000905
Step 800/3000, Loss: 0.0954, LR: 0.000794
Step 900/3000, Loss: 0.0884, LR: 0.000655
Step 1000/3000, Loss: 0.0992, LR: 0.000501
Step 1100/3000, Loss: 0.0659, LR: 0.000346
Step 1200/3000, Loss: 0.0653, LR: 0.000207
Step 1300/3000, Loss: 0.0474, LR: 0.000096
Step 1400/3000, Loss: 0.0279, LR: 0.000025
Step 1500/3000, Loss: 0.0510, LR: 0.001000
Step 1600/3000, Loss: 0.1121, LR: 0.000994
Step 1700/3000, Loss: 0.0801, LR: 0.000976
Step 1800/3000, Loss: 0.0713, LR: 0.000946
Step 1900/3000, Loss: 0.0502, LR: 0.000905
Step 2000/3000, Loss: 0.0539, LR: 0.000854
Step 2100/3000, Loss: 0.0591, LR: 0.000794
Step 2200/3000, Loss: 0.0447, LR: 0.000727
Step 2300/3000, Loss: 0.0174, LR: 0.000655
Step 2400/3000, Loss: 0.0282, LR: 0.000579
Step 2500/3000, Loss: 0.0437, LR: 0.000501
Step 2600/3000, Loss: 0.0186, LR: 0.000422
Step 2700/3000, Loss: 0.0326, LR: 0.000346
Step 2800/3000, Loss: 0.0113, LR: 0.000274
Step 2900/3000, Loss: 0.0129, LR: 0.000207
Step 3000/3000, Loss: 0.0116, LR: 0.000147
Training time: 10.80s (3.60ms per step)
Evaluating model...
Hard Accuracy: 0.8632
Soft Accuracy: 0.9824
Evaluation time: 0.49s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8632
Axiom (neutrality) Satisfaction Rate: 0.8338
Axiom (condorcet) Satisfaction Rate: 0.8096
Axiom (pareto) Satisfaction Rate: 0.0432
Measuring inference time...
Average inference time (single sample): 0.2712ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.8632
  Soft Accuracy: 0.9824
  Training Time: 10.80s
  Inference Time: 0.2712ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 50,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 17.73s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2891, LR: 0.000905
Step 200/3000, Loss: 0.1762, LR: 0.000655
Step 300/3000, Loss: 0.1730, LR: 0.000346
Step 400/3000, Loss: 0.1358, LR: 0.000096
Step 500/3000, Loss: 0.1158, LR: 0.001000
Step 600/3000, Loss: 0.1202, LR: 0.000976
Step 700/3000, Loss: 0.1344, LR: 0.000905
Step 800/3000, Loss: 0.0989, LR: 0.000794
Step 900/3000, Loss: 0.0944, LR: 0.000655
Step 1000/3000, Loss: 0.0795, LR: 0.000501
Step 1100/3000, Loss: 0.0631, LR: 0.000346
Step 1200/3000, Loss: 0.0578, LR: 0.000207
Step 1300/3000, Loss: 0.0539, LR: 0.000096
Step 1400/3000, Loss: 0.0603, LR: 0.000025
Step 1500/3000, Loss: 0.0601, LR: 0.001000
Step 1600/3000, Loss: 0.0792, LR: 0.000994
Step 1700/3000, Loss: 0.1013, LR: 0.000976
Step 1800/3000, Loss: 0.0807, LR: 0.000946
Step 1900/3000, Loss: 0.0763, LR: 0.000905
Step 2000/3000, Loss: 0.0796, LR: 0.000854
Step 2100/3000, Loss: 0.0732, LR: 0.000794
Step 2200/3000, Loss: 0.0636, LR: 0.000727
Step 2300/3000, Loss: 0.0499, LR: 0.000655
Step 2400/3000, Loss: 0.0411, LR: 0.000579
Step 2500/3000, Loss: 0.0320, LR: 0.000501
Step 2600/3000, Loss: 0.0267, LR: 0.000422
Step 2700/3000, Loss: 0.0318, LR: 0.000346
Step 2800/3000, Loss: 0.0196, LR: 0.000274
Step 2900/3000, Loss: 0.0167, LR: 0.000207
Step 3000/3000, Loss: 0.0258, LR: 0.000147
Training time: 10.43s (3.48ms per step)
Evaluating model...
Hard Accuracy: 0.806
Soft Accuracy: 0.9626
Evaluation time: 0.22s
Checking axiom satisfaction...
4030
Axiom (anonymity) Satisfaction Rate: 0.806
3895
Axiom (neutrality) Satisfaction Rate: 0.779
4094
Axiom (condorcet) Satisfaction Rate: 0.8188
247
Axiom (pareto) Satisfaction Rate: 0.0494
Measuring inference time...
Average inference time (single sample): 0.2323ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8060
  Soft Accuracy: 0.9626
  Training Time: 10.43s
  Inference Time: 0.2323ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 150,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 118.37s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.2074, LR: 0.000905
Step 200/3000, Loss: 0.1566, LR: 0.000655
Step 300/3000, Loss: 0.0945, LR: 0.000346
Step 400/3000, Loss: 0.1154, LR: 0.000096
Step 500/3000, Loss: 0.0882, LR: 0.001000
Step 600/3000, Loss: 0.0928, LR: 0.000976
Step 700/3000, Loss: 0.0992, LR: 0.000905
Step 800/3000, Loss: 0.0780, LR: 0.000794
Step 900/3000, Loss: 0.0903, LR: 0.000655
Step 1000/3000, Loss: 0.0709, LR: 0.000501
Step 1100/3000, Loss: 0.0616, LR: 0.000346
Step 1200/3000, Loss: 0.0885, LR: 0.000207
Step 1300/3000, Loss: 0.0713, LR: 0.000096
Step 1400/3000, Loss: 0.0681, LR: 0.000025
Step 1500/3000, Loss: 0.0735, LR: 0.001000
Step 1600/3000, Loss: 0.0950, LR: 0.000994
Step 1700/3000, Loss: 0.0741, LR: 0.000976
Step 1800/3000, Loss: 0.0742, LR: 0.000946
Step 1900/3000, Loss: 0.0510, LR: 0.000905
Step 2000/3000, Loss: 0.0613, LR: 0.000854
Step 2100/3000, Loss: 0.0547, LR: 0.000794
Step 2200/3000, Loss: 0.0767, LR: 0.000727
Step 2300/3000, Loss: 0.0654, LR: 0.000655
Step 2400/3000, Loss: 0.0592, LR: 0.000579
Step 2500/3000, Loss: 0.0482, LR: 0.000501
Step 2600/3000, Loss: 0.0442, LR: 0.000422
Step 2700/3000, Loss: 0.0470, LR: 0.000346
Step 2800/3000, Loss: 0.0538, LR: 0.000274
Step 2900/3000, Loss: 0.0722, LR: 0.000207
Step 3000/3000, Loss: 0.0518, LR: 0.000147
Training time: 7.61s (2.54ms per step)
Evaluating model...
Hard Accuracy: 0.9258
Soft Accuracy: 0.999
Evaluation time: 0.22s
Checking axiom satisfaction...
4629
Axiom (anonymity) Satisfaction Rate: 0.9258
4447
Axiom (neutrality) Satisfaction Rate: 0.8894
4034
Axiom (condorcet) Satisfaction Rate: 0.8068
237
Axiom (pareto) Satisfaction Rate: 0.0474
Measuring inference time...
Average inference time (single sample): 0.2347ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9258
  Soft Accuracy: 0.9990
  Training Time: 7.61s
  Inference Time: 0.2347ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 64.60s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1314, LR: 0.000905
Step 200/3000, Loss: 0.1439, LR: 0.000655
Step 300/3000, Loss: 0.1336, LR: 0.000346
Step 400/3000, Loss: 0.1048, LR: 0.000096
Step 500/3000, Loss: 0.1178, LR: 0.001000
Step 600/3000, Loss: 0.1320, LR: 0.000976
Step 700/3000, Loss: 0.1120, LR: 0.000905
Step 800/3000, Loss: 0.0882, LR: 0.000794
Step 900/3000, Loss: 0.0897, LR: 0.000655
Step 1000/3000, Loss: 0.0739, LR: 0.000501
Step 1100/3000, Loss: 0.0732, LR: 0.000346
Step 1200/3000, Loss: 0.0818, LR: 0.000207
Step 1300/3000, Loss: 0.1009, LR: 0.000096
Step 1400/3000, Loss: 0.0964, LR: 0.000025
Step 1500/3000, Loss: 0.1008, LR: 0.001000
Step 1600/3000, Loss: 0.1022, LR: 0.000994
Step 1700/3000, Loss: 0.0779, LR: 0.000976
Step 1800/3000, Loss: 0.0953, LR: 0.000946
Step 1900/3000, Loss: 0.0838, LR: 0.000905
Step 2000/3000, Loss: 0.0742, LR: 0.000854
Step 2100/3000, Loss: 0.0709, LR: 0.000794
Step 2200/3000, Loss: 0.0671, LR: 0.000727
Step 2300/3000, Loss: 0.0616, LR: 0.000655
Step 2400/3000, Loss: 0.0648, LR: 0.000579
Step 2500/3000, Loss: 0.0637, LR: 0.000501
Step 2600/3000, Loss: 0.0433, LR: 0.000422
Step 2700/3000, Loss: 0.0501, LR: 0.000346
Step 2800/3000, Loss: 0.0330, LR: 0.000274
Step 2900/3000, Loss: 0.0407, LR: 0.000207
Step 3000/3000, Loss: 0.0387, LR: 0.000147
Training time: 10.15s (3.38ms per step)
Evaluating model...
Hard Accuracy: 0.9068
Soft Accuracy: 0.9894
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9068
Axiom (neutrality) Satisfaction Rate: 0.8658
Axiom (condorcet) Satisfaction Rate: 0.8092
Axiom (pareto) Satisfaction Rate: 0.0486
Measuring inference time...
Average inference time (single sample): 0.2680ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.9068
  Soft Accuracy: 0.9894
  Training Time: 10.15s
  Inference Time: 0.2680ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 150,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 53.61s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2639, LR: 0.000905
Step 200/3000, Loss: 0.1583, LR: 0.000655
Step 300/3000, Loss: 0.1490, LR: 0.000346
Step 400/3000, Loss: 0.1473, LR: 0.000096
Step 500/3000, Loss: 0.1252, LR: 0.001000
Step 600/3000, Loss: 0.1424, LR: 0.000976
Step 700/3000, Loss: 0.1268, LR: 0.000905
Step 800/3000, Loss: 0.1075, LR: 0.000794
Step 900/3000, Loss: 0.1282, LR: 0.000655
Step 1000/3000, Loss: 0.1114, LR: 0.000501
Step 1100/3000, Loss: 0.1045, LR: 0.000346
Step 1200/3000, Loss: 0.1175, LR: 0.000207
Step 1300/3000, Loss: 0.1259, LR: 0.000096
Step 1400/3000, Loss: 0.1174, LR: 0.000025
Step 1500/3000, Loss: 0.1007, LR: 0.001000
Step 1600/3000, Loss: 0.0999, LR: 0.000994
Step 1700/3000, Loss: 0.1117, LR: 0.000976
Step 1800/3000, Loss: 0.1229, LR: 0.000946
Step 1900/3000, Loss: 0.1083, LR: 0.000905
Step 2000/3000, Loss: 0.0948, LR: 0.000854
Step 2100/3000, Loss: 0.1208, LR: 0.000794
Step 2200/3000, Loss: 0.1208, LR: 0.000727
Step 2300/3000, Loss: 0.0874, LR: 0.000655
Step 2400/3000, Loss: 0.0997, LR: 0.000579
Step 2500/3000, Loss: 0.0831, LR: 0.000501
Step 2600/3000, Loss: 0.0839, LR: 0.000422
Step 2700/3000, Loss: 0.0834, LR: 0.000346
Step 2800/3000, Loss: 0.0754, LR: 0.000274
Step 2900/3000, Loss: 0.0971, LR: 0.000207
Step 3000/3000, Loss: 0.0888, LR: 0.000147
Training time: 10.54s (3.51ms per step)
Evaluating model...
Hard Accuracy: 0.8446
Soft Accuracy: 0.9682
Evaluation time: 0.22s
Checking axiom satisfaction...
4223
Axiom (anonymity) Satisfaction Rate: 0.8446
4124
Axiom (neutrality) Satisfaction Rate: 0.8248
4070
Axiom (condorcet) Satisfaction Rate: 0.814
229
Axiom (pareto) Satisfaction Rate: 0.0458
Measuring inference time...
Average inference time (single sample): 0.2319ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8446
  Soft Accuracy: 0.9682
  Training Time: 10.54s
  Inference Time: 0.2319ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 500,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 396.99s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1841, LR: 0.000905
Step 200/3000, Loss: 0.1507, LR: 0.000655
Step 300/3000, Loss: 0.1468, LR: 0.000346
Step 400/3000, Loss: 0.0976, LR: 0.000096
Step 500/3000, Loss: 0.1157, LR: 0.001000
Step 600/3000, Loss: 0.1184, LR: 0.000976
Step 700/3000, Loss: 0.0607, LR: 0.000905
Step 800/3000, Loss: 0.1122, LR: 0.000794
Step 900/3000, Loss: 0.1128, LR: 0.000655
Step 1000/3000, Loss: 0.0863, LR: 0.000501
Step 1100/3000, Loss: 0.0729, LR: 0.000346
Step 1200/3000, Loss: 0.0673, LR: 0.000207
Step 1300/3000, Loss: 0.0668, LR: 0.000096
Step 1400/3000, Loss: 0.0941, LR: 0.000025
Step 1500/3000, Loss: 0.0874, LR: 0.001000
Step 1600/3000, Loss: 0.0820, LR: 0.000994
Step 1700/3000, Loss: 0.0540, LR: 0.000976
Step 1800/3000, Loss: 0.0959, LR: 0.000946
Step 1900/3000, Loss: 0.0747, LR: 0.000905
Step 2000/3000, Loss: 0.0777, LR: 0.000854
Step 2100/3000, Loss: 0.0726, LR: 0.000794
Step 2200/3000, Loss: 0.0632, LR: 0.000727
Step 2300/3000, Loss: 0.0782, LR: 0.000655
Step 2400/3000, Loss: 0.0564, LR: 0.000579
Step 2500/3000, Loss: 0.0567, LR: 0.000501
Step 2600/3000, Loss: 0.0467, LR: 0.000422
Step 2700/3000, Loss: 0.0550, LR: 0.000346
Step 2800/3000, Loss: 0.0475, LR: 0.000274
Step 2900/3000, Loss: 0.0671, LR: 0.000207
Step 3000/3000, Loss: 0.0513, LR: 0.000147
Training time: 7.75s (2.58ms per step)
Evaluating model...
Hard Accuracy: 0.9274
Soft Accuracy: 0.9986
Evaluation time: 0.22s
Checking axiom satisfaction...
4637
Axiom (anonymity) Satisfaction Rate: 0.9274
4448
Axiom (neutrality) Satisfaction Rate: 0.8896
4007
Axiom (condorcet) Satisfaction Rate: 0.8014
233
Axiom (pareto) Satisfaction Rate: 0.0466
Measuring inference time...
Average inference time (single sample): 0.2335ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9274
  Soft Accuracy: 0.9986
  Training Time: 7.75s
  Inference Time: 0.2335ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 217.97s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1561, LR: 0.000905
Step 200/3000, Loss: 0.1204, LR: 0.000655
Step 300/3000, Loss: 0.1494, LR: 0.000346
Step 400/3000, Loss: 0.1013, LR: 0.000096
Step 500/3000, Loss: 0.1347, LR: 0.001000
Step 600/3000, Loss: 0.1224, LR: 0.000976
Step 700/3000, Loss: 0.1298, LR: 0.000905
Step 800/3000, Loss: 0.1130, LR: 0.000794
Step 900/3000, Loss: 0.1243, LR: 0.000655
Step 1000/3000, Loss: 0.1140, LR: 0.000501
Step 1100/3000, Loss: 0.0955, LR: 0.000346
Step 1200/3000, Loss: 0.1055, LR: 0.000207
Step 1300/3000, Loss: 0.0990, LR: 0.000096
Step 1400/3000, Loss: 0.1060, LR: 0.000025
Step 1500/3000, Loss: 0.0810, LR: 0.001000
Step 1600/3000, Loss: 0.1487, LR: 0.000994
Step 1700/3000, Loss: 0.1001, LR: 0.000976
Step 1800/3000, Loss: 0.0939, LR: 0.000946
Step 1900/3000, Loss: 0.0680, LR: 0.000905
Step 2000/3000, Loss: 0.0912, LR: 0.000854
Step 2100/3000, Loss: 0.0683, LR: 0.000794
Step 2200/3000, Loss: 0.0820, LR: 0.000727
Step 2300/3000, Loss: 0.0827, LR: 0.000655
Step 2400/3000, Loss: 0.0840, LR: 0.000579
Step 2500/3000, Loss: 0.0938, LR: 0.000501
Step 2600/3000, Loss: 0.0636, LR: 0.000422
Step 2700/3000, Loss: 0.0510, LR: 0.000346
Step 2800/3000, Loss: 0.0425, LR: 0.000274
Step 2900/3000, Loss: 0.0952, LR: 0.000207
Step 3000/3000, Loss: 0.0748, LR: 0.000147
Training time: 9.36s (3.12ms per step)
Evaluating model...
Hard Accuracy: 0.9106
Soft Accuracy: 0.994
Evaluation time: 0.83s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9106
Axiom (neutrality) Satisfaction Rate: 0.874
Axiom (condorcet) Satisfaction Rate: 0.812
Axiom (pareto) Satisfaction Rate: 0.0452
Measuring inference time...
Average inference time (single sample): 0.2696ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.9106
  Soft Accuracy: 0.9940
  Training Time: 9.36s
  Inference Time: 0.2696ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 500,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 178.43s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.3000, LR: 0.000905
Step 200/3000, Loss: 0.1952, LR: 0.000655
Step 300/3000, Loss: 0.1803, LR: 0.000346
Step 400/3000, Loss: 0.1471, LR: 0.000096
Step 500/3000, Loss: 0.1676, LR: 0.001000
Step 600/3000, Loss: 0.1540, LR: 0.000976
Step 700/3000, Loss: 0.1459, LR: 0.000905
Step 800/3000, Loss: 0.1366, LR: 0.000794
Step 900/3000, Loss: 0.1113, LR: 0.000655
Step 1000/3000, Loss: 0.1457, LR: 0.000501
Step 1100/3000, Loss: 0.0980, LR: 0.000346
Step 1200/3000, Loss: 0.0990, LR: 0.000207
Step 1300/3000, Loss: 0.0792, LR: 0.000096
Step 1400/3000, Loss: 0.1296, LR: 0.000025
Step 1500/3000, Loss: 0.0743, LR: 0.001000
Step 1600/3000, Loss: 0.1480, LR: 0.000994
Step 1700/3000, Loss: 0.1170, LR: 0.000976
Step 1800/3000, Loss: 0.0911, LR: 0.000946
Step 1900/3000, Loss: 0.1251, LR: 0.000905
Step 2000/3000, Loss: 0.1025, LR: 0.000854
Step 2100/3000, Loss: 0.0947, LR: 0.000794
Step 2200/3000, Loss: 0.1006, LR: 0.000727
Step 2300/3000, Loss: 0.1278, LR: 0.000655
Step 2400/3000, Loss: 0.1160, LR: 0.000579
Step 2500/3000, Loss: 0.0952, LR: 0.000501
Step 2600/3000, Loss: 0.0676, LR: 0.000422
Step 2700/3000, Loss: 0.0804, LR: 0.000346
Step 2800/3000, Loss: 0.0858, LR: 0.000274
Step 2900/3000, Loss: 0.0909, LR: 0.000207
Step 3000/3000, Loss: 0.0856, LR: 0.000147
Training time: 10.50s (3.50ms per step)
Evaluating model...
Hard Accuracy: 0.8454
Soft Accuracy: 0.9728
Evaluation time: 0.72s
Checking axiom satisfaction...
4227
Axiom (anonymity) Satisfaction Rate: 0.8454
4107
Axiom (neutrality) Satisfaction Rate: 0.8214
4045
Axiom (condorcet) Satisfaction Rate: 0.809
250
Axiom (pareto) Satisfaction Rate: 0.05
Measuring inference time...
Average inference time (single sample): 0.2288ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8454
  Soft Accuracy: 0.9728
  Training Time: 10.50s
  Inference Time: 0.2288ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 1,000,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 788.81s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training model for 3000 steps...
Step 100/3000, Loss: 0.1778, LR: 0.000905
Step 200/3000, Loss: 0.1268, LR: 0.000655
Step 300/3000, Loss: 0.1503, LR: 0.000346
Step 400/3000, Loss: 0.1072, LR: 0.000096
Step 500/3000, Loss: 0.0867, LR: 0.001000
Step 600/3000, Loss: 0.1145, LR: 0.000976
Step 700/3000, Loss: 0.1010, LR: 0.000905
Step 800/3000, Loss: 0.0782, LR: 0.000794
Step 900/3000, Loss: 0.0914, LR: 0.000655
Step 1000/3000, Loss: 0.0812, LR: 0.000501
Step 1100/3000, Loss: 0.0959, LR: 0.000346
Step 1200/3000, Loss: 0.0454, LR: 0.000207
Step 1300/3000, Loss: 0.1019, LR: 0.000096
Step 1400/3000, Loss: 0.0624, LR: 0.000025
Step 1500/3000, Loss: 0.0511, LR: 0.001000
Step 1600/3000, Loss: 0.0978, LR: 0.000994
Step 1700/3000, Loss: 0.0848, LR: 0.000976
Step 1800/3000, Loss: 0.0924, LR: 0.000946
Step 1900/3000, Loss: 0.0542, LR: 0.000905
Step 2000/3000, Loss: 0.0788, LR: 0.000854
Step 2100/3000, Loss: 0.0533, LR: 0.000794
Step 2200/3000, Loss: 0.0635, LR: 0.000727
Step 2300/3000, Loss: 0.0684, LR: 0.000655
Step 2400/3000, Loss: 0.0398, LR: 0.000579
Step 2500/3000, Loss: 0.0558, LR: 0.000501
Step 2600/3000, Loss: 0.0494, LR: 0.000422
Step 2700/3000, Loss: 0.0500, LR: 0.000346
Step 2800/3000, Loss: 0.0572, LR: 0.000274
Step 2900/3000, Loss: 0.0336, LR: 0.000207
Step 3000/3000, Loss: 0.0674, LR: 0.000147
Training time: 7.89s (2.63ms per step)
Evaluating model...
Hard Accuracy: 0.9258
Soft Accuracy: 0.9978
Evaluation time: 0.22s
Checking axiom satisfaction...
4629
Axiom (anonymity) Satisfaction Rate: 0.9258
4445
Axiom (neutrality) Satisfaction Rate: 0.889
3999
Axiom (condorcet) Satisfaction Rate: 0.7998
248
Axiom (pareto) Satisfaction Rate: 0.0496
Measuring inference time...
Average inference time (single sample): 0.2366ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9258
  Soft Accuracy: 0.9978
  Training Time: 7.89s
  Inference Time: 0.2366ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 431.51s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training model for 3000 steps...
Step 100/3000, Loss: 0.1577, LR: 0.000905
Step 200/3000, Loss: 0.1371, LR: 0.000655
Step 300/3000, Loss: 0.1156, LR: 0.000346
Step 400/3000, Loss: 0.1145, LR: 0.000096
Step 500/3000, Loss: 0.1079, LR: 0.001000
Step 600/3000, Loss: 0.1485, LR: 0.000976
Step 700/3000, Loss: 0.1203, LR: 0.000905
Step 800/3000, Loss: 0.1154, LR: 0.000794
Step 900/3000, Loss: 0.1135, LR: 0.000655
Step 1000/3000, Loss: 0.0999, LR: 0.000501
Step 1100/3000, Loss: 0.1000, LR: 0.000346
Step 1200/3000, Loss: 0.0991, LR: 0.000207
Step 1300/3000, Loss: 0.0612, LR: 0.000096
Step 1400/3000, Loss: 0.0979, LR: 0.000025
Step 1500/3000, Loss: 0.0864, LR: 0.001000
Step 1600/3000, Loss: 0.1004, LR: 0.000994
Step 1700/3000, Loss: 0.0953, LR: 0.000976
Step 1800/3000, Loss: 0.1089, LR: 0.000946
Step 1900/3000, Loss: 0.0756, LR: 0.000905
Step 2000/3000, Loss: 0.0784, LR: 0.000854
Step 2100/3000, Loss: 0.0880, LR: 0.000794
Step 2200/3000, Loss: 0.0635, LR: 0.000727
Step 2300/3000, Loss: 0.0663, LR: 0.000655
Step 2400/3000, Loss: 0.0664, LR: 0.000579
Step 2500/3000, Loss: 0.0495, LR: 0.000501
Step 2600/3000, Loss: 0.0687, LR: 0.000422
Step 2700/3000, Loss: 0.0684, LR: 0.000346
Step 2800/3000, Loss: 0.0650, LR: 0.000274
Step 2900/3000, Loss: 0.0797, LR: 0.000207
Step 3000/3000, Loss: 0.0719, LR: 0.000147
Training time: 9.12s (3.04ms per step)
Evaluating model...
Hard Accuracy: 0.9062
Soft Accuracy: 0.9898
Evaluation time: 0.23s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9062
Axiom (neutrality) Satisfaction Rate: 0.8702
Axiom (condorcet) Satisfaction Rate: 0.8062
Axiom (pareto) Satisfaction Rate: 0.047
Measuring inference time...
Average inference time (single sample): 0.2668ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.9062
  Soft Accuracy: 0.9898
  Training Time: 9.12s
  Inference Time: 0.2668ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 1,000,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 362.92s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training model for 3000 steps...
Step 100/3000, Loss: 0.2710, LR: 0.000905
Step 200/3000, Loss: 0.1799, LR: 0.000655
Step 300/3000, Loss: 0.1802, LR: 0.000346
Step 400/3000, Loss: 0.1573, LR: 0.000096
Step 500/3000, Loss: 0.1407, LR: 0.001000
Step 600/3000, Loss: 0.1515, LR: 0.000976
Step 700/3000, Loss: 0.1141, LR: 0.000905
Step 800/3000, Loss: 0.1205, LR: 0.000794
Step 900/3000, Loss: 0.1318, LR: 0.000655
Step 1000/3000, Loss: 0.1116, LR: 0.000501
Step 1100/3000, Loss: 0.1219, LR: 0.000346
Step 1200/3000, Loss: 0.1014, LR: 0.000207
Step 1300/3000, Loss: 0.0714, LR: 0.000096
Step 1400/3000, Loss: 0.1206, LR: 0.000025
Step 1500/3000, Loss: 0.1029, LR: 0.001000
Step 1600/3000, Loss: 0.1089, LR: 0.000994
Step 1700/3000, Loss: 0.1005, LR: 0.000976
Step 1800/3000, Loss: 0.1342, LR: 0.000946
Step 1900/3000, Loss: 0.1118, LR: 0.000905
Step 2000/3000, Loss: 0.1107, LR: 0.000854
Step 2100/3000, Loss: 0.1193, LR: 0.000794
Step 2200/3000, Loss: 0.0880, LR: 0.000727
Step 2300/3000, Loss: 0.1290, LR: 0.000655
Step 2400/3000, Loss: 0.0994, LR: 0.000579
Step 2500/3000, Loss: 0.0823, LR: 0.000501
Step 2600/3000, Loss: 0.1091, LR: 0.000422
Step 2700/3000, Loss: 0.0984, LR: 0.000346
Step 2800/3000, Loss: 0.0906, LR: 0.000274
Step 2900/3000, Loss: 0.0984, LR: 0.000207
Step 3000/3000, Loss: 0.0884, LR: 0.000147
Training time: 8.80s (2.93ms per step)
Evaluating model...
Hard Accuracy: 0.8392
Soft Accuracy: 0.97
Evaluation time: 0.22s
Checking axiom satisfaction...
4196
Axiom (anonymity) Satisfaction Rate: 0.8392
4090
Axiom (neutrality) Satisfaction Rate: 0.818
4066
Axiom (condorcet) Satisfaction Rate: 0.8132
253
Axiom (pareto) Satisfaction Rate: 0.0506
Measuring inference time...
Average inference time (single sample): 0.2295ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8392
  Soft Accuracy: 0.9700
  Training Time: 8.80s
  Inference Time: 0.2295ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

====================================================================================================
COMPREHENSIVE BENCHMARK SUMMARY
====================================================================================================


====================================================================================================
RESULTS FOR BORDA VOTING METHOD
====================================================================================================


====================================================================================================
Dataset Size: 1,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 5,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 15,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 50,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 150,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 500,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 1,000,000 samples
====================================================================================================

Metric                             Pairwise            Pairwise Per Voter  Onehot              
-----------------------------------------------------------------------------------------------
Input Size                         10                  550                 1375                
Number of Parameters               236,293             241,669             226,309             
Hard Accuracy                      0.9806              0.9852              0.9734              
Soft Accuracy                      1.0000              1.0000              0.9968              
Training Time (s)                  7.84                11.31               11.01               
Time per Step (ms)                 2.61                3.77                3.67                
Inference Time (ms)                0.2376              0.2676              0.2311              

Axiom Satisfaction Rates           
-----------------------------------------------------------------------------------------------
Anonymity                          0.9806              0.9852              0.9734              
Neutrality                         0.9576              0.9534              0.9414              
Condorcet                          0.7170              0.7194              0.7096              
Pareto                             0.0432              0.0490              0.0466              

Improvement vs One-hot (%)         
-----------------------------------------------------------------------------------------------
Input Size                                       +99.27              +60.000.00                
Num Parameters                                    -4.41               -6.790.00                
Hard Accuracy                                     +0.74               +1.210.00                
Soft Accuracy                                     +0.32               +0.320.00                
Training Time                                    +28.72               -2.780.00                
Inference Time                                    -2.83              -15.800.00                

====================================================================================================
RESULTS FOR PLURALITY VOTING METHOD
====================================================================================================


====================================================================================================
Dataset Size: 1,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 5,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 15,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 50,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 150,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 500,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 1,000,000 samples
====================================================================================================

Metric                             Pairwise            Pairwise Per Voter  Onehot              
-----------------------------------------------------------------------------------------------
Input Size                         10                  550                 1375                
Number of Parameters               236,293             241,669             226,309             
Hard Accuracy                      0.7318              0.7740              0.9944              
Soft Accuracy                      0.8586              0.8774              1.0000              
Training Time (s)                  8.39                11.23               11.05               
Time per Step (ms)                 2.80                3.74                3.68                
Inference Time (ms)                0.2322              0.2682              0.2329              

Axiom Satisfaction Rates           
-----------------------------------------------------------------------------------------------
Anonymity                          0.7318              0.7740              0.9944              
Neutrality                         0.7260              0.7466              0.9202              
Condorcet                          0.5532              0.5364              0.5574              
Pareto                             0.0482              0.0430              0.0520              

Improvement vs One-hot (%)         
-----------------------------------------------------------------------------------------------
Input Size                                       +99.27              +60.000.00                
Num Parameters                                    -4.41               -6.790.00                
Hard Accuracy                                    -26.41              -22.160.00                
Soft Accuracy                                    -14.14              -12.260.00                
Training Time                                    +24.08               -1.640.00                
Inference Time                                    +0.27              -15.150.00                

====================================================================================================
RESULTS FOR COPELAND VOTING METHOD
====================================================================================================


====================================================================================================
Dataset Size: 1,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 5,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 15,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 50,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 150,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 500,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 1,000,000 samples
====================================================================================================

Metric                             Pairwise            Pairwise Per Voter  Onehot              
-----------------------------------------------------------------------------------------------
Input Size                         10                  550                 1375                
Number of Parameters               236,293             241,669             226,309             
Hard Accuracy                      0.9258              0.9062              0.8392              
Soft Accuracy                      0.9978              0.9898              0.9700              
Training Time (s)                  7.89                9.12                8.80                
Time per Step (ms)                 2.63                3.04                2.93                
Inference Time (ms)                0.2366              0.2668              0.2295              

Axiom Satisfaction Rates           
-----------------------------------------------------------------------------------------------
Anonymity                          0.9258              0.9062              0.8392              
Neutrality                         0.8890              0.8702              0.8180              
Condorcet                          0.7998              0.8062              0.8132              
Pareto                             0.0496              0.0470              0.0506              

Improvement vs One-hot (%)         
-----------------------------------------------------------------------------------------------
Input Size                                       +99.27              +60.000.00                
Num Parameters                                    -4.41               -6.790.00                
Hard Accuracy                                    +10.32               +7.980.00                
Soft Accuracy                                     +2.87               +2.040.00                
Training Time                                    +10.31               -3.610.00                
Inference Time                                    -3.11              -16.280.00                


====================================================================================================
OVERALL COMPARISON - HARD ACCURACY BY DATASET SIZE
====================================================================================================


BORDA Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.9230              0.7320              0.5920              
5,000               0.9666              0.8444              0.7422              
15,000              0.9750              0.9058              0.8302              
50,000              0.9766              0.9602              0.9264              
150,000             0.9772              0.9798              0.9618              
500,000             0.9782              0.9856              0.9698              
1,000,000           0.9806              0.9852              0.9734              

PLURALITY Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.6668              0.6066              0.5936              
5,000               0.6944              0.6708              0.7724              
15,000              0.7226              0.7058              0.8740              
50,000              0.7296              0.7436              0.9606              
150,000             0.7282              0.7638              0.9884              
500,000             0.7152              0.7682              0.9954              
1,000,000           0.7318              0.7740              0.9944              

COPELAND Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.8468              0.6930              0.5750              
5,000               0.8992              0.7734              0.6926              
15,000              0.9184              0.8174              0.7508              
50,000              0.9220              0.8632              0.8060              
150,000             0.9258              0.9068              0.8446              
500,000             0.9274              0.9106              0.8454              
1,000,000           0.9258              0.9062              0.8392              


====================================================================================================
OVERALL COMPARISON - SOFT ACCURACY BY DATASET SIZE
====================================================================================================


BORDA Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.9968              0.8070              0.7786              
5,000               1.0000              0.9166              0.9150              
15,000              1.0000              0.9716              0.9690              
50,000              1.0000              0.9952              0.9892              
150,000             1.0000              0.9996              0.9948              
500,000             1.0000              0.9994              0.9962              
1,000,000           1.0000              1.0000              0.9968              

PLURALITY Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.8398              0.7516              0.8866              
5,000               0.8438              0.8180              0.9630              
15,000              0.8628              0.8564              0.9938              
50,000              0.8552              0.8864              0.9990              
150,000             0.8578              0.8850              1.0000              
500,000             0.8450              0.8770              1.0000              
1,000,000           0.8586              0.8774              1.0000              

COPELAND Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.9798              0.8220              0.8162              
5,000               0.9942              0.9118              0.9150              
15,000              0.9972              0.9458              0.9636              
50,000              0.9994              0.9824              0.9626              
150,000             0.9990              0.9894              0.9682              
500,000             0.9986              0.9940              0.9728              
1,000,000           0.9978              0.9898              0.9700              


====================================================================================================
OVERALL COMPARISON - TRAINING TIME (s) BY DATASET SIZE
====================================================================================================


BORDA Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               8.35                11.14               8.53                
5,000               7.54                8.48                10.89               
15,000              7.70                10.22               10.49               
50,000              7.57                11.14               10.57               
150,000             16.27               13.82               14.90               
500,000             21.65               12.03               11.73               
1,000,000           7.84                11.31               11.01               

PLURALITY Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               7.63                8.80                10.06               
5,000               7.53                8.43                9.06                
15,000              7.50                10.29               8.58                
50,000              7.72                14.91               12.57               
150,000             9.13                8.78                11.59               
500,000             8.00                12.45               10.97               
1,000,000           8.39                11.23               11.05               

COPELAND Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               7.48                10.26               10.34               
5,000               7.48                10.25               9.97                
15,000              7.50                10.92               10.98               
50,000              9.10                10.80               10.43               
150,000             7.61                10.15               10.54               
500,000             7.75                9.36                10.50               
1,000,000           7.89                9.12                8.80                


Detailed results saved to benchmark_results_full.json

Creating summary CSV files...
Summary CSV files created: benchmark_hard_accuracy.csv, benchmark_training_time.csv
