nohup: ignoring input
Using device: cuda
GPU: NVIDIA A40
CUDA Version: 11.8
Starting comprehensive benchmark comparison

Testing with 3 voting methods: borda, plurality, copeland
Testing with 3 encoding types: pairwise, pairwise_per_voter, onehot
Testing with 7 dataset sizes: 1,000, 5,000, 15,000, 50,000, 150,000, 500,000, 1,000,000


################################################################################
# Testing with BORDA voting method
################################################################################

--------------------------------------------------------------------------------
Dataset size: 1,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 3.64s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.20s (13.04ms per step)
Evaluating model...
Hard Accuracy: 0.0
Soft Accuracy: 0.0
Evaluation time: 8.71s
Checking axiom satisfaction...
0
Axiom (anonymity) Satisfaction Rate: 0.0
0
Axiom (neutrality) Satisfaction Rate: 0.0
3603
Axiom (condorcet) Satisfaction Rate: 0.7206
250
Axiom (pareto) Satisfaction Rate: 0.05
Measuring inference time...
Average inference time (single sample): 0.2694ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.0000
  Soft Accuracy: 0.0000
  Training Time: 0.20s
  Inference Time: 0.2694ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 0.43s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.08s (5.34ms per step)
Evaluating model...
Hard Accuracy: 0.3942
Soft Accuracy: 0.5478
Evaluation time: 10.38s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.3942
Axiom (neutrality) Satisfaction Rate: 0.3872
Axiom (condorcet) Satisfaction Rate: 0.7174
Axiom (pareto) Satisfaction Rate: 0.0472
Measuring inference time...
Average inference time (single sample): 0.2898ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.3942
  Soft Accuracy: 0.5478
  Training Time: 0.08s
  Inference Time: 0.2898ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 1,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 0.35s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.09s (5.72ms per step)
Evaluating model...
Hard Accuracy: 0.0
Soft Accuracy: 0.0
Evaluation time: 12.60s
Checking axiom satisfaction...
0
Axiom (anonymity) Satisfaction Rate: 0.0
0
Axiom (neutrality) Satisfaction Rate: 0.0
3579
Axiom (condorcet) Satisfaction Rate: 0.7158
240
Axiom (pareto) Satisfaction Rate: 0.048
Measuring inference time...
Average inference time (single sample): 0.2591ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.0000
  Soft Accuracy: 0.0000
  Training Time: 0.09s
  Inference Time: 0.2591ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 5,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 4.24s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.48s (6.38ms per step)
Evaluating model...
Hard Accuracy: 0.7994
Soft Accuracy: 0.8862
Evaluation time: 9.66s
Checking axiom satisfaction...
3997
Axiom (anonymity) Satisfaction Rate: 0.7994
3976
Axiom (neutrality) Satisfaction Rate: 0.7952
3631
Axiom (condorcet) Satisfaction Rate: 0.7262
243
Axiom (pareto) Satisfaction Rate: 0.0486
Measuring inference time...
Average inference time (single sample): 0.2523ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.7994
  Soft Accuracy: 0.8862
  Training Time: 0.48s
  Inference Time: 0.2523ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 2.16s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.39s (5.25ms per step)
Evaluating model...
Hard Accuracy: 0.7954
Soft Accuracy: 0.894
Evaluation time: 7.21s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7954
Axiom (neutrality) Satisfaction Rate: 0.7914
Axiom (condorcet) Satisfaction Rate: 0.7162
Axiom (pareto) Satisfaction Rate: 0.051
Measuring inference time...
Average inference time (single sample): 0.2773ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.7954
  Soft Accuracy: 0.8940
  Training Time: 0.39s
  Inference Time: 0.2773ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 5,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 1.92s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.25s (3.39ms per step)
Evaluating model...
Hard Accuracy: 0.458
Soft Accuracy: 0.4892
Evaluation time: 2.20s
Checking axiom satisfaction...
2290
Axiom (anonymity) Satisfaction Rate: 0.458
2290
Axiom (neutrality) Satisfaction Rate: 0.458
3572
Axiom (condorcet) Satisfaction Rate: 0.7144
237
Axiom (pareto) Satisfaction Rate: 0.0474
Measuring inference time...
Average inference time (single sample): 0.2442ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.4580
  Soft Accuracy: 0.4892
  Training Time: 0.25s
  Inference Time: 0.2442ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 15,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 11.87s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.1127, LR: 0.000905
Step 200/225, Loss: 0.0438, LR: 0.000655
Training time: 0.67s (3.00ms per step)
Evaluating model...
Hard Accuracy: 0.934
Soft Accuracy: 0.993
Evaluation time: 2.20s
Checking axiom satisfaction...
4670
Axiom (anonymity) Satisfaction Rate: 0.934
4600
Axiom (neutrality) Satisfaction Rate: 0.92
3597
Axiom (condorcet) Satisfaction Rate: 0.7194
243
Axiom (pareto) Satisfaction Rate: 0.0486
Measuring inference time...
Average inference time (single sample): 0.2438ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9340
  Soft Accuracy: 0.9930
  Training Time: 0.67s
  Inference Time: 0.2438ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 6.55s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.0927, LR: 0.000905
Step 200/225, Loss: 0.0584, LR: 0.000655
Training time: 0.85s (3.80ms per step)
Evaluating model...
Hard Accuracy: 0.8896
Soft Accuracy: 0.9592
Evaluation time: 3.88s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8896
Axiom (neutrality) Satisfaction Rate: 0.8768
Axiom (condorcet) Satisfaction Rate: 0.724
Axiom (pareto) Satisfaction Rate: 0.0506
Measuring inference time...
Average inference time (single sample): 0.2779ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.8896
  Soft Accuracy: 0.9592
  Training Time: 0.85s
  Inference Time: 0.2779ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 15,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 5.76s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.2501, LR: 0.000905
Step 200/225, Loss: 0.1666, LR: 0.000655
Training time: 1.03s (4.56ms per step)
Evaluating model...
Hard Accuracy: 0.758
Soft Accuracy: 0.8082
Evaluation time: 4.94s
Checking axiom satisfaction...
3790
Axiom (anonymity) Satisfaction Rate: 0.758
3727
Axiom (neutrality) Satisfaction Rate: 0.7454
3508
Axiom (condorcet) Satisfaction Rate: 0.7016
231
Axiom (pareto) Satisfaction Rate: 0.0462
Measuring inference time...
Average inference time (single sample): 0.2357ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.7580
  Soft Accuracy: 0.8082
  Training Time: 1.03s
  Inference Time: 0.2357ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 50,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 40.47s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.1193, LR: 0.000905
Step 200/750, Loss: 0.0384, LR: 0.000655
Step 300/750, Loss: 0.0234, LR: 0.000346
Step 400/750, Loss: 0.0284, LR: 0.000096
Step 500/750, Loss: 0.0373, LR: 0.001000
Step 600/750, Loss: 0.0316, LR: 0.000976
Step 700/750, Loss: 0.0287, LR: 0.000905
Training time: 2.23s (2.97ms per step)
Evaluating model...
Hard Accuracy: 0.9574
Soft Accuracy: 0.9996
Evaluation time: 2.19s
Checking axiom satisfaction...
4787
Axiom (anonymity) Satisfaction Rate: 0.9574
4703
Axiom (neutrality) Satisfaction Rate: 0.9406
3536
Axiom (condorcet) Satisfaction Rate: 0.7072
242
Axiom (pareto) Satisfaction Rate: 0.0484
Measuring inference time...
Average inference time (single sample): 0.2441ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9574
  Soft Accuracy: 0.9996
  Training Time: 2.23s
  Inference Time: 0.2441ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 22.00s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.1321, LR: 0.000905
Step 200/750, Loss: 0.0805, LR: 0.000655
Step 300/750, Loss: 0.0578, LR: 0.000346
Step 400/750, Loss: 0.0338, LR: 0.000096
Step 500/750, Loss: 0.0301, LR: 0.001000
Step 600/750, Loss: 0.0492, LR: 0.000976
Step 700/750, Loss: 0.0457, LR: 0.000905
Training time: 2.28s (3.04ms per step)
Evaluating model...
Hard Accuracy: 0.9074
Soft Accuracy: 0.9752
Evaluation time: 2.19s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9074
Axiom (neutrality) Satisfaction Rate: 0.8902
Axiom (condorcet) Satisfaction Rate: 0.7068
Axiom (pareto) Satisfaction Rate: 0.0542
Measuring inference time...
Average inference time (single sample): 0.2768ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9074
  Soft Accuracy: 0.9752
  Training Time: 2.28s
  Inference Time: 0.2768ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 50,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 22.02s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.2617, LR: 0.000905
Step 200/750, Loss: 0.1636, LR: 0.000655
Step 300/750, Loss: 0.1138, LR: 0.000346
Step 400/750, Loss: 0.1212, LR: 0.000096
Step 500/750, Loss: 0.1006, LR: 0.001000
Step 600/750, Loss: 0.0781, LR: 0.000976
Step 700/750, Loss: 0.0836, LR: 0.000905
Training time: 3.47s (4.63ms per step)
Evaluating model...
Hard Accuracy: 0.8928
Soft Accuracy: 0.9556
Evaluation time: 2.19s
Checking axiom satisfaction...
4464
Axiom (anonymity) Satisfaction Rate: 0.8928
4399
Axiom (neutrality) Satisfaction Rate: 0.8798
3600
Axiom (condorcet) Satisfaction Rate: 0.72
239
Axiom (pareto) Satisfaction Rate: 0.0478
Measuring inference time...
Average inference time (single sample): 0.2414ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.8928
  Soft Accuracy: 0.9556
  Training Time: 3.47s
  Inference Time: 0.2414ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 150,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 121.28s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.0731, LR: 0.000905
Step 200/2250, Loss: 0.0474, LR: 0.000655
Step 300/2250, Loss: 0.0314, LR: 0.000346
Step 400/2250, Loss: 0.0337, LR: 0.000096
Step 500/2250, Loss: 0.0257, LR: 0.001000
Step 600/2250, Loss: 0.0460, LR: 0.000976
Step 700/2250, Loss: 0.0240, LR: 0.000905
Step 800/2250, Loss: 0.0192, LR: 0.000794
Step 900/2250, Loss: 0.0176, LR: 0.000655
Step 1000/2250, Loss: 0.0226, LR: 0.000501
Step 1100/2250, Loss: 0.0204, LR: 0.000346
Step 1200/2250, Loss: 0.0160, LR: 0.000207
Step 1300/2250, Loss: 0.0155, LR: 0.000096
Step 1400/2250, Loss: 0.0139, LR: 0.000025
Step 1500/2250, Loss: 0.0205, LR: 0.001000
Step 1600/2250, Loss: 0.0594, LR: 0.000994
Step 1700/2250, Loss: 0.0388, LR: 0.000976
Step 1800/2250, Loss: 0.0153, LR: 0.000946
Step 1900/2250, Loss: 0.0129, LR: 0.000905
Step 2000/2250, Loss: 0.0178, LR: 0.000854
Step 2100/2250, Loss: 0.0173, LR: 0.000794
Step 2200/2250, Loss: 0.0234, LR: 0.000727
Training time: 6.79s (3.02ms per step)
Evaluating model...
Hard Accuracy: 0.9728
Soft Accuracy: 1.0
Evaluation time: 2.20s
Checking axiom satisfaction...
4864
Axiom (anonymity) Satisfaction Rate: 0.9728
4746
Axiom (neutrality) Satisfaction Rate: 0.9492
3491
Axiom (condorcet) Satisfaction Rate: 0.6982
249
Axiom (pareto) Satisfaction Rate: 0.0498
Measuring inference time...
Average inference time (single sample): 0.2376ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9728
  Soft Accuracy: 1.0000
  Training Time: 6.79s
  Inference Time: 0.2376ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 67.18s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.1210, LR: 0.000905
Step 200/2250, Loss: 0.0886, LR: 0.000655
Step 300/2250, Loss: 0.0544, LR: 0.000346
Step 400/2250, Loss: 0.0349, LR: 0.000096
Step 500/2250, Loss: 0.0278, LR: 0.001000
Step 600/2250, Loss: 0.0647, LR: 0.000976
Step 700/2250, Loss: 0.0590, LR: 0.000905
Step 800/2250, Loss: 0.0501, LR: 0.000794
Step 900/2250, Loss: 0.0354, LR: 0.000655
Step 1000/2250, Loss: 0.0367, LR: 0.000501
Step 1100/2250, Loss: 0.0275, LR: 0.000346
Step 1200/2250, Loss: 0.0133, LR: 0.000207
Step 1300/2250, Loss: 0.0182, LR: 0.000096
Step 1400/2250, Loss: 0.0144, LR: 0.000025
Step 1500/2250, Loss: 0.0142, LR: 0.001000
Step 1600/2250, Loss: 0.0533, LR: 0.000994
Step 1700/2250, Loss: 0.0455, LR: 0.000976
Step 1800/2250, Loss: 0.0361, LR: 0.000946
Step 1900/2250, Loss: 0.0359, LR: 0.000905
Step 2000/2250, Loss: 0.0302, LR: 0.000854
Step 2100/2250, Loss: 0.0380, LR: 0.000794
Step 2200/2250, Loss: 0.0193, LR: 0.000727
Training time: 7.33s (3.26ms per step)
Evaluating model...
Hard Accuracy: 0.9502
Soft Accuracy: 0.9936
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9502
Axiom (neutrality) Satisfaction Rate: 0.9278
Axiom (condorcet) Satisfaction Rate: 0.7128
Axiom (pareto) Satisfaction Rate: 0.0438
Measuring inference time...
Average inference time (single sample): 0.2784ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9502
  Soft Accuracy: 0.9936
  Training Time: 7.33s
  Inference Time: 0.2784ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 150,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 57.11s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.2703, LR: 0.000905
Step 200/2250, Loss: 0.1744, LR: 0.000655
Step 300/2250, Loss: 0.1371, LR: 0.000346
Step 400/2250, Loss: 0.0998, LR: 0.000096
Step 500/2250, Loss: 0.1134, LR: 0.001000
Step 600/2250, Loss: 0.0918, LR: 0.000976
Step 700/2250, Loss: 0.0648, LR: 0.000905
Step 800/2250, Loss: 0.0767, LR: 0.000794
Step 900/2250, Loss: 0.0470, LR: 0.000655
Step 1000/2250, Loss: 0.0443, LR: 0.000501
Step 1100/2250, Loss: 0.0476, LR: 0.000346
Step 1200/2250, Loss: 0.0441, LR: 0.000207
Step 1300/2250, Loss: 0.0358, LR: 0.000096
Step 1400/2250, Loss: 0.0197, LR: 0.000025
Step 1500/2250, Loss: 0.0358, LR: 0.001000
Step 1600/2250, Loss: 0.0362, LR: 0.000994
Step 1700/2250, Loss: 0.0645, LR: 0.000976
Step 1800/2250, Loss: 0.0487, LR: 0.000946
Step 1900/2250, Loss: 0.0539, LR: 0.000905
Step 2000/2250, Loss: 0.0331, LR: 0.000854
Step 2100/2250, Loss: 0.0314, LR: 0.000794
Step 2200/2250, Loss: 0.0347, LR: 0.000727
Training time: 7.61s (3.38ms per step)
Evaluating model...
Hard Accuracy: 0.9422
Soft Accuracy: 0.9934
Evaluation time: 0.22s
Checking axiom satisfaction...
4711
Axiom (anonymity) Satisfaction Rate: 0.9422
4600
Axiom (neutrality) Satisfaction Rate: 0.92
3568
Axiom (condorcet) Satisfaction Rate: 0.7136
223
Axiom (pareto) Satisfaction Rate: 0.0446
Measuring inference time...
Average inference time (single sample): 0.2375ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9422
  Soft Accuracy: 0.9934
  Training Time: 7.61s
  Inference Time: 0.2375ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 500,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 400.96s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.1242, LR: 0.000905
Step 200/7500, Loss: 0.0684, LR: 0.000655
Step 300/7500, Loss: 0.0430, LR: 0.000346
Step 400/7500, Loss: 0.0353, LR: 0.000096
Step 500/7500, Loss: 0.0327, LR: 0.001000
Step 600/7500, Loss: 0.0275, LR: 0.000976
Step 700/7500, Loss: 0.0392, LR: 0.000905
Step 800/7500, Loss: 0.0203, LR: 0.000794
Step 900/7500, Loss: 0.0249, LR: 0.000655
Step 1000/7500, Loss: 0.0178, LR: 0.000501
Step 1100/7500, Loss: 0.0363, LR: 0.000346
Step 1200/7500, Loss: 0.0269, LR: 0.000207
Step 1300/7500, Loss: 0.0359, LR: 0.000096
Step 1400/7500, Loss: 0.0236, LR: 0.000025
Step 1500/7500, Loss: 0.0209, LR: 0.001000
Step 1600/7500, Loss: 0.0437, LR: 0.000994
Step 1700/7500, Loss: 0.0363, LR: 0.000976
Step 1800/7500, Loss: 0.0334, LR: 0.000946
Step 1900/7500, Loss: 0.0218, LR: 0.000905
Step 2000/7500, Loss: 0.0180, LR: 0.000854
Step 2100/7500, Loss: 0.0195, LR: 0.000794
Step 2200/7500, Loss: 0.0163, LR: 0.000727
Step 2300/7500, Loss: 0.0327, LR: 0.000655
Step 2400/7500, Loss: 0.0154, LR: 0.000579
Step 2500/7500, Loss: 0.0377, LR: 0.000501
Step 2600/7500, Loss: 0.0223, LR: 0.000422
Step 2700/7500, Loss: 0.0152, LR: 0.000346
Step 2800/7500, Loss: 0.0340, LR: 0.000274
Step 2900/7500, Loss: 0.0178, LR: 0.000207
Step 3000/7500, Loss: 0.0120, LR: 0.000147
Step 3100/7500, Loss: 0.0074, LR: 0.000096
Step 3200/7500, Loss: 0.0179, LR: 0.000055
Step 3300/7500, Loss: 0.0140, LR: 0.000025
Step 3400/7500, Loss: 0.0154, LR: 0.000007
Step 3500/7500, Loss: 0.0103, LR: 0.001000
Step 3600/7500, Loss: 0.0535, LR: 0.000998
Step 3700/7500, Loss: 0.0238, LR: 0.000994
Step 3800/7500, Loss: 0.0177, LR: 0.000986
Step 3900/7500, Loss: 0.0196, LR: 0.000976
Step 4000/7500, Loss: 0.0135, LR: 0.000962
Step 4100/7500, Loss: 0.0231, LR: 0.000946
Step 4200/7500, Loss: 0.0167, LR: 0.000926
Step 4300/7500, Loss: 0.0086, LR: 0.000905
Step 4400/7500, Loss: 0.0354, LR: 0.000880
Step 4500/7500, Loss: 0.0254, LR: 0.000854
Step 4600/7500, Loss: 0.0256, LR: 0.000825
Step 4700/7500, Loss: 0.0194, LR: 0.000794
Step 4800/7500, Loss: 0.0176, LR: 0.000761
Step 4900/7500, Loss: 0.0060, LR: 0.000727
Step 5000/7500, Loss: 0.0180, LR: 0.000692
Step 5100/7500, Loss: 0.0153, LR: 0.000655
Step 5200/7500, Loss: 0.0184, LR: 0.000617
Step 5300/7500, Loss: 0.0175, LR: 0.000579
Step 5400/7500, Loss: 0.0201, LR: 0.000540
Step 5500/7500, Loss: 0.0167, LR: 0.000501
Step 5600/7500, Loss: 0.0151, LR: 0.000461
Step 5700/7500, Loss: 0.0179, LR: 0.000422
Step 5800/7500, Loss: 0.0291, LR: 0.000384
Step 5900/7500, Loss: 0.0153, LR: 0.000346
Step 6000/7500, Loss: 0.0125, LR: 0.000309
Step 6100/7500, Loss: 0.0144, LR: 0.000274
Step 6200/7500, Loss: 0.0212, LR: 0.000240
Step 6300/7500, Loss: 0.0148, LR: 0.000207
Step 6400/7500, Loss: 0.0141, LR: 0.000176
Step 6500/7500, Loss: 0.0226, LR: 0.000147
Step 6600/7500, Loss: 0.0143, LR: 0.000121
Step 6700/7500, Loss: 0.0208, LR: 0.000096
Step 6800/7500, Loss: 0.0196, LR: 0.000075
Step 6900/7500, Loss: 0.0079, LR: 0.000055
Step 7000/7500, Loss: 0.0239, LR: 0.000039
Step 7100/7500, Loss: 0.0115, LR: 0.000025
Step 7200/7500, Loss: 0.0064, LR: 0.000015
Step 7300/7500, Loss: 0.0212, LR: 0.000007
Step 7400/7500, Loss: 0.0323, LR: 0.000003
Step 7500/7500, Loss: 0.0071, LR: 0.001000
Training time: 19.66s (2.62ms per step)
Evaluating model...
Hard Accuracy: 0.9776
Soft Accuracy: 1.0
Evaluation time: 0.21s
Checking axiom satisfaction...
4888
Axiom (anonymity) Satisfaction Rate: 0.9776
4748
Axiom (neutrality) Satisfaction Rate: 0.9496
3570
Axiom (condorcet) Satisfaction Rate: 0.714
239
Axiom (pareto) Satisfaction Rate: 0.0478
Measuring inference time...
Average inference time (single sample): 0.2361ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9776
  Soft Accuracy: 1.0000
  Training Time: 19.66s
  Inference Time: 0.2361ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 219.81s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.1144, LR: 0.000905
Step 200/7500, Loss: 0.0720, LR: 0.000655
Step 300/7500, Loss: 0.0613, LR: 0.000346
Step 400/7500, Loss: 0.0348, LR: 0.000096
Step 500/7500, Loss: 0.0389, LR: 0.001000
Step 600/7500, Loss: 0.0601, LR: 0.000976
Step 700/7500, Loss: 0.0734, LR: 0.000905
Step 800/7500, Loss: 0.0507, LR: 0.000794
Step 900/7500, Loss: 0.0420, LR: 0.000655
Step 1000/7500, Loss: 0.0299, LR: 0.000501
Step 1100/7500, Loss: 0.0344, LR: 0.000346
Step 1200/7500, Loss: 0.0291, LR: 0.000207
Step 1300/7500, Loss: 0.0199, LR: 0.000096
Step 1400/7500, Loss: 0.0183, LR: 0.000025
Step 1500/7500, Loss: 0.0161, LR: 0.001000
Step 1600/7500, Loss: 0.0592, LR: 0.000994
Step 1700/7500, Loss: 0.0417, LR: 0.000976
Step 1800/7500, Loss: 0.0532, LR: 0.000946
Step 1900/7500, Loss: 0.0215, LR: 0.000905
Step 2000/7500, Loss: 0.0428, LR: 0.000854
Step 2100/7500, Loss: 0.0165, LR: 0.000794
Step 2200/7500, Loss: 0.0264, LR: 0.000727
Step 2300/7500, Loss: 0.0206, LR: 0.000655
Step 2400/7500, Loss: 0.0241, LR: 0.000579
Step 2500/7500, Loss: 0.0252, LR: 0.000501
Step 2600/7500, Loss: 0.0124, LR: 0.000422
Step 2700/7500, Loss: 0.0191, LR: 0.000346
Step 2800/7500, Loss: 0.0148, LR: 0.000274
Step 2900/7500, Loss: 0.0151, LR: 0.000207
Step 3000/7500, Loss: 0.0118, LR: 0.000147
Step 3100/7500, Loss: 0.0061, LR: 0.000096
Step 3200/7500, Loss: 0.0078, LR: 0.000055
Step 3300/7500, Loss: 0.0107, LR: 0.000025
Step 3400/7500, Loss: 0.0047, LR: 0.000007
Step 3500/7500, Loss: 0.0076, LR: 0.001000
Step 3600/7500, Loss: 0.0381, LR: 0.000998
Step 3700/7500, Loss: 0.0303, LR: 0.000994
Step 3800/7500, Loss: 0.0439, LR: 0.000986
Step 3900/7500, Loss: 0.0187, LR: 0.000976
Step 4000/7500, Loss: 0.0467, LR: 0.000962
Step 4100/7500, Loss: 0.0310, LR: 0.000946
Step 4200/7500, Loss: 0.0294, LR: 0.000926
Step 4300/7500, Loss: 0.0360, LR: 0.000905
Step 4400/7500, Loss: 0.0219, LR: 0.000880
Step 4500/7500, Loss: 0.0329, LR: 0.000854
Step 4600/7500, Loss: 0.0189, LR: 0.000825
Step 4700/7500, Loss: 0.0222, LR: 0.000794
Step 4800/7500, Loss: 0.0264, LR: 0.000761
Step 4900/7500, Loss: 0.0267, LR: 0.000727
Step 5000/7500, Loss: 0.0219, LR: 0.000692
Step 5100/7500, Loss: 0.0049, LR: 0.000655
Step 5200/7500, Loss: 0.0160, LR: 0.000617
Step 5300/7500, Loss: 0.0159, LR: 0.000579
Step 5400/7500, Loss: 0.0091, LR: 0.000540
Step 5500/7500, Loss: 0.0321, LR: 0.000501
Step 5600/7500, Loss: 0.0151, LR: 0.000461
Step 5700/7500, Loss: 0.0070, LR: 0.000422
Step 5800/7500, Loss: 0.0059, LR: 0.000384
Step 5900/7500, Loss: 0.0083, LR: 0.000346
Step 6000/7500, Loss: 0.0112, LR: 0.000309
Step 6100/7500, Loss: 0.0045, LR: 0.000274
Step 6200/7500, Loss: 0.0088, LR: 0.000240
Step 6300/7500, Loss: 0.0020, LR: 0.000207
Step 6400/7500, Loss: 0.0032, LR: 0.000176
Step 6500/7500, Loss: 0.0049, LR: 0.000147
Step 6600/7500, Loss: 0.0057, LR: 0.000121
Step 6700/7500, Loss: 0.0018, LR: 0.000096
Step 6800/7500, Loss: 0.0021, LR: 0.000075
Step 6900/7500, Loss: 0.0069, LR: 0.000055
Step 7000/7500, Loss: 0.0025, LR: 0.000039
Step 7100/7500, Loss: 0.0030, LR: 0.000025
Step 7200/7500, Loss: 0.0030, LR: 0.000015
Step 7300/7500, Loss: 0.0085, LR: 0.000007
Step 7400/7500, Loss: 0.0018, LR: 0.000003
Step 7500/7500, Loss: 0.0021, LR: 0.001000
Training time: 27.19s (3.63ms per step)
Evaluating model...
Hard Accuracy: 0.9942
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9942
Axiom (neutrality) Satisfaction Rate: 0.9564
Axiom (condorcet) Satisfaction Rate: 0.7272
Axiom (pareto) Satisfaction Rate: 0.0492
Measuring inference time...
Average inference time (single sample): 0.2747ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9942
  Soft Accuracy: 1.0000
  Training Time: 27.19s
  Inference Time: 0.2747ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 500,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 193.63s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.2417, LR: 0.000905
Step 200/7500, Loss: 0.1562, LR: 0.000655
Step 300/7500, Loss: 0.1188, LR: 0.000346
Step 400/7500, Loss: 0.1072, LR: 0.000096
Step 500/7500, Loss: 0.0951, LR: 0.001000
Step 600/7500, Loss: 0.1070, LR: 0.000976
Step 700/7500, Loss: 0.0753, LR: 0.000905
Step 800/7500, Loss: 0.0604, LR: 0.000794
Step 900/7500, Loss: 0.0589, LR: 0.000655
Step 1000/7500, Loss: 0.0501, LR: 0.000501
Step 1100/7500, Loss: 0.0378, LR: 0.000346
Step 1200/7500, Loss: 0.0336, LR: 0.000207
Step 1300/7500, Loss: 0.0315, LR: 0.000096
Step 1400/7500, Loss: 0.0427, LR: 0.000025
Step 1500/7500, Loss: 0.0309, LR: 0.001000
Step 1600/7500, Loss: 0.0462, LR: 0.000994
Step 1700/7500, Loss: 0.0822, LR: 0.000976
Step 1800/7500, Loss: 0.0360, LR: 0.000946
Step 1900/7500, Loss: 0.0504, LR: 0.000905
Step 2000/7500, Loss: 0.0451, LR: 0.000854
Step 2100/7500, Loss: 0.0396, LR: 0.000794
Step 2200/7500, Loss: 0.0279, LR: 0.000727
Step 2300/7500, Loss: 0.0312, LR: 0.000655
Step 2400/7500, Loss: 0.0382, LR: 0.000579
Step 2500/7500, Loss: 0.0311, LR: 0.000501
Step 2600/7500, Loss: 0.0215, LR: 0.000422
Step 2700/7500, Loss: 0.0185, LR: 0.000346
Step 2800/7500, Loss: 0.0078, LR: 0.000274
Step 2900/7500, Loss: 0.0281, LR: 0.000207
Step 3000/7500, Loss: 0.0096, LR: 0.000147
Step 3100/7500, Loss: 0.0175, LR: 0.000096
Step 3200/7500, Loss: 0.0230, LR: 0.000055
Step 3300/7500, Loss: 0.0161, LR: 0.000025
Step 3400/7500, Loss: 0.0080, LR: 0.000007
Step 3500/7500, Loss: 0.0191, LR: 0.001000
Step 3600/7500, Loss: 0.0613, LR: 0.000998
Step 3700/7500, Loss: 0.0364, LR: 0.000994
Step 3800/7500, Loss: 0.0334, LR: 0.000986
Step 3900/7500, Loss: 0.0392, LR: 0.000976
Step 4000/7500, Loss: 0.0356, LR: 0.000962
Step 4100/7500, Loss: 0.0371, LR: 0.000946
Step 4200/7500, Loss: 0.0366, LR: 0.000926
Step 4300/7500, Loss: 0.0319, LR: 0.000905
Step 4400/7500, Loss: 0.0325, LR: 0.000880
Step 4500/7500, Loss: 0.0320, LR: 0.000854
Step 4600/7500, Loss: 0.0321, LR: 0.000825
Step 4700/7500, Loss: 0.0416, LR: 0.000794
Step 4800/7500, Loss: 0.0262, LR: 0.000761
Step 4900/7500, Loss: 0.0373, LR: 0.000727
Step 5000/7500, Loss: 0.0272, LR: 0.000692
Step 5100/7500, Loss: 0.0319, LR: 0.000655
Step 5200/7500, Loss: 0.0326, LR: 0.000617
Step 5300/7500, Loss: 0.0333, LR: 0.000579
Step 5400/7500, Loss: 0.0189, LR: 0.000540
Step 5500/7500, Loss: 0.0281, LR: 0.000501
Step 5600/7500, Loss: 0.0289, LR: 0.000461
Step 5700/7500, Loss: 0.0103, LR: 0.000422
Step 5800/7500, Loss: 0.0128, LR: 0.000384
Step 5900/7500, Loss: 0.0073, LR: 0.000346
Step 6000/7500, Loss: 0.0107, LR: 0.000309
Step 6100/7500, Loss: 0.0176, LR: 0.000274
Step 6200/7500, Loss: 0.0070, LR: 0.000240
Step 6300/7500, Loss: 0.0082, LR: 0.000207
Step 6400/7500, Loss: 0.0131, LR: 0.000176
Step 6500/7500, Loss: 0.0065, LR: 0.000147
Step 6600/7500, Loss: 0.0140, LR: 0.000121
Step 6700/7500, Loss: 0.0075, LR: 0.000096
Step 6800/7500, Loss: 0.0047, LR: 0.000075
Step 6900/7500, Loss: 0.0069, LR: 0.000055
Step 7000/7500, Loss: 0.0114, LR: 0.000039
Step 7100/7500, Loss: 0.0053, LR: 0.000025
Step 7200/7500, Loss: 0.0073, LR: 0.000015
Step 7300/7500, Loss: 0.0029, LR: 0.000007
Step 7400/7500, Loss: 0.0073, LR: 0.000003
Step 7500/7500, Loss: 0.0042, LR: 0.001000
Training time: 27.84s (3.71ms per step)
Evaluating model...
Hard Accuracy: 0.9866
Soft Accuracy: 0.9994
Evaluation time: 0.23s
Checking axiom satisfaction...
4933
Axiom (anonymity) Satisfaction Rate: 0.9866
4775
Axiom (neutrality) Satisfaction Rate: 0.955
3545
Axiom (condorcet) Satisfaction Rate: 0.709
228
Axiom (pareto) Satisfaction Rate: 0.0456
Measuring inference time...
Average inference time (single sample): 0.2405ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9866
  Soft Accuracy: 0.9994
  Training Time: 27.84s
  Inference Time: 0.2405ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 1,000,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with BORDA
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 791.52s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.1434, LR: 0.000905
Step 200/15000, Loss: 0.0734, LR: 0.000655
Step 300/15000, Loss: 0.0438, LR: 0.000346
Step 400/15000, Loss: 0.0249, LR: 0.000096
Step 500/15000, Loss: 0.0264, LR: 0.001000
Step 600/15000, Loss: 0.0336, LR: 0.000976
Step 700/15000, Loss: 0.0229, LR: 0.000905
Step 800/15000, Loss: 0.0495, LR: 0.000794
Step 900/15000, Loss: 0.0328, LR: 0.000655
Step 1000/15000, Loss: 0.0285, LR: 0.000501
Step 1100/15000, Loss: 0.0223, LR: 0.000346
Step 1200/15000, Loss: 0.0172, LR: 0.000207
Step 1300/15000, Loss: 0.0112, LR: 0.000096
Step 1400/15000, Loss: 0.0246, LR: 0.000025
Step 1500/15000, Loss: 0.0182, LR: 0.001000
Step 1600/15000, Loss: 0.0397, LR: 0.000994
Step 1700/15000, Loss: 0.0192, LR: 0.000976
Step 1800/15000, Loss: 0.0179, LR: 0.000946
Step 1900/15000, Loss: 0.0230, LR: 0.000905
Step 2000/15000, Loss: 0.0283, LR: 0.000854
Step 2100/15000, Loss: 0.0170, LR: 0.000794
Step 2200/15000, Loss: 0.0175, LR: 0.000727
Step 2300/15000, Loss: 0.0183, LR: 0.000655
Step 2400/15000, Loss: 0.0212, LR: 0.000579
Step 2500/15000, Loss: 0.0111, LR: 0.000501
Step 2600/15000, Loss: 0.0231, LR: 0.000422
Step 2700/15000, Loss: 0.0173, LR: 0.000346
Step 2800/15000, Loss: 0.0225, LR: 0.000274
Step 2900/15000, Loss: 0.0143, LR: 0.000207
Step 3000/15000, Loss: 0.0107, LR: 0.000147
Step 3100/15000, Loss: 0.0283, LR: 0.000096
Step 3200/15000, Loss: 0.0304, LR: 0.000055
Step 3300/15000, Loss: 0.0290, LR: 0.000025
Step 3400/15000, Loss: 0.0173, LR: 0.000007
Step 3500/15000, Loss: 0.0117, LR: 0.001000
Step 3600/15000, Loss: 0.0228, LR: 0.000998
Step 3700/15000, Loss: 0.0120, LR: 0.000994
Step 3800/15000, Loss: 0.0265, LR: 0.000986
Step 3900/15000, Loss: 0.0159, LR: 0.000976
Step 4000/15000, Loss: 0.0260, LR: 0.000962
Step 4100/15000, Loss: 0.0187, LR: 0.000946
Step 4200/15000, Loss: 0.0341, LR: 0.000926
Step 4300/15000, Loss: 0.0174, LR: 0.000905
Step 4400/15000, Loss: 0.0175, LR: 0.000880
Step 4500/15000, Loss: 0.0198, LR: 0.000854
Step 4600/15000, Loss: 0.0377, LR: 0.000825
Step 4700/15000, Loss: 0.0134, LR: 0.000794
Step 4800/15000, Loss: 0.0186, LR: 0.000761
Step 4900/15000, Loss: 0.0198, LR: 0.000727
Step 5000/15000, Loss: 0.0167, LR: 0.000692
Step 5100/15000, Loss: 0.0178, LR: 0.000655
Step 5200/15000, Loss: 0.0196, LR: 0.000617
Step 5300/15000, Loss: 0.0198, LR: 0.000579
Step 5400/15000, Loss: 0.0082, LR: 0.000540
Step 5500/15000, Loss: 0.0145, LR: 0.000501
Step 5600/15000, Loss: 0.0119, LR: 0.000461
Step 5700/15000, Loss: 0.0118, LR: 0.000422
Step 5800/15000, Loss: 0.0172, LR: 0.000384
Step 5900/15000, Loss: 0.0150, LR: 0.000346
Step 6000/15000, Loss: 0.0090, LR: 0.000309
Step 6100/15000, Loss: 0.0222, LR: 0.000274
Step 6200/15000, Loss: 0.0195, LR: 0.000240
Step 6300/15000, Loss: 0.0257, LR: 0.000207
Step 6400/15000, Loss: 0.0106, LR: 0.000176
Step 6500/15000, Loss: 0.0170, LR: 0.000147
Step 6600/15000, Loss: 0.0226, LR: 0.000121
Step 6700/15000, Loss: 0.0069, LR: 0.000096
Step 6800/15000, Loss: 0.0114, LR: 0.000075
Step 6900/15000, Loss: 0.0181, LR: 0.000055
Step 7000/15000, Loss: 0.0172, LR: 0.000039
Step 7100/15000, Loss: 0.0175, LR: 0.000025
Step 7200/15000, Loss: 0.0118, LR: 0.000015
Step 7300/15000, Loss: 0.0140, LR: 0.000007
Step 7400/15000, Loss: 0.0062, LR: 0.000003
Step 7500/15000, Loss: 0.0237, LR: 0.001000
Step 7600/15000, Loss: 0.0164, LR: 0.001000
Step 7700/15000, Loss: 0.0299, LR: 0.000998
Step 7800/15000, Loss: 0.0200, LR: 0.000997
Step 7900/15000, Loss: 0.0086, LR: 0.000994
Step 8000/15000, Loss: 0.0158, LR: 0.000990
Step 8100/15000, Loss: 0.0121, LR: 0.000986
Step 8200/15000, Loss: 0.0220, LR: 0.000981
Step 8300/15000, Loss: 0.0173, LR: 0.000976
Step 8400/15000, Loss: 0.0155, LR: 0.000969
Step 8500/15000, Loss: 0.0381, LR: 0.000962
Step 8600/15000, Loss: 0.0230, LR: 0.000954
Step 8700/15000, Loss: 0.0199, LR: 0.000946
Step 8800/15000, Loss: 0.0152, LR: 0.000936
Step 8900/15000, Loss: 0.0205, LR: 0.000926
Step 9000/15000, Loss: 0.0119, LR: 0.000916
Step 9100/15000, Loss: 0.0279, LR: 0.000905
Step 9200/15000, Loss: 0.0125, LR: 0.000893
Step 9300/15000, Loss: 0.0172, LR: 0.000880
Step 9400/15000, Loss: 0.0115, LR: 0.000867
Step 9500/15000, Loss: 0.0142, LR: 0.000854
Step 9600/15000, Loss: 0.0290, LR: 0.000840
Step 9700/15000, Loss: 0.0138, LR: 0.000825
Step 9800/15000, Loss: 0.0100, LR: 0.000810
Step 9900/15000, Loss: 0.0087, LR: 0.000794
Step 10000/15000, Loss: 0.0179, LR: 0.000778
Step 10100/15000, Loss: 0.0289, LR: 0.000761
Step 10200/15000, Loss: 0.0162, LR: 0.000745
Step 10300/15000, Loss: 0.0160, LR: 0.000727
Step 10400/15000, Loss: 0.0286, LR: 0.000710
Step 10500/15000, Loss: 0.0154, LR: 0.000692
Step 10600/15000, Loss: 0.0195, LR: 0.000673
Step 10700/15000, Loss: 0.0179, LR: 0.000655
Step 10800/15000, Loss: 0.0104, LR: 0.000636
Step 10900/15000, Loss: 0.0137, LR: 0.000617
Step 11000/15000, Loss: 0.0163, LR: 0.000598
Step 11100/15000, Loss: 0.0092, LR: 0.000579
Step 11200/15000, Loss: 0.0085, LR: 0.000559
Step 11300/15000, Loss: 0.0155, LR: 0.000540
Step 11400/15000, Loss: 0.0110, LR: 0.000520
Step 11500/15000, Loss: 0.0101, LR: 0.000501
Step 11600/15000, Loss: 0.0171, LR: 0.000481
Step 11700/15000, Loss: 0.0163, LR: 0.000461
Step 11800/15000, Loss: 0.0300, LR: 0.000442
Step 11900/15000, Loss: 0.0119, LR: 0.000422
Step 12000/15000, Loss: 0.0189, LR: 0.000403
Step 12100/15000, Loss: 0.0114, LR: 0.000384
Step 12200/15000, Loss: 0.0114, LR: 0.000365
Step 12300/15000, Loss: 0.0142, LR: 0.000346
Step 12400/15000, Loss: 0.0170, LR: 0.000328
Step 12500/15000, Loss: 0.0217, LR: 0.000309
Step 12600/15000, Loss: 0.0146, LR: 0.000291
Step 12700/15000, Loss: 0.0181, LR: 0.000274
Step 12800/15000, Loss: 0.0133, LR: 0.000256
Step 12900/15000, Loss: 0.0114, LR: 0.000240
Step 13000/15000, Loss: 0.0102, LR: 0.000223
Step 13100/15000, Loss: 0.0162, LR: 0.000207
Step 13200/15000, Loss: 0.0266, LR: 0.000191
Step 13300/15000, Loss: 0.0140, LR: 0.000176
Step 13400/15000, Loss: 0.0214, LR: 0.000161
Step 13500/15000, Loss: 0.0134, LR: 0.000147
Step 13600/15000, Loss: 0.0163, LR: 0.000134
Step 13700/15000, Loss: 0.0217, LR: 0.000121
Step 13800/15000, Loss: 0.0116, LR: 0.000108
Step 13900/15000, Loss: 0.0096, LR: 0.000096
Step 14000/15000, Loss: 0.0114, LR: 0.000085
Step 14100/15000, Loss: 0.0152, LR: 0.000075
Step 14200/15000, Loss: 0.0285, LR: 0.000065
Step 14300/15000, Loss: 0.0088, LR: 0.000055
Step 14400/15000, Loss: 0.0187, LR: 0.000047
Step 14500/15000, Loss: 0.0233, LR: 0.000039
Step 14600/15000, Loss: 0.0082, LR: 0.000032
Step 14700/15000, Loss: 0.0116, LR: 0.000025
Step 14800/15000, Loss: 0.0123, LR: 0.000020
Step 14900/15000, Loss: 0.0082, LR: 0.000015
Step 15000/15000, Loss: 0.0243, LR: 0.000011
Training time: 47.22s (3.15ms per step)
Evaluating model...
Hard Accuracy: 0.982
Soft Accuracy: 1.0
Evaluation time: 0.23s
Checking axiom satisfaction...
4910
Axiom (anonymity) Satisfaction Rate: 0.982
4779
Axiom (neutrality) Satisfaction Rate: 0.9558
3546
Axiom (condorcet) Satisfaction Rate: 0.7092
248
Axiom (pareto) Satisfaction Rate: 0.0496
Measuring inference time...
Average inference time (single sample): 0.2507ms

PAIRWISE Results (BORDA):
  Hard Accuracy: 0.9820
  Soft Accuracy: 1.0000
  Training Time: 47.22s
  Inference Time: 0.2507ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with BORDA
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 447.58s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.1104, LR: 0.000905
Step 200/15000, Loss: 0.0626, LR: 0.000655
Step 300/15000, Loss: 0.0750, LR: 0.000346
Step 400/15000, Loss: 0.0392, LR: 0.000096
Step 500/15000, Loss: 0.0364, LR: 0.001000
Step 600/15000, Loss: 0.0528, LR: 0.000976
Step 700/15000, Loss: 0.0559, LR: 0.000905
Step 800/15000, Loss: 0.0436, LR: 0.000794
Step 900/15000, Loss: 0.0460, LR: 0.000655
Step 1000/15000, Loss: 0.0317, LR: 0.000501
Step 1100/15000, Loss: 0.0195, LR: 0.000346
Step 1200/15000, Loss: 0.0196, LR: 0.000207
Step 1300/15000, Loss: 0.0168, LR: 0.000096
Step 1400/15000, Loss: 0.0188, LR: 0.000025
Step 1500/15000, Loss: 0.0199, LR: 0.001000
Step 1600/15000, Loss: 0.0752, LR: 0.000994
Step 1700/15000, Loss: 0.0640, LR: 0.000976
Step 1800/15000, Loss: 0.0242, LR: 0.000946
Step 1900/15000, Loss: 0.0277, LR: 0.000905
Step 2000/15000, Loss: 0.0490, LR: 0.000854
Step 2100/15000, Loss: 0.0474, LR: 0.000794
Step 2200/15000, Loss: 0.0401, LR: 0.000727
Step 2300/15000, Loss: 0.0196, LR: 0.000655
Step 2400/15000, Loss: 0.0288, LR: 0.000579
Step 2500/15000, Loss: 0.0176, LR: 0.000501
Step 2600/15000, Loss: 0.0178, LR: 0.000422
Step 2700/15000, Loss: 0.0108, LR: 0.000346
Step 2800/15000, Loss: 0.0090, LR: 0.000274
Step 2900/15000, Loss: 0.0114, LR: 0.000207
Step 3000/15000, Loss: 0.0158, LR: 0.000147
Step 3100/15000, Loss: 0.0107, LR: 0.000096
Step 3200/15000, Loss: 0.0054, LR: 0.000055
Step 3300/15000, Loss: 0.0118, LR: 0.000025
Step 3400/15000, Loss: 0.0066, LR: 0.000007
Step 3500/15000, Loss: 0.0089, LR: 0.001000
Step 3600/15000, Loss: 0.0282, LR: 0.000998
Step 3700/15000, Loss: 0.0282, LR: 0.000994
Step 3800/15000, Loss: 0.0204, LR: 0.000986
Step 3900/15000, Loss: 0.0307, LR: 0.000976
Step 4000/15000, Loss: 0.0219, LR: 0.000962
Step 4100/15000, Loss: 0.0206, LR: 0.000946
Step 4200/15000, Loss: 0.0176, LR: 0.000926
Step 4300/15000, Loss: 0.0187, LR: 0.000905
Step 4400/15000, Loss: 0.0159, LR: 0.000880
Step 4500/15000, Loss: 0.0285, LR: 0.000854
Step 4600/15000, Loss: 0.0168, LR: 0.000825
Step 4700/15000, Loss: 0.0228, LR: 0.000794
Step 4800/15000, Loss: 0.0171, LR: 0.000761
Step 4900/15000, Loss: 0.0284, LR: 0.000727
Step 5000/15000, Loss: 0.0181, LR: 0.000692
Step 5100/15000, Loss: 0.0176, LR: 0.000655
Step 5200/15000, Loss: 0.0180, LR: 0.000617
Step 5300/15000, Loss: 0.0117, LR: 0.000579
Step 5400/15000, Loss: 0.0102, LR: 0.000540
Step 5500/15000, Loss: 0.0147, LR: 0.000501
Step 5600/15000, Loss: 0.0156, LR: 0.000461
Step 5700/15000, Loss: 0.0039, LR: 0.000422
Step 5800/15000, Loss: 0.0067, LR: 0.000384
Step 5900/15000, Loss: 0.0091, LR: 0.000346
Step 6000/15000, Loss: 0.0057, LR: 0.000309
Step 6100/15000, Loss: 0.0089, LR: 0.000274
Step 6200/15000, Loss: 0.0092, LR: 0.000240
Step 6300/15000, Loss: 0.0066, LR: 0.000207
Step 6400/15000, Loss: 0.0043, LR: 0.000176
Step 6500/15000, Loss: 0.0038, LR: 0.000147
Step 6600/15000, Loss: 0.0046, LR: 0.000121
Step 6700/15000, Loss: 0.0049, LR: 0.000096
Step 6800/15000, Loss: 0.0065, LR: 0.000075
Step 6900/15000, Loss: 0.0037, LR: 0.000055
Step 7000/15000, Loss: 0.0020, LR: 0.000039
Step 7100/15000, Loss: 0.0038, LR: 0.000025
Step 7200/15000, Loss: 0.0030, LR: 0.000015
Step 7300/15000, Loss: 0.0030, LR: 0.000007
Step 7400/15000, Loss: 0.0070, LR: 0.000003
Step 7500/15000, Loss: 0.0033, LR: 0.001000
Step 7600/15000, Loss: 0.0261, LR: 0.001000
Step 7700/15000, Loss: 0.0329, LR: 0.000998
Step 7800/15000, Loss: 0.0154, LR: 0.000997
Step 7900/15000, Loss: 0.0237, LR: 0.000994
Step 8000/15000, Loss: 0.0289, LR: 0.000990
Step 8100/15000, Loss: 0.0361, LR: 0.000986
Step 8200/15000, Loss: 0.0446, LR: 0.000981
Step 8300/15000, Loss: 0.0238, LR: 0.000976
Step 8400/15000, Loss: 0.0192, LR: 0.000969
Step 8500/15000, Loss: 0.0146, LR: 0.000962
Step 8600/15000, Loss: 0.0226, LR: 0.000954
Step 8700/15000, Loss: 0.0241, LR: 0.000946
Step 8800/15000, Loss: 0.0130, LR: 0.000936
Step 8900/15000, Loss: 0.0205, LR: 0.000926
Step 9000/15000, Loss: 0.0100, LR: 0.000916
Step 9100/15000, Loss: 0.0216, LR: 0.000905
Step 9200/15000, Loss: 0.0155, LR: 0.000893
Step 9300/15000, Loss: 0.0216, LR: 0.000880
Step 9400/15000, Loss: 0.0115, LR: 0.000867
Step 9500/15000, Loss: 0.0134, LR: 0.000854
Step 9600/15000, Loss: 0.0062, LR: 0.000840
Step 9700/15000, Loss: 0.0150, LR: 0.000825
Step 9800/15000, Loss: 0.0270, LR: 0.000810
Step 9900/15000, Loss: 0.0232, LR: 0.000794
Step 10000/15000, Loss: 0.0102, LR: 0.000778
Step 10100/15000, Loss: 0.0190, LR: 0.000761
Step 10200/15000, Loss: 0.0094, LR: 0.000745
Step 10300/15000, Loss: 0.0156, LR: 0.000727
Step 10400/15000, Loss: 0.0130, LR: 0.000710
Step 10500/15000, Loss: 0.0177, LR: 0.000692
Step 10600/15000, Loss: 0.0128, LR: 0.000673
Step 10700/15000, Loss: 0.0129, LR: 0.000655
Step 10800/15000, Loss: 0.0065, LR: 0.000636
Step 10900/15000, Loss: 0.0127, LR: 0.000617
Step 11000/15000, Loss: 0.0062, LR: 0.000598
Step 11100/15000, Loss: 0.0084, LR: 0.000579
Step 11200/15000, Loss: 0.0041, LR: 0.000559
Step 11300/15000, Loss: 0.0113, LR: 0.000540
Step 11400/15000, Loss: 0.0037, LR: 0.000520
Step 11500/15000, Loss: 0.0122, LR: 0.000501
Step 11600/15000, Loss: 0.0036, LR: 0.000481
Step 11700/15000, Loss: 0.0088, LR: 0.000461
Step 11800/15000, Loss: 0.0083, LR: 0.000442
Step 11900/15000, Loss: 0.0064, LR: 0.000422
Step 12000/15000, Loss: 0.0022, LR: 0.000403
Step 12100/15000, Loss: 0.0076, LR: 0.000384
Step 12200/15000, Loss: 0.0100, LR: 0.000365
Step 12300/15000, Loss: 0.0087, LR: 0.000346
Step 12400/15000, Loss: 0.0058, LR: 0.000328
Step 12500/15000, Loss: 0.0071, LR: 0.000309
Step 12600/15000, Loss: 0.0019, LR: 0.000291
Step 12700/15000, Loss: 0.0056, LR: 0.000274
Step 12800/15000, Loss: 0.0054, LR: 0.000256
Step 12900/15000, Loss: 0.0064, LR: 0.000240
Step 13000/15000, Loss: 0.0023, LR: 0.000223
Step 13100/15000, Loss: 0.0005, LR: 0.000207
Step 13200/15000, Loss: 0.0024, LR: 0.000191
Step 13300/15000, Loss: 0.0014, LR: 0.000176
Step 13400/15000, Loss: 0.0011, LR: 0.000161
Step 13500/15000, Loss: 0.0021, LR: 0.000147
Step 13600/15000, Loss: 0.0014, LR: 0.000134
Step 13700/15000, Loss: 0.0005, LR: 0.000121
Step 13800/15000, Loss: 0.0026, LR: 0.000108
Step 13900/15000, Loss: 0.0017, LR: 0.000096
Step 14000/15000, Loss: 0.0008, LR: 0.000085
Step 14100/15000, Loss: 0.0012, LR: 0.000075
Step 14200/15000, Loss: 0.0008, LR: 0.000065
Step 14300/15000, Loss: 0.0023, LR: 0.000055
Step 14400/15000, Loss: 0.0004, LR: 0.000047
Step 14500/15000, Loss: 0.0016, LR: 0.000039
Step 14600/15000, Loss: 0.0026, LR: 0.000032
Step 14700/15000, Loss: 0.0008, LR: 0.000025
Step 14800/15000, Loss: 0.0010, LR: 0.000020
Step 14900/15000, Loss: 0.0008, LR: 0.000015
Step 15000/15000, Loss: 0.0016, LR: 0.000011
Training time: 46.40s (3.09ms per step)
Evaluating model...
Hard Accuracy: 0.9986
Soft Accuracy: 1.0
Evaluation time: 0.23s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9986
Axiom (neutrality) Satisfaction Rate: 0.9608
Axiom (condorcet) Satisfaction Rate: 0.7256
Axiom (pareto) Satisfaction Rate: 0.0526
Measuring inference time...
Average inference time (single sample): 0.2916ms

PAIRWISE_PER_VOTER Results (BORDA):
  Hard Accuracy: 0.9986
  Soft Accuracy: 1.0000
  Training Time: 46.40s
  Inference Time: 0.2916ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with BORDA
Dataset size: 1,000,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 390.94s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.2235, LR: 0.000905
Step 200/15000, Loss: 0.1639, LR: 0.000655
Step 300/15000, Loss: 0.1013, LR: 0.000346
Step 400/15000, Loss: 0.1021, LR: 0.000096
Step 500/15000, Loss: 0.1167, LR: 0.001000
Step 600/15000, Loss: 0.1131, LR: 0.000976
Step 700/15000, Loss: 0.0782, LR: 0.000905
Step 800/15000, Loss: 0.0798, LR: 0.000794
Step 900/15000, Loss: 0.0566, LR: 0.000655
Step 1000/15000, Loss: 0.0515, LR: 0.000501
Step 1100/15000, Loss: 0.0483, LR: 0.000346
Step 1200/15000, Loss: 0.0401, LR: 0.000207
Step 1300/15000, Loss: 0.0316, LR: 0.000096
Step 1400/15000, Loss: 0.0356, LR: 0.000025
Step 1500/15000, Loss: 0.0362, LR: 0.001000
Step 1600/15000, Loss: 0.0627, LR: 0.000994
Step 1700/15000, Loss: 0.0586, LR: 0.000976
Step 1800/15000, Loss: 0.0686, LR: 0.000946
Step 1900/15000, Loss: 0.0464, LR: 0.000905
Step 2000/15000, Loss: 0.0485, LR: 0.000854
Step 2100/15000, Loss: 0.0313, LR: 0.000794
Step 2200/15000, Loss: 0.0385, LR: 0.000727
Step 2300/15000, Loss: 0.0576, LR: 0.000655
Step 2400/15000, Loss: 0.0183, LR: 0.000579
Step 2500/15000, Loss: 0.0299, LR: 0.000501
Step 2600/15000, Loss: 0.0346, LR: 0.000422
Step 2700/15000, Loss: 0.0309, LR: 0.000346
Step 2800/15000, Loss: 0.0166, LR: 0.000274
Step 2900/15000, Loss: 0.0264, LR: 0.000207
Step 3000/15000, Loss: 0.0162, LR: 0.000147
Step 3100/15000, Loss: 0.0140, LR: 0.000096
Step 3200/15000, Loss: 0.0322, LR: 0.000055
Step 3300/15000, Loss: 0.0217, LR: 0.000025
Step 3400/15000, Loss: 0.0185, LR: 0.000007
Step 3500/15000, Loss: 0.0162, LR: 0.001000
Step 3600/15000, Loss: 0.0748, LR: 0.000998
Step 3700/15000, Loss: 0.0367, LR: 0.000994
Step 3800/15000, Loss: 0.0627, LR: 0.000986
Step 3900/15000, Loss: 0.0347, LR: 0.000976
Step 4000/15000, Loss: 0.0375, LR: 0.000962
Step 4100/15000, Loss: 0.0444, LR: 0.000946
Step 4200/15000, Loss: 0.0352, LR: 0.000926
Step 4300/15000, Loss: 0.0342, LR: 0.000905
Step 4400/15000, Loss: 0.0267, LR: 0.000880
Step 4500/15000, Loss: 0.0372, LR: 0.000854
Step 4600/15000, Loss: 0.0191, LR: 0.000825
Step 4700/15000, Loss: 0.0340, LR: 0.000794
Step 4800/15000, Loss: 0.0351, LR: 0.000761
Step 4900/15000, Loss: 0.0257, LR: 0.000727
Step 5000/15000, Loss: 0.0349, LR: 0.000692
Step 5100/15000, Loss: 0.0239, LR: 0.000655
Step 5200/15000, Loss: 0.0197, LR: 0.000617
Step 5300/15000, Loss: 0.0144, LR: 0.000579
Step 5400/15000, Loss: 0.0279, LR: 0.000540
Step 5500/15000, Loss: 0.0301, LR: 0.000501
Step 5600/15000, Loss: 0.0136, LR: 0.000461
Step 5700/15000, Loss: 0.0170, LR: 0.000422
Step 5800/15000, Loss: 0.0211, LR: 0.000384
Step 5900/15000, Loss: 0.0196, LR: 0.000346
Step 6000/15000, Loss: 0.0111, LR: 0.000309
Step 6100/15000, Loss: 0.0162, LR: 0.000274
Step 6200/15000, Loss: 0.0175, LR: 0.000240
Step 6300/15000, Loss: 0.0106, LR: 0.000207
Step 6400/15000, Loss: 0.0174, LR: 0.000176
Step 6500/15000, Loss: 0.0125, LR: 0.000147
Step 6600/15000, Loss: 0.0186, LR: 0.000121
Step 6700/15000, Loss: 0.0050, LR: 0.000096
Step 6800/15000, Loss: 0.0055, LR: 0.000075
Step 6900/15000, Loss: 0.0040, LR: 0.000055
Step 7000/15000, Loss: 0.0115, LR: 0.000039
Step 7100/15000, Loss: 0.0047, LR: 0.000025
Step 7200/15000, Loss: 0.0082, LR: 0.000015
Step 7300/15000, Loss: 0.0065, LR: 0.000007
Step 7400/15000, Loss: 0.0059, LR: 0.000003
Step 7500/15000, Loss: 0.0082, LR: 0.001000
Step 7600/15000, Loss: 0.0342, LR: 0.001000
Step 7700/15000, Loss: 0.0379, LR: 0.000998
Step 7800/15000, Loss: 0.0343, LR: 0.000997
Step 7900/15000, Loss: 0.0342, LR: 0.000994
Step 8000/15000, Loss: 0.0312, LR: 0.000990
Step 8100/15000, Loss: 0.0214, LR: 0.000986
Step 8200/15000, Loss: 0.0399, LR: 0.000981
Step 8300/15000, Loss: 0.0294, LR: 0.000976
Step 8400/15000, Loss: 0.0398, LR: 0.000969
Step 8500/15000, Loss: 0.0263, LR: 0.000962
Step 8600/15000, Loss: 0.0258, LR: 0.000954
Step 8700/15000, Loss: 0.0256, LR: 0.000946
Step 8800/15000, Loss: 0.0307, LR: 0.000936
Step 8900/15000, Loss: 0.0322, LR: 0.000926
Step 9000/15000, Loss: 0.0257, LR: 0.000916
Step 9100/15000, Loss: 0.0148, LR: 0.000905
Step 9200/15000, Loss: 0.0423, LR: 0.000893
Step 9300/15000, Loss: 0.0166, LR: 0.000880
Step 9400/15000, Loss: 0.0188, LR: 0.000867
Step 9500/15000, Loss: 0.0243, LR: 0.000854
Step 9600/15000, Loss: 0.0231, LR: 0.000840
Step 9700/15000, Loss: 0.0250, LR: 0.000825
Step 9800/15000, Loss: 0.0290, LR: 0.000810
Step 9900/15000, Loss: 0.0138, LR: 0.000794
Step 10000/15000, Loss: 0.0230, LR: 0.000778
Step 10100/15000, Loss: 0.0174, LR: 0.000761
Step 10200/15000, Loss: 0.0265, LR: 0.000745
Step 10300/15000, Loss: 0.0082, LR: 0.000727
Step 10400/15000, Loss: 0.0319, LR: 0.000710
Step 10500/15000, Loss: 0.0126, LR: 0.000692
Step 10600/15000, Loss: 0.0190, LR: 0.000673
Step 10700/15000, Loss: 0.0139, LR: 0.000655
Step 10800/15000, Loss: 0.0117, LR: 0.000636
Step 10900/15000, Loss: 0.0141, LR: 0.000617
Step 11000/15000, Loss: 0.0137, LR: 0.000598
Step 11100/15000, Loss: 0.0096, LR: 0.000579
Step 11200/15000, Loss: 0.0097, LR: 0.000559
Step 11300/15000, Loss: 0.0247, LR: 0.000540
Step 11400/15000, Loss: 0.0088, LR: 0.000520
Step 11500/15000, Loss: 0.0177, LR: 0.000501
Step 11600/15000, Loss: 0.0055, LR: 0.000481
Step 11700/15000, Loss: 0.0108, LR: 0.000461
Step 11800/15000, Loss: 0.0111, LR: 0.000442
Step 11900/15000, Loss: 0.0099, LR: 0.000422
Step 12000/15000, Loss: 0.0124, LR: 0.000403
Step 12100/15000, Loss: 0.0141, LR: 0.000384
Step 12200/15000, Loss: 0.0239, LR: 0.000365
Step 12300/15000, Loss: 0.0122, LR: 0.000346
Step 12400/15000, Loss: 0.0145, LR: 0.000328
Step 12500/15000, Loss: 0.0044, LR: 0.000309
Step 12600/15000, Loss: 0.0055, LR: 0.000291
Step 12700/15000, Loss: 0.0110, LR: 0.000274
Step 12800/15000, Loss: 0.0128, LR: 0.000256
Step 12900/15000, Loss: 0.0069, LR: 0.000240
Step 13000/15000, Loss: 0.0057, LR: 0.000223
Step 13100/15000, Loss: 0.0051, LR: 0.000207
Step 13200/15000, Loss: 0.0063, LR: 0.000191
Step 13300/15000, Loss: 0.0153, LR: 0.000176
Step 13400/15000, Loss: 0.0086, LR: 0.000161
Step 13500/15000, Loss: 0.0053, LR: 0.000147
Step 13600/15000, Loss: 0.0072, LR: 0.000134
Step 13700/15000, Loss: 0.0026, LR: 0.000121
Step 13800/15000, Loss: 0.0015, LR: 0.000108
Step 13900/15000, Loss: 0.0074, LR: 0.000096
Step 14000/15000, Loss: 0.0025, LR: 0.000085
Step 14100/15000, Loss: 0.0048, LR: 0.000075
Step 14200/15000, Loss: 0.0031, LR: 0.000065
Step 14300/15000, Loss: 0.0065, LR: 0.000055
Step 14400/15000, Loss: 0.0006, LR: 0.000047
Step 14500/15000, Loss: 0.0033, LR: 0.000039
Step 14600/15000, Loss: 0.0015, LR: 0.000032
Step 14700/15000, Loss: 0.0048, LR: 0.000025
Step 14800/15000, Loss: 0.0024, LR: 0.000020
Step 14900/15000, Loss: 0.0032, LR: 0.000015
Step 15000/15000, Loss: 0.0031, LR: 0.000011
Training time: 55.00s (3.67ms per step)
Evaluating model...
Hard Accuracy: 0.994
Soft Accuracy: 0.9998
Evaluation time: 0.23s
Checking axiom satisfaction...
4970
Axiom (anonymity) Satisfaction Rate: 0.994
4776
Axiom (neutrality) Satisfaction Rate: 0.9552
3590
Axiom (condorcet) Satisfaction Rate: 0.718
260
Axiom (pareto) Satisfaction Rate: 0.052
Measuring inference time...
Average inference time (single sample): 0.2361ms

ONEHOT Results (BORDA):
  Hard Accuracy: 0.9940
  Soft Accuracy: 0.9998
  Training Time: 55.00s
  Inference Time: 0.2361ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

################################################################################
# Testing with PLURALITY voting method
################################################################################

--------------------------------------------------------------------------------
Dataset size: 1,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 1.08s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.04s (2.83ms per step)
Evaluating model...
Hard Accuracy: 0.4266
Soft Accuracy: 0.5036
Evaluation time: 0.22s
Checking axiom satisfaction...
2133
Axiom (anonymity) Satisfaction Rate: 0.4266
2133
Axiom (neutrality) Satisfaction Rate: 0.4266
2787
Axiom (condorcet) Satisfaction Rate: 0.5574
247
Axiom (pareto) Satisfaction Rate: 0.0494
Measuring inference time...
Average inference time (single sample): 0.2367ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.4266
  Soft Accuracy: 0.5036
  Training Time: 0.04s
  Inference Time: 0.2367ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 0.42s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.04s (2.96ms per step)
Evaluating model...
Hard Accuracy: 0.4116
Soft Accuracy: 0.4684
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.4116
Axiom (neutrality) Satisfaction Rate: 0.4108
Axiom (condorcet) Satisfaction Rate: 0.543
Axiom (pareto) Satisfaction Rate: 0.05
Measuring inference time...
Average inference time (single sample): 0.2746ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.4116
  Soft Accuracy: 0.4684
  Training Time: 0.04s
  Inference Time: 0.2746ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 1,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 0.35s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.04s (2.98ms per step)
Evaluating model...
Hard Accuracy: 0.0
Soft Accuracy: 0.0
Evaluation time: 0.22s
Checking axiom satisfaction...
0
Axiom (anonymity) Satisfaction Rate: 0.0
0
Axiom (neutrality) Satisfaction Rate: 0.0
2713
Axiom (condorcet) Satisfaction Rate: 0.5426
233
Axiom (pareto) Satisfaction Rate: 0.0466
Measuring inference time...
Average inference time (single sample): 0.2372ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.0000
  Soft Accuracy: 0.0000
  Training Time: 0.04s
  Inference Time: 0.2372ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 5,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 4.04s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.26s (3.43ms per step)
Evaluating model...
Hard Accuracy: 0.651
Soft Accuracy: 0.7744
Evaluation time: 0.52s
Checking axiom satisfaction...
3255
Axiom (anonymity) Satisfaction Rate: 0.651
3245
Axiom (neutrality) Satisfaction Rate: 0.649
2716
Axiom (condorcet) Satisfaction Rate: 0.5432
255
Axiom (pareto) Satisfaction Rate: 0.051
Measuring inference time...
Average inference time (single sample): 0.2398ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.6510
  Soft Accuracy: 0.7744
  Training Time: 0.26s
  Inference Time: 0.2398ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 2.09s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.22s (2.97ms per step)
Evaluating model...
Hard Accuracy: 0.676
Soft Accuracy: 0.8206
Evaluation time: 0.77s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.676
Axiom (neutrality) Satisfaction Rate: 0.6658
Axiom (condorcet) Satisfaction Rate: 0.5384
Axiom (pareto) Satisfaction Rate: 0.0504
Measuring inference time...
Average inference time (single sample): 0.2794ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.6760
  Soft Accuracy: 0.8206
  Training Time: 0.22s
  Inference Time: 0.2794ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 5,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 1.72s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.25s (3.40ms per step)
Evaluating model...
Hard Accuracy: 0.5302
Soft Accuracy: 0.6096
Evaluation time: 0.22s
Checking axiom satisfaction...
2651
Axiom (anonymity) Satisfaction Rate: 0.5302
2643
Axiom (neutrality) Satisfaction Rate: 0.5286
2739
Axiom (condorcet) Satisfaction Rate: 0.5478
232
Axiom (pareto) Satisfaction Rate: 0.0464
Measuring inference time...
Average inference time (single sample): 0.2420ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.5302
  Soft Accuracy: 0.6096
  Training Time: 0.25s
  Inference Time: 0.2420ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 15,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 11.86s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.2366, LR: 0.000905
Step 200/225, Loss: 0.2667, LR: 0.000655
Training time: 0.57s (2.53ms per step)
Evaluating model...
Hard Accuracy: 0.7202
Soft Accuracy: 0.8576
Evaluation time: 0.22s
Checking axiom satisfaction...
3601
Axiom (anonymity) Satisfaction Rate: 0.7202
3577
Axiom (neutrality) Satisfaction Rate: 0.7154
2698
Axiom (condorcet) Satisfaction Rate: 0.5396
255
Axiom (pareto) Satisfaction Rate: 0.051
Measuring inference time...
Average inference time (single sample): 0.2481ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7202
  Soft Accuracy: 0.8576
  Training Time: 0.57s
  Inference Time: 0.2481ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 6.65s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.2096, LR: 0.000905
Step 200/225, Loss: 0.1621, LR: 0.000655
Training time: 1.63s (7.26ms per step)
Evaluating model...
Hard Accuracy: 0.685
Soft Accuracy: 0.8398
Evaluation time: 0.32s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.685
Axiom (neutrality) Satisfaction Rate: 0.6778
Axiom (condorcet) Satisfaction Rate: 0.554
Axiom (pareto) Satisfaction Rate: 0.05
Measuring inference time...
Average inference time (single sample): 0.2869ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.6850
  Soft Accuracy: 0.8398
  Training Time: 1.63s
  Inference Time: 0.2869ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 15,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 5.20s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.2497, LR: 0.000905
Step 200/225, Loss: 0.1002, LR: 0.000655
Training time: 0.70s (3.09ms per step)
Evaluating model...
Hard Accuracy: 0.8152
Soft Accuracy: 0.9224
Evaluation time: 0.22s
Checking axiom satisfaction...
4076
Axiom (anonymity) Satisfaction Rate: 0.8152
3977
Axiom (neutrality) Satisfaction Rate: 0.7954
2688
Axiom (condorcet) Satisfaction Rate: 0.5376
229
Axiom (pareto) Satisfaction Rate: 0.0458
Measuring inference time...
Average inference time (single sample): 0.2331ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.8152
  Soft Accuracy: 0.9224
  Training Time: 0.70s
  Inference Time: 0.2331ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 50,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 39.21s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.3021, LR: 0.000905
Step 200/750, Loss: 0.2228, LR: 0.000655
Step 300/750, Loss: 0.2296, LR: 0.000346
Step 400/750, Loss: 0.2789, LR: 0.000096
Step 500/750, Loss: 0.2234, LR: 0.001000
Step 600/750, Loss: 0.2246, LR: 0.000976
Step 700/750, Loss: 0.2360, LR: 0.000905
Training time: 1.89s (2.52ms per step)
Evaluating model...
Hard Accuracy: 0.7212
Soft Accuracy: 0.8592
Evaluation time: 0.22s
Checking axiom satisfaction...
3606
Axiom (anonymity) Satisfaction Rate: 0.7212
3588
Axiom (neutrality) Satisfaction Rate: 0.7176
2723
Axiom (condorcet) Satisfaction Rate: 0.5446
242
Axiom (pareto) Satisfaction Rate: 0.0484
Measuring inference time...
Average inference time (single sample): 0.2323ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7212
  Soft Accuracy: 0.8592
  Training Time: 1.89s
  Inference Time: 0.2323ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 21.90s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.1833, LR: 0.000905
Step 200/750, Loss: 0.2123, LR: 0.000655
Step 300/750, Loss: 0.1743, LR: 0.000346
Step 400/750, Loss: 0.1692, LR: 0.000096
Step 500/750, Loss: 0.2275, LR: 0.001000
Step 600/750, Loss: 0.1527, LR: 0.000976
Step 700/750, Loss: 0.1856, LR: 0.000905
Training time: 2.70s (3.59ms per step)
Evaluating model...
Hard Accuracy: 0.7284
Soft Accuracy: 0.8482
Evaluation time: 0.23s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7284
Axiom (neutrality) Satisfaction Rate: 0.7146
Axiom (condorcet) Satisfaction Rate: 0.559
Axiom (pareto) Satisfaction Rate: 0.0492
Measuring inference time...
Average inference time (single sample): 0.2839ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7284
  Soft Accuracy: 0.8482
  Training Time: 2.70s
  Inference Time: 0.2839ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 50,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 18.04s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.2627, LR: 0.000905
Step 200/750, Loss: 0.1379, LR: 0.000655
Step 300/750, Loss: 0.0782, LR: 0.000346
Step 400/750, Loss: 0.0901, LR: 0.000096
Step 500/750, Loss: 0.0811, LR: 0.001000
Step 600/750, Loss: 0.0889, LR: 0.000976
Step 700/750, Loss: 0.0714, LR: 0.000905
Training time: 2.81s (3.75ms per step)
Evaluating model...
Hard Accuracy: 0.919
Soft Accuracy: 0.992
Evaluation time: 0.77s
Checking axiom satisfaction...
4595
Axiom (anonymity) Satisfaction Rate: 0.919
4369
Axiom (neutrality) Satisfaction Rate: 0.8738
2818
Axiom (condorcet) Satisfaction Rate: 0.5636
238
Axiom (pareto) Satisfaction Rate: 0.0476
Measuring inference time...
Average inference time (single sample): 0.2334ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9190
  Soft Accuracy: 0.9920
  Training Time: 2.81s
  Inference Time: 0.2334ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 150,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 117.84s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.2624, LR: 0.000905
Step 200/2250, Loss: 0.2324, LR: 0.000655
Step 300/2250, Loss: 0.2197, LR: 0.000346
Step 400/2250, Loss: 0.2205, LR: 0.000096
Step 500/2250, Loss: 0.2299, LR: 0.001000
Step 600/2250, Loss: 0.2531, LR: 0.000976
Step 700/2250, Loss: 0.2247, LR: 0.000905
Step 800/2250, Loss: 0.2248, LR: 0.000794
Step 900/2250, Loss: 0.2122, LR: 0.000655
Step 1000/2250, Loss: 0.2068, LR: 0.000501
Step 1100/2250, Loss: 0.2141, LR: 0.000346
Step 1200/2250, Loss: 0.2351, LR: 0.000207
Step 1300/2250, Loss: 0.2029, LR: 0.000096
Step 1400/2250, Loss: 0.2271, LR: 0.000025
Step 1500/2250, Loss: 0.1810, LR: 0.001000
Step 1600/2250, Loss: 0.2384, LR: 0.000994
Step 1700/2250, Loss: 0.1860, LR: 0.000976
Step 1800/2250, Loss: 0.2039, LR: 0.000946
Step 1900/2250, Loss: 0.1985, LR: 0.000905
Step 2000/2250, Loss: 0.2002, LR: 0.000854
Step 2100/2250, Loss: 0.1985, LR: 0.000794
Step 2200/2250, Loss: 0.1997, LR: 0.000727
Training time: 6.26s (2.78ms per step)
Evaluating model...
Hard Accuracy: 0.7134
Soft Accuracy: 0.8476
Evaluation time: 0.22s
Checking axiom satisfaction...
3567
Axiom (anonymity) Satisfaction Rate: 0.7134
3529
Axiom (neutrality) Satisfaction Rate: 0.7058
2648
Axiom (condorcet) Satisfaction Rate: 0.5296
235
Axiom (pareto) Satisfaction Rate: 0.047
Measuring inference time...
Average inference time (single sample): 0.2375ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7134
  Soft Accuracy: 0.8476
  Training Time: 6.26s
  Inference Time: 0.2375ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 64.11s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.2462, LR: 0.000905
Step 200/2250, Loss: 0.2143, LR: 0.000655
Step 300/2250, Loss: 0.2074, LR: 0.000346
Step 400/2250, Loss: 0.2087, LR: 0.000096
Step 500/2250, Loss: 0.1941, LR: 0.001000
Step 600/2250, Loss: 0.2305, LR: 0.000976
Step 700/2250, Loss: 0.2068, LR: 0.000905
Step 800/2250, Loss: 0.1990, LR: 0.000794
Step 900/2250, Loss: 0.2102, LR: 0.000655
Step 1000/2250, Loss: 0.1807, LR: 0.000501
Step 1100/2250, Loss: 0.1708, LR: 0.000346
Step 1200/2250, Loss: 0.1661, LR: 0.000207
Step 1300/2250, Loss: 0.1454, LR: 0.000096
Step 1400/2250, Loss: 0.1490, LR: 0.000025
Step 1500/2250, Loss: 0.1681, LR: 0.001000
Step 1600/2250, Loss: 0.2068, LR: 0.000994
Step 1700/2250, Loss: 0.1984, LR: 0.000976
Step 1800/2250, Loss: 0.1514, LR: 0.000946
Step 1900/2250, Loss: 0.1896, LR: 0.000905
Step 2000/2250, Loss: 0.1430, LR: 0.000854
Step 2100/2250, Loss: 0.1791, LR: 0.000794
Step 2200/2250, Loss: 0.1679, LR: 0.000727
Training time: 8.00s (3.56ms per step)
Evaluating model...
Hard Accuracy: 0.7432
Soft Accuracy: 0.8628
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7432
Axiom (neutrality) Satisfaction Rate: 0.7188
Axiom (condorcet) Satisfaction Rate: 0.538
Axiom (pareto) Satisfaction Rate: 0.047
Measuring inference time...
Average inference time (single sample): 0.2746ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7432
  Soft Accuracy: 0.8628
  Training Time: 8.00s
  Inference Time: 0.2746ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 150,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 58.63s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.2387, LR: 0.000905
Step 200/2250, Loss: 0.1370, LR: 0.000655
Step 300/2250, Loss: 0.0936, LR: 0.000346
Step 400/2250, Loss: 0.0870, LR: 0.000096
Step 500/2250, Loss: 0.0983, LR: 0.001000
Step 600/2250, Loss: 0.0837, LR: 0.000976
Step 700/2250, Loss: 0.0520, LR: 0.000905
Step 800/2250, Loss: 0.0419, LR: 0.000794
Step 900/2250, Loss: 0.0271, LR: 0.000655
Step 1000/2250, Loss: 0.0326, LR: 0.000501
Step 1100/2250, Loss: 0.0225, LR: 0.000346
Step 1200/2250, Loss: 0.0077, LR: 0.000207
Step 1300/2250, Loss: 0.0192, LR: 0.000096
Step 1400/2250, Loss: 0.0081, LR: 0.000025
Step 1500/2250, Loss: 0.0167, LR: 0.001000
Step 1600/2250, Loss: 0.0405, LR: 0.000994
Step 1700/2250, Loss: 0.0513, LR: 0.000976
Step 1800/2250, Loss: 0.0273, LR: 0.000946
Step 1900/2250, Loss: 0.0197, LR: 0.000905
Step 2000/2250, Loss: 0.0232, LR: 0.000854
Step 2100/2250, Loss: 0.0207, LR: 0.000794
Step 2200/2250, Loss: 0.0150, LR: 0.000727
Training time: 10.25s (4.56ms per step)
Evaluating model...
Hard Accuracy: 0.9704
Soft Accuracy: 0.9992
Evaluation time: 0.24s
Checking axiom satisfaction...
4852
Axiom (anonymity) Satisfaction Rate: 0.9704
4559
Axiom (neutrality) Satisfaction Rate: 0.9118
2721
Axiom (condorcet) Satisfaction Rate: 0.5442
249
Axiom (pareto) Satisfaction Rate: 0.0498
Measuring inference time...
Average inference time (single sample): 0.2499ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9704
  Soft Accuracy: 0.9992
  Training Time: 10.25s
  Inference Time: 0.2499ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 500,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 391.85s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.2791, LR: 0.000905
Step 200/7500, Loss: 0.2315, LR: 0.000655
Step 300/7500, Loss: 0.2629, LR: 0.000346
Step 400/7500, Loss: 0.2010, LR: 0.000096
Step 500/7500, Loss: 0.2130, LR: 0.001000
Step 600/7500, Loss: 0.2195, LR: 0.000976
Step 700/7500, Loss: 0.2256, LR: 0.000905
Step 800/7500, Loss: 0.2086, LR: 0.000794
Step 900/7500, Loss: 0.2354, LR: 0.000655
Step 1000/7500, Loss: 0.2716, LR: 0.000501
Step 1100/7500, Loss: 0.1852, LR: 0.000346
Step 1200/7500, Loss: 0.1988, LR: 0.000207
Step 1300/7500, Loss: 0.2019, LR: 0.000096
Step 1400/7500, Loss: 0.2083, LR: 0.000025
Step 1500/7500, Loss: 0.2104, LR: 0.001000
Step 1600/7500, Loss: 0.2088, LR: 0.000994
Step 1700/7500, Loss: 0.1675, LR: 0.000976
Step 1800/7500, Loss: 0.2247, LR: 0.000946
Step 1900/7500, Loss: 0.1843, LR: 0.000905
Step 2000/7500, Loss: 0.2086, LR: 0.000854
Step 2100/7500, Loss: 0.1785, LR: 0.000794
Step 2200/7500, Loss: 0.2474, LR: 0.000727
Step 2300/7500, Loss: 0.1904, LR: 0.000655
Step 2400/7500, Loss: 0.1723, LR: 0.000579
Step 2500/7500, Loss: 0.1769, LR: 0.000501
Step 2600/7500, Loss: 0.2091, LR: 0.000422
Step 2700/7500, Loss: 0.1766, LR: 0.000346
Step 2800/7500, Loss: 0.2040, LR: 0.000274
Step 2900/7500, Loss: 0.2165, LR: 0.000207
Step 3000/7500, Loss: 0.1992, LR: 0.000147
Step 3100/7500, Loss: 0.1947, LR: 0.000096
Step 3200/7500, Loss: 0.2193, LR: 0.000055
Step 3300/7500, Loss: 0.1975, LR: 0.000025
Step 3400/7500, Loss: 0.2208, LR: 0.000007
Step 3500/7500, Loss: 0.2271, LR: 0.001000
Step 3600/7500, Loss: 0.1858, LR: 0.000998
Step 3700/7500, Loss: 0.1892, LR: 0.000994
Step 3800/7500, Loss: 0.2345, LR: 0.000986
Step 3900/7500, Loss: 0.2076, LR: 0.000976
Step 4000/7500, Loss: 0.2175, LR: 0.000962
Step 4100/7500, Loss: 0.2274, LR: 0.000946
Step 4200/7500, Loss: 0.1846, LR: 0.000926
Step 4300/7500, Loss: 0.1666, LR: 0.000905
Step 4400/7500, Loss: 0.2138, LR: 0.000880
Step 4500/7500, Loss: 0.1994, LR: 0.000854
Step 4600/7500, Loss: 0.2367, LR: 0.000825
Step 4700/7500, Loss: 0.2221, LR: 0.000794
Step 4800/7500, Loss: 0.1383, LR: 0.000761
Step 4900/7500, Loss: 0.1903, LR: 0.000727
Step 5000/7500, Loss: 0.1499, LR: 0.000692
Step 5100/7500, Loss: 0.1821, LR: 0.000655
Step 5200/7500, Loss: 0.1851, LR: 0.000617
Step 5300/7500, Loss: 0.1679, LR: 0.000579
Step 5400/7500, Loss: 0.2041, LR: 0.000540
Step 5500/7500, Loss: 0.2062, LR: 0.000501
Step 5600/7500, Loss: 0.2030, LR: 0.000461
Step 5700/7500, Loss: 0.2098, LR: 0.000422
Step 5800/7500, Loss: 0.2227, LR: 0.000384
Step 5900/7500, Loss: 0.1723, LR: 0.000346
Step 6000/7500, Loss: 0.2163, LR: 0.000309
Step 6100/7500, Loss: 0.1999, LR: 0.000274
Step 6200/7500, Loss: 0.1760, LR: 0.000240
Step 6300/7500, Loss: 0.1838, LR: 0.000207
Step 6400/7500, Loss: 0.1686, LR: 0.000176
Step 6500/7500, Loss: 0.1930, LR: 0.000147
Step 6600/7500, Loss: 0.1820, LR: 0.000121
Step 6700/7500, Loss: 0.2087, LR: 0.000096
Step 6800/7500, Loss: 0.1963, LR: 0.000075
Step 6900/7500, Loss: 0.1889, LR: 0.000055
Step 7000/7500, Loss: 0.2378, LR: 0.000039
Step 7100/7500, Loss: 0.2097, LR: 0.000025
Step 7200/7500, Loss: 0.2104, LR: 0.000015
Step 7300/7500, Loss: 0.1958, LR: 0.000007
Step 7400/7500, Loss: 0.2376, LR: 0.000003
Step 7500/7500, Loss: 0.2048, LR: 0.001000
Training time: 19.63s (2.62ms per step)
Evaluating model...
Hard Accuracy: 0.7196
Soft Accuracy: 0.8536
Evaluation time: 0.22s
Checking axiom satisfaction...
3598
Axiom (anonymity) Satisfaction Rate: 0.7196
3561
Axiom (neutrality) Satisfaction Rate: 0.7122
2648
Axiom (condorcet) Satisfaction Rate: 0.5296
227
Axiom (pareto) Satisfaction Rate: 0.0454
Measuring inference time...
Average inference time (single sample): 0.2397ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7196
  Soft Accuracy: 0.8536
  Training Time: 19.63s
  Inference Time: 0.2397ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 214.68s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.2170, LR: 0.000905
Step 200/7500, Loss: 0.1932, LR: 0.000655
Step 300/7500, Loss: 0.2090, LR: 0.000346
Step 400/7500, Loss: 0.1795, LR: 0.000096
Step 500/7500, Loss: 0.1886, LR: 0.001000
Step 600/7500, Loss: 0.2169, LR: 0.000976
Step 700/7500, Loss: 0.1897, LR: 0.000905
Step 800/7500, Loss: 0.1857, LR: 0.000794
Step 900/7500, Loss: 0.2052, LR: 0.000655
Step 1000/7500, Loss: 0.1968, LR: 0.000501
Step 1100/7500, Loss: 0.1545, LR: 0.000346
Step 1200/7500, Loss: 0.1854, LR: 0.000207
Step 1300/7500, Loss: 0.1531, LR: 0.000096
Step 1400/7500, Loss: 0.1708, LR: 0.000025
Step 1500/7500, Loss: 0.1703, LR: 0.001000
Step 1600/7500, Loss: 0.1871, LR: 0.000994
Step 1700/7500, Loss: 0.1754, LR: 0.000976
Step 1800/7500, Loss: 0.2002, LR: 0.000946
Step 1900/7500, Loss: 0.1771, LR: 0.000905
Step 2000/7500, Loss: 0.1734, LR: 0.000854
Step 2100/7500, Loss: 0.1655, LR: 0.000794
Step 2200/7500, Loss: 0.1972, LR: 0.000727
Step 2300/7500, Loss: 0.1641, LR: 0.000655
Step 2400/7500, Loss: 0.1911, LR: 0.000579
Step 2500/7500, Loss: 0.1796, LR: 0.000501
Step 2600/7500, Loss: 0.2080, LR: 0.000422
Step 2700/7500, Loss: 0.1378, LR: 0.000346
Step 2800/7500, Loss: 0.1688, LR: 0.000274
Step 2900/7500, Loss: 0.1471, LR: 0.000207
Step 3000/7500, Loss: 0.1543, LR: 0.000147
Step 3100/7500, Loss: 0.1597, LR: 0.000096
Step 3200/7500, Loss: 0.1360, LR: 0.000055
Step 3300/7500, Loss: 0.1823, LR: 0.000025
Step 3400/7500, Loss: 0.1433, LR: 0.000007
Step 3500/7500, Loss: 0.1254, LR: 0.001000
Step 3600/7500, Loss: 0.1658, LR: 0.000998
Step 3700/7500, Loss: 0.1505, LR: 0.000994
Step 3800/7500, Loss: 0.1833, LR: 0.000986
Step 3900/7500, Loss: 0.1750, LR: 0.000976
Step 4000/7500, Loss: 0.1664, LR: 0.000962
Step 4100/7500, Loss: 0.1674, LR: 0.000946
Step 4200/7500, Loss: 0.1298, LR: 0.000926
Step 4300/7500, Loss: 0.1713, LR: 0.000905
Step 4400/7500, Loss: 0.1456, LR: 0.000880
Step 4500/7500, Loss: 0.1885, LR: 0.000854
Step 4600/7500, Loss: 0.1323, LR: 0.000825
Step 4700/7500, Loss: 0.1316, LR: 0.000794
Step 4800/7500, Loss: 0.1494, LR: 0.000761
Step 4900/7500, Loss: 0.1356, LR: 0.000727
Step 5000/7500, Loss: 0.1243, LR: 0.000692
Step 5100/7500, Loss: 0.1271, LR: 0.000655
Step 5200/7500, Loss: 0.1445, LR: 0.000617
Step 5300/7500, Loss: 0.1461, LR: 0.000579
Step 5400/7500, Loss: 0.1396, LR: 0.000540
Step 5500/7500, Loss: 0.1196, LR: 0.000501
Step 5600/7500, Loss: 0.1545, LR: 0.000461
Step 5700/7500, Loss: 0.1312, LR: 0.000422
Step 5800/7500, Loss: 0.1328, LR: 0.000384
Step 5900/7500, Loss: 0.1343, LR: 0.000346
Step 6000/7500, Loss: 0.1456, LR: 0.000309
Step 6100/7500, Loss: 0.1392, LR: 0.000274
Step 6200/7500, Loss: 0.1426, LR: 0.000240
Step 6300/7500, Loss: 0.1368, LR: 0.000207
Step 6400/7500, Loss: 0.1183, LR: 0.000176
Step 6500/7500, Loss: 0.1063, LR: 0.000147
Step 6600/7500, Loss: 0.1272, LR: 0.000121
Step 6700/7500, Loss: 0.1118, LR: 0.000096
Step 6800/7500, Loss: 0.1303, LR: 0.000075
Step 6900/7500, Loss: 0.1250, LR: 0.000055
Step 7000/7500, Loss: 0.1476, LR: 0.000039
Step 7100/7500, Loss: 0.1284, LR: 0.000025
Step 7200/7500, Loss: 0.1038, LR: 0.000015
Step 7300/7500, Loss: 0.1368, LR: 0.000007
Step 7400/7500, Loss: 0.1137, LR: 0.000003
Step 7500/7500, Loss: 0.1186, LR: 0.001000
Training time: 24.82s (3.31ms per step)
Evaluating model...
Hard Accuracy: 0.796
Soft Accuracy: 0.9044
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.796
Axiom (neutrality) Satisfaction Rate: 0.7698
Axiom (condorcet) Satisfaction Rate: 0.5442
Axiom (pareto) Satisfaction Rate: 0.047
Measuring inference time...
Average inference time (single sample): 0.2786ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.7960
  Soft Accuracy: 0.9044
  Training Time: 24.82s
  Inference Time: 0.2786ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 500,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 190.88s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.2264, LR: 0.000905
Step 200/7500, Loss: 0.1506, LR: 0.000655
Step 300/7500, Loss: 0.1082, LR: 0.000346
Step 400/7500, Loss: 0.0974, LR: 0.000096
Step 500/7500, Loss: 0.0829, LR: 0.001000
Step 600/7500, Loss: 0.0842, LR: 0.000976
Step 700/7500, Loss: 0.0572, LR: 0.000905
Step 800/7500, Loss: 0.0343, LR: 0.000794
Step 900/7500, Loss: 0.0488, LR: 0.000655
Step 1000/7500, Loss: 0.0207, LR: 0.000501
Step 1100/7500, Loss: 0.0247, LR: 0.000346
Step 1200/7500, Loss: 0.0180, LR: 0.000207
Step 1300/7500, Loss: 0.0121, LR: 0.000096
Step 1400/7500, Loss: 0.0174, LR: 0.000025
Step 1500/7500, Loss: 0.0168, LR: 0.001000
Step 1600/7500, Loss: 0.0332, LR: 0.000994
Step 1700/7500, Loss: 0.0267, LR: 0.000976
Step 1800/7500, Loss: 0.0312, LR: 0.000946
Step 1900/7500, Loss: 0.0143, LR: 0.000905
Step 2000/7500, Loss: 0.0256, LR: 0.000854
Step 2100/7500, Loss: 0.0335, LR: 0.000794
Step 2200/7500, Loss: 0.0217, LR: 0.000727
Step 2300/7500, Loss: 0.0120, LR: 0.000655
Step 2400/7500, Loss: 0.0128, LR: 0.000579
Step 2500/7500, Loss: 0.0103, LR: 0.000501
Step 2600/7500, Loss: 0.0040, LR: 0.000422
Step 2700/7500, Loss: 0.0114, LR: 0.000346
Step 2800/7500, Loss: 0.0058, LR: 0.000274
Step 2900/7500, Loss: 0.0046, LR: 0.000207
Step 3000/7500, Loss: 0.0042, LR: 0.000147
Step 3100/7500, Loss: 0.0020, LR: 0.000096
Step 3200/7500, Loss: 0.0030, LR: 0.000055
Step 3300/7500, Loss: 0.0026, LR: 0.000025
Step 3400/7500, Loss: 0.0018, LR: 0.000007
Step 3500/7500, Loss: 0.0024, LR: 0.001000
Step 3600/7500, Loss: 0.0511, LR: 0.000998
Step 3700/7500, Loss: 0.0257, LR: 0.000994
Step 3800/7500, Loss: 0.0218, LR: 0.000986
Step 3900/7500, Loss: 0.0161, LR: 0.000976
Step 4000/7500, Loss: 0.0362, LR: 0.000962
Step 4100/7500, Loss: 0.0237, LR: 0.000946
Step 4200/7500, Loss: 0.0156, LR: 0.000926
Step 4300/7500, Loss: 0.0243, LR: 0.000905
Step 4400/7500, Loss: 0.0211, LR: 0.000880
Step 4500/7500, Loss: 0.0176, LR: 0.000854
Step 4600/7500, Loss: 0.0161, LR: 0.000825
Step 4700/7500, Loss: 0.0091, LR: 0.000794
Step 4800/7500, Loss: 0.0092, LR: 0.000761
Step 4900/7500, Loss: 0.0077, LR: 0.000727
Step 5000/7500, Loss: 0.0096, LR: 0.000692
Step 5100/7500, Loss: 0.0079, LR: 0.000655
Step 5200/7500, Loss: 0.0083, LR: 0.000617
Step 5300/7500, Loss: 0.0053, LR: 0.000579
Step 5400/7500, Loss: 0.0180, LR: 0.000540
Step 5500/7500, Loss: 0.0049, LR: 0.000501
Step 5600/7500, Loss: 0.0026, LR: 0.000461
Step 5700/7500, Loss: 0.0055, LR: 0.000422
Step 5800/7500, Loss: 0.0076, LR: 0.000384
Step 5900/7500, Loss: 0.0016, LR: 0.000346
Step 6000/7500, Loss: 0.0022, LR: 0.000309
Step 6100/7500, Loss: 0.0057, LR: 0.000274
Step 6200/7500, Loss: 0.0011, LR: 0.000240
Step 6300/7500, Loss: 0.0005, LR: 0.000207
Step 6400/7500, Loss: 0.0007, LR: 0.000176
Step 6500/7500, Loss: 0.0005, LR: 0.000147
Step 6600/7500, Loss: 0.0004, LR: 0.000121
Step 6700/7500, Loss: 0.0006, LR: 0.000096
Step 6800/7500, Loss: 0.0022, LR: 0.000075
Step 6900/7500, Loss: 0.0008, LR: 0.000055
Step 7000/7500, Loss: 0.0003, LR: 0.000039
Step 7100/7500, Loss: 0.0019, LR: 0.000025
Step 7200/7500, Loss: 0.0005, LR: 0.000015
Step 7300/7500, Loss: 0.0008, LR: 0.000007
Step 7400/7500, Loss: 0.0005, LR: 0.000003
Step 7500/7500, Loss: 0.0003, LR: 0.001000
Training time: 27.64s (3.69ms per step)
Evaluating model...
Hard Accuracy: 0.9986
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4993
Axiom (anonymity) Satisfaction Rate: 0.9986
4613
Axiom (neutrality) Satisfaction Rate: 0.9226
2790
Axiom (condorcet) Satisfaction Rate: 0.558
259
Axiom (pareto) Satisfaction Rate: 0.0518
Measuring inference time...
Average inference time (single sample): 0.2409ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 0.9986
  Soft Accuracy: 1.0000
  Training Time: 27.64s
  Inference Time: 0.2409ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 1,000,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with PLURALITY
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 798.04s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.2873, LR: 0.000905
Step 200/15000, Loss: 0.2327, LR: 0.000655
Step 300/15000, Loss: 0.2157, LR: 0.000346
Step 400/15000, Loss: 0.2548, LR: 0.000096
Step 500/15000, Loss: 0.2164, LR: 0.001000
Step 600/15000, Loss: 0.2548, LR: 0.000976
Step 700/15000, Loss: 0.2139, LR: 0.000905
Step 800/15000, Loss: 0.2481, LR: 0.000794
Step 900/15000, Loss: 0.1843, LR: 0.000655
Step 1000/15000, Loss: 0.2318, LR: 0.000501
Step 1100/15000, Loss: 0.2246, LR: 0.000346
Step 1200/15000, Loss: 0.1916, LR: 0.000207
Step 1300/15000, Loss: 0.2378, LR: 0.000096
Step 1400/15000, Loss: 0.2015, LR: 0.000025
Step 1500/15000, Loss: 0.2545, LR: 0.001000
Step 1600/15000, Loss: 0.1784, LR: 0.000994
Step 1700/15000, Loss: 0.2114, LR: 0.000976
Step 1800/15000, Loss: 0.2302, LR: 0.000946
Step 1900/15000, Loss: 0.2219, LR: 0.000905
Step 2000/15000, Loss: 0.2157, LR: 0.000854
Step 2100/15000, Loss: 0.1793, LR: 0.000794
Step 2200/15000, Loss: 0.2172, LR: 0.000727
Step 2300/15000, Loss: 0.2507, LR: 0.000655
Step 2400/15000, Loss: 0.2164, LR: 0.000579
Step 2500/15000, Loss: 0.2175, LR: 0.000501
Step 2600/15000, Loss: 0.1750, LR: 0.000422
Step 2700/15000, Loss: 0.1977, LR: 0.000346
Step 2800/15000, Loss: 0.1953, LR: 0.000274
Step 2900/15000, Loss: 0.2187, LR: 0.000207
Step 3000/15000, Loss: 0.1949, LR: 0.000147
Step 3100/15000, Loss: 0.1838, LR: 0.000096
Step 3200/15000, Loss: 0.1983, LR: 0.000055
Step 3300/15000, Loss: 0.2633, LR: 0.000025
Step 3400/15000, Loss: 0.1863, LR: 0.000007
Step 3500/15000, Loss: 0.2071, LR: 0.001000
Step 3600/15000, Loss: 0.2371, LR: 0.000998
Step 3700/15000, Loss: 0.2272, LR: 0.000994
Step 3800/15000, Loss: 0.1942, LR: 0.000986
Step 3900/15000, Loss: 0.2034, LR: 0.000976
Step 4000/15000, Loss: 0.2100, LR: 0.000962
Step 4100/15000, Loss: 0.2044, LR: 0.000946
Step 4200/15000, Loss: 0.2141, LR: 0.000926
Step 4300/15000, Loss: 0.2067, LR: 0.000905
Step 4400/15000, Loss: 0.1681, LR: 0.000880
Step 4500/15000, Loss: 0.2226, LR: 0.000854
Step 4600/15000, Loss: 0.1738, LR: 0.000825
Step 4700/15000, Loss: 0.1983, LR: 0.000794
Step 4800/15000, Loss: 0.1719, LR: 0.000761
Step 4900/15000, Loss: 0.2033, LR: 0.000727
Step 5000/15000, Loss: 0.1611, LR: 0.000692
Step 5100/15000, Loss: 0.2056, LR: 0.000655
Step 5200/15000, Loss: 0.1906, LR: 0.000617
Step 5300/15000, Loss: 0.1856, LR: 0.000579
Step 5400/15000, Loss: 0.2069, LR: 0.000540
Step 5500/15000, Loss: 0.2024, LR: 0.000501
Step 5600/15000, Loss: 0.1829, LR: 0.000461
Step 5700/15000, Loss: 0.1774, LR: 0.000422
Step 5800/15000, Loss: 0.1681, LR: 0.000384
Step 5900/15000, Loss: 0.1830, LR: 0.000346
Step 6000/15000, Loss: 0.2161, LR: 0.000309
Step 6100/15000, Loss: 0.2002, LR: 0.000274
Step 6200/15000, Loss: 0.2089, LR: 0.000240
Step 6300/15000, Loss: 0.1956, LR: 0.000207
Step 6400/15000, Loss: 0.2022, LR: 0.000176
Step 6500/15000, Loss: 0.1659, LR: 0.000147
Step 6600/15000, Loss: 0.2024, LR: 0.000121
Step 6700/15000, Loss: 0.1899, LR: 0.000096
Step 6800/15000, Loss: 0.1985, LR: 0.000075
Step 6900/15000, Loss: 0.1989, LR: 0.000055
Step 7000/15000, Loss: 0.1695, LR: 0.000039
Step 7100/15000, Loss: 0.1916, LR: 0.000025
Step 7200/15000, Loss: 0.1804, LR: 0.000015
Step 7300/15000, Loss: 0.2055, LR: 0.000007
Step 7400/15000, Loss: 0.1973, LR: 0.000003
Step 7500/15000, Loss: 0.2322, LR: 0.001000
Step 7600/15000, Loss: 0.2097, LR: 0.001000
Step 7700/15000, Loss: 0.2097, LR: 0.000998
Step 7800/15000, Loss: 0.2022, LR: 0.000997
Step 7900/15000, Loss: 0.2169, LR: 0.000994
Step 8000/15000, Loss: 0.2134, LR: 0.000990
Step 8100/15000, Loss: 0.2470, LR: 0.000986
Step 8200/15000, Loss: 0.1992, LR: 0.000981
Step 8300/15000, Loss: 0.2089, LR: 0.000976
Step 8400/15000, Loss: 0.2126, LR: 0.000969
Step 8500/15000, Loss: 0.2140, LR: 0.000962
Step 8600/15000, Loss: 0.1854, LR: 0.000954
Step 8700/15000, Loss: 0.1706, LR: 0.000946
Step 8800/15000, Loss: 0.2221, LR: 0.000936
Step 8900/15000, Loss: 0.2633, LR: 0.000926
Step 9000/15000, Loss: 0.1649, LR: 0.000916
Step 9100/15000, Loss: 0.2211, LR: 0.000905
Step 9200/15000, Loss: 0.2088, LR: 0.000893
Step 9300/15000, Loss: 0.1535, LR: 0.000880
Step 9400/15000, Loss: 0.2073, LR: 0.000867
Step 9500/15000, Loss: 0.2177, LR: 0.000854
Step 9600/15000, Loss: 0.2258, LR: 0.000840
Step 9700/15000, Loss: 0.1966, LR: 0.000825
Step 9800/15000, Loss: 0.2054, LR: 0.000810
Step 9900/15000, Loss: 0.2543, LR: 0.000794
Step 10000/15000, Loss: 0.1776, LR: 0.000778
Step 10100/15000, Loss: 0.2147, LR: 0.000761
Step 10200/15000, Loss: 0.1810, LR: 0.000745
Step 10300/15000, Loss: 0.1813, LR: 0.000727
Step 10400/15000, Loss: 0.2080, LR: 0.000710
Step 10500/15000, Loss: 0.1871, LR: 0.000692
Step 10600/15000, Loss: 0.1924, LR: 0.000673
Step 10700/15000, Loss: 0.1845, LR: 0.000655
Step 10800/15000, Loss: 0.1744, LR: 0.000636
Step 10900/15000, Loss: 0.2019, LR: 0.000617
Step 11000/15000, Loss: 0.2092, LR: 0.000598
Step 11100/15000, Loss: 0.1858, LR: 0.000579
Step 11200/15000, Loss: 0.2087, LR: 0.000559
Step 11300/15000, Loss: 0.2177, LR: 0.000540
Step 11400/15000, Loss: 0.2230, LR: 0.000520
Step 11500/15000, Loss: 0.1979, LR: 0.000501
Step 11600/15000, Loss: 0.1828, LR: 0.000481
Step 11700/15000, Loss: 0.2176, LR: 0.000461
Step 11800/15000, Loss: 0.1741, LR: 0.000442
Step 11900/15000, Loss: 0.1918, LR: 0.000422
Step 12000/15000, Loss: 0.1846, LR: 0.000403
Step 12100/15000, Loss: 0.2233, LR: 0.000384
Step 12200/15000, Loss: 0.2183, LR: 0.000365
Step 12300/15000, Loss: 0.1671, LR: 0.000346
Step 12400/15000, Loss: 0.1702, LR: 0.000328
Step 12500/15000, Loss: 0.2344, LR: 0.000309
Step 12600/15000, Loss: 0.1702, LR: 0.000291
Step 12700/15000, Loss: 0.1955, LR: 0.000274
Step 12800/15000, Loss: 0.2255, LR: 0.000256
Step 12900/15000, Loss: 0.2431, LR: 0.000240
Step 13000/15000, Loss: 0.2064, LR: 0.000223
Step 13100/15000, Loss: 0.2085, LR: 0.000207
Step 13200/15000, Loss: 0.2167, LR: 0.000191
Step 13300/15000, Loss: 0.2175, LR: 0.000176
Step 13400/15000, Loss: 0.1992, LR: 0.000161
Step 13500/15000, Loss: 0.1900, LR: 0.000147
Step 13600/15000, Loss: 0.1973, LR: 0.000134
Step 13700/15000, Loss: 0.2074, LR: 0.000121
Step 13800/15000, Loss: 0.1762, LR: 0.000108
Step 13900/15000, Loss: 0.2067, LR: 0.000096
Step 14000/15000, Loss: 0.2345, LR: 0.000085
Step 14100/15000, Loss: 0.1927, LR: 0.000075
Step 14200/15000, Loss: 0.1819, LR: 0.000065
Step 14300/15000, Loss: 0.2269, LR: 0.000055
Step 14400/15000, Loss: 0.2095, LR: 0.000047
Step 14500/15000, Loss: 0.1649, LR: 0.000039
Step 14600/15000, Loss: 0.1816, LR: 0.000032
Step 14700/15000, Loss: 0.1923, LR: 0.000025
Step 14800/15000, Loss: 0.1694, LR: 0.000020
Step 14900/15000, Loss: 0.1719, LR: 0.000015
Step 15000/15000, Loss: 0.1399, LR: 0.000011
Training time: 44.56s (2.97ms per step)
Evaluating model...
Hard Accuracy: 0.731
Soft Accuracy: 0.8608
Evaluation time: 0.77s
Checking axiom satisfaction...
3655
Axiom (anonymity) Satisfaction Rate: 0.731
3616
Axiom (neutrality) Satisfaction Rate: 0.7232
2736
Axiom (condorcet) Satisfaction Rate: 0.5472
224
Axiom (pareto) Satisfaction Rate: 0.0448
Measuring inference time...
Average inference time (single sample): 0.2357ms

PAIRWISE Results (PLURALITY):
  Hard Accuracy: 0.7310
  Soft Accuracy: 0.8608
  Training Time: 44.56s
  Inference Time: 0.2357ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with PLURALITY
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 427.54s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.2416, LR: 0.000905
Step 200/15000, Loss: 0.1936, LR: 0.000655
Step 300/15000, Loss: 0.1824, LR: 0.000346
Step 400/15000, Loss: 0.1851, LR: 0.000096
Step 500/15000, Loss: 0.2076, LR: 0.001000
Step 600/15000, Loss: 0.2265, LR: 0.000976
Step 700/15000, Loss: 0.1901, LR: 0.000905
Step 800/15000, Loss: 0.1952, LR: 0.000794
Step 900/15000, Loss: 0.1987, LR: 0.000655
Step 1000/15000, Loss: 0.1731, LR: 0.000501
Step 1100/15000, Loss: 0.1878, LR: 0.000346
Step 1200/15000, Loss: 0.1893, LR: 0.000207
Step 1300/15000, Loss: 0.1662, LR: 0.000096
Step 1400/15000, Loss: 0.1627, LR: 0.000025
Step 1500/15000, Loss: 0.1759, LR: 0.001000
Step 1600/15000, Loss: 0.1966, LR: 0.000994
Step 1700/15000, Loss: 0.1710, LR: 0.000976
Step 1800/15000, Loss: 0.1963, LR: 0.000946
Step 1900/15000, Loss: 0.1787, LR: 0.000905
Step 2000/15000, Loss: 0.1471, LR: 0.000854
Step 2100/15000, Loss: 0.1720, LR: 0.000794
Step 2200/15000, Loss: 0.1585, LR: 0.000727
Step 2300/15000, Loss: 0.1731, LR: 0.000655
Step 2400/15000, Loss: 0.1804, LR: 0.000579
Step 2500/15000, Loss: 0.1468, LR: 0.000501
Step 2600/15000, Loss: 0.1306, LR: 0.000422
Step 2700/15000, Loss: 0.1904, LR: 0.000346
Step 2800/15000, Loss: 0.1642, LR: 0.000274
Step 2900/15000, Loss: 0.1482, LR: 0.000207
Step 3000/15000, Loss: 0.1531, LR: 0.000147
Step 3100/15000, Loss: 0.1700, LR: 0.000096
Step 3200/15000, Loss: 0.1677, LR: 0.000055
Step 3300/15000, Loss: 0.1412, LR: 0.000025
Step 3400/15000, Loss: 0.1382, LR: 0.000007
Step 3500/15000, Loss: 0.1342, LR: 0.001000
Step 3600/15000, Loss: 0.1790, LR: 0.000998
Step 3700/15000, Loss: 0.1627, LR: 0.000994
Step 3800/15000, Loss: 0.1575, LR: 0.000986
Step 3900/15000, Loss: 0.1911, LR: 0.000976
Step 4000/15000, Loss: 0.1389, LR: 0.000962
Step 4100/15000, Loss: 0.1761, LR: 0.000946
Step 4200/15000, Loss: 0.1478, LR: 0.000926
Step 4300/15000, Loss: 0.1824, LR: 0.000905
Step 4400/15000, Loss: 0.1577, LR: 0.000880
Step 4500/15000, Loss: 0.1847, LR: 0.000854
Step 4600/15000, Loss: 0.1586, LR: 0.000825
Step 4700/15000, Loss: 0.1508, LR: 0.000794
Step 4800/15000, Loss: 0.1408, LR: 0.000761
Step 4900/15000, Loss: 0.1694, LR: 0.000727
Step 5000/15000, Loss: 0.1863, LR: 0.000692
Step 5100/15000, Loss: 0.1661, LR: 0.000655
Step 5200/15000, Loss: 0.1376, LR: 0.000617
Step 5300/15000, Loss: 0.1329, LR: 0.000579
Step 5400/15000, Loss: 0.1600, LR: 0.000540
Step 5500/15000, Loss: 0.1624, LR: 0.000501
Step 5600/15000, Loss: 0.1768, LR: 0.000461
Step 5700/15000, Loss: 0.1470, LR: 0.000422
Step 5800/15000, Loss: 0.1437, LR: 0.000384
Step 5900/15000, Loss: 0.1337, LR: 0.000346
Step 6000/15000, Loss: 0.1316, LR: 0.000309
Step 6100/15000, Loss: 0.1569, LR: 0.000274
Step 6200/15000, Loss: 0.1415, LR: 0.000240
Step 6300/15000, Loss: 0.1664, LR: 0.000207
Step 6400/15000, Loss: 0.1382, LR: 0.000176
Step 6500/15000, Loss: 0.1211, LR: 0.000147
Step 6600/15000, Loss: 0.1358, LR: 0.000121
Step 6700/15000, Loss: 0.1274, LR: 0.000096
Step 6800/15000, Loss: 0.1680, LR: 0.000075
Step 6900/15000, Loss: 0.1256, LR: 0.000055
Step 7000/15000, Loss: 0.1504, LR: 0.000039
Step 7100/15000, Loss: 0.1237, LR: 0.000025
Step 7200/15000, Loss: 0.1495, LR: 0.000015
Step 7300/15000, Loss: 0.1223, LR: 0.000007
Step 7400/15000, Loss: 0.1502, LR: 0.000003
Step 7500/15000, Loss: 0.1386, LR: 0.001000
Step 7600/15000, Loss: 0.1302, LR: 0.001000
Step 7700/15000, Loss: 0.1383, LR: 0.000998
Step 7800/15000, Loss: 0.1245, LR: 0.000997
Step 7900/15000, Loss: 0.1294, LR: 0.000994
Step 8000/15000, Loss: 0.1512, LR: 0.000990
Step 8100/15000, Loss: 0.1435, LR: 0.000986
Step 8200/15000, Loss: 0.1474, LR: 0.000981
Step 8300/15000, Loss: 0.1457, LR: 0.000976
Step 8400/15000, Loss: 0.1526, LR: 0.000969
Step 8500/15000, Loss: 0.1486, LR: 0.000962
Step 8600/15000, Loss: 0.1567, LR: 0.000954
Step 8700/15000, Loss: 0.1478, LR: 0.000946
Step 8800/15000, Loss: 0.1357, LR: 0.000936
Step 8900/15000, Loss: 0.1324, LR: 0.000926
Step 9000/15000, Loss: 0.1469, LR: 0.000916
Step 9100/15000, Loss: 0.1357, LR: 0.000905
Step 9200/15000, Loss: 0.1333, LR: 0.000893
Step 9300/15000, Loss: 0.1154, LR: 0.000880
Step 9400/15000, Loss: 0.1584, LR: 0.000867
Step 9500/15000, Loss: 0.1077, LR: 0.000854
Step 9600/15000, Loss: 0.1476, LR: 0.000840
Step 9700/15000, Loss: 0.1441, LR: 0.000825
Step 9800/15000, Loss: 0.1486, LR: 0.000810
Step 9900/15000, Loss: 0.1072, LR: 0.000794
Step 10000/15000, Loss: 0.1597, LR: 0.000778
Step 10100/15000, Loss: 0.1299, LR: 0.000761
Step 10200/15000, Loss: 0.1160, LR: 0.000745
Step 10300/15000, Loss: 0.0959, LR: 0.000727
Step 10400/15000, Loss: 0.1200, LR: 0.000710
Step 10500/15000, Loss: 0.1381, LR: 0.000692
Step 10600/15000, Loss: 0.1189, LR: 0.000673
Step 10700/15000, Loss: 0.1349, LR: 0.000655
Step 10800/15000, Loss: 0.1321, LR: 0.000636
Step 10900/15000, Loss: 0.1063, LR: 0.000617
Step 11000/15000, Loss: 0.1141, LR: 0.000598
Step 11100/15000, Loss: 0.0810, LR: 0.000579
Step 11200/15000, Loss: 0.1176, LR: 0.000559
Step 11300/15000, Loss: 0.1143, LR: 0.000540
Step 11400/15000, Loss: 0.1257, LR: 0.000520
Step 11500/15000, Loss: 0.1239, LR: 0.000501
Step 11600/15000, Loss: 0.1264, LR: 0.000481
Step 11700/15000, Loss: 0.0962, LR: 0.000461
Step 11800/15000, Loss: 0.1233, LR: 0.000442
Step 11900/15000, Loss: 0.1291, LR: 0.000422
Step 12000/15000, Loss: 0.1187, LR: 0.000403
Step 12100/15000, Loss: 0.1049, LR: 0.000384
Step 12200/15000, Loss: 0.1441, LR: 0.000365
Step 12300/15000, Loss: 0.1419, LR: 0.000346
Step 12400/15000, Loss: 0.1452, LR: 0.000328
Step 12500/15000, Loss: 0.0978, LR: 0.000309
Step 12600/15000, Loss: 0.0942, LR: 0.000291
Step 12700/15000, Loss: 0.1113, LR: 0.000274
Step 12800/15000, Loss: 0.1056, LR: 0.000256
Step 12900/15000, Loss: 0.1331, LR: 0.000240
Step 13000/15000, Loss: 0.1101, LR: 0.000223
Step 13100/15000, Loss: 0.1049, LR: 0.000207
Step 13200/15000, Loss: 0.1266, LR: 0.000191
Step 13300/15000, Loss: 0.1089, LR: 0.000176
Step 13400/15000, Loss: 0.1099, LR: 0.000161
Step 13500/15000, Loss: 0.1001, LR: 0.000147
Step 13600/15000, Loss: 0.0954, LR: 0.000134
Step 13700/15000, Loss: 0.1480, LR: 0.000121
Step 13800/15000, Loss: 0.1178, LR: 0.000108
Step 13900/15000, Loss: 0.1072, LR: 0.000096
Step 14000/15000, Loss: 0.1300, LR: 0.000085
Step 14100/15000, Loss: 0.1165, LR: 0.000075
Step 14200/15000, Loss: 0.1039, LR: 0.000065
Step 14300/15000, Loss: 0.1091, LR: 0.000055
Step 14400/15000, Loss: 0.1096, LR: 0.000047
Step 14500/15000, Loss: 0.1410, LR: 0.000039
Step 14600/15000, Loss: 0.1267, LR: 0.000032
Step 14700/15000, Loss: 0.1345, LR: 0.000025
Step 14800/15000, Loss: 0.1325, LR: 0.000020
Step 14900/15000, Loss: 0.0964, LR: 0.000015
Step 15000/15000, Loss: 0.1048, LR: 0.000011
Training time: 48.51s (3.23ms per step)
Evaluating model...
Hard Accuracy: 0.8172
Soft Accuracy: 0.9162
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8172
Axiom (neutrality) Satisfaction Rate: 0.7852
Axiom (condorcet) Satisfaction Rate: 0.5232
Axiom (pareto) Satisfaction Rate: 0.048
Measuring inference time...
Average inference time (single sample): 0.2734ms

PAIRWISE_PER_VOTER Results (PLURALITY):
  Hard Accuracy: 0.8172
  Soft Accuracy: 0.9162
  Training Time: 48.51s
  Inference Time: 0.2734ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with PLURALITY
Dataset size: 1,000,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 362.38s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.2288, LR: 0.000905
Step 200/15000, Loss: 0.1169, LR: 0.000655
Step 300/15000, Loss: 0.0900, LR: 0.000346
Step 400/15000, Loss: 0.1019, LR: 0.000096
Step 500/15000, Loss: 0.1068, LR: 0.001000
Step 600/15000, Loss: 0.0615, LR: 0.000976
Step 700/15000, Loss: 0.0577, LR: 0.000905
Step 800/15000, Loss: 0.0231, LR: 0.000794
Step 900/15000, Loss: 0.0363, LR: 0.000655
Step 1000/15000, Loss: 0.0169, LR: 0.000501
Step 1100/15000, Loss: 0.0136, LR: 0.000346
Step 1200/15000, Loss: 0.0146, LR: 0.000207
Step 1300/15000, Loss: 0.0092, LR: 0.000096
Step 1400/15000, Loss: 0.0077, LR: 0.000025
Step 1500/15000, Loss: 0.0073, LR: 0.001000
Step 1600/15000, Loss: 0.0352, LR: 0.000994
Step 1700/15000, Loss: 0.0243, LR: 0.000976
Step 1800/15000, Loss: 0.0233, LR: 0.000946
Step 1900/15000, Loss: 0.0269, LR: 0.000905
Step 2000/15000, Loss: 0.0176, LR: 0.000854
Step 2100/15000, Loss: 0.0311, LR: 0.000794
Step 2200/15000, Loss: 0.0174, LR: 0.000727
Step 2300/15000, Loss: 0.0162, LR: 0.000655
Step 2400/15000, Loss: 0.0097, LR: 0.000579
Step 2500/15000, Loss: 0.0266, LR: 0.000501
Step 2600/15000, Loss: 0.0033, LR: 0.000422
Step 2700/15000, Loss: 0.0084, LR: 0.000346
Step 2800/15000, Loss: 0.0068, LR: 0.000274
Step 2900/15000, Loss: 0.0077, LR: 0.000207
Step 3000/15000, Loss: 0.0092, LR: 0.000147
Step 3100/15000, Loss: 0.0037, LR: 0.000096
Step 3200/15000, Loss: 0.0032, LR: 0.000055
Step 3300/15000, Loss: 0.0013, LR: 0.000025
Step 3400/15000, Loss: 0.0076, LR: 0.000007
Step 3500/15000, Loss: 0.0026, LR: 0.001000
Step 3600/15000, Loss: 0.0400, LR: 0.000998
Step 3700/15000, Loss: 0.0253, LR: 0.000994
Step 3800/15000, Loss: 0.0240, LR: 0.000986
Step 3900/15000, Loss: 0.0262, LR: 0.000976
Step 4000/15000, Loss: 0.0251, LR: 0.000962
Step 4100/15000, Loss: 0.0160, LR: 0.000946
Step 4200/15000, Loss: 0.0087, LR: 0.000926
Step 4300/15000, Loss: 0.0127, LR: 0.000905
Step 4400/15000, Loss: 0.0133, LR: 0.000880
Step 4500/15000, Loss: 0.0053, LR: 0.000854
Step 4600/15000, Loss: 0.0096, LR: 0.000825
Step 4700/15000, Loss: 0.0101, LR: 0.000794
Step 4800/15000, Loss: 0.0179, LR: 0.000761
Step 4900/15000, Loss: 0.0115, LR: 0.000727
Step 5000/15000, Loss: 0.0107, LR: 0.000692
Step 5100/15000, Loss: 0.0074, LR: 0.000655
Step 5200/15000, Loss: 0.0045, LR: 0.000617
Step 5300/15000, Loss: 0.0103, LR: 0.000579
Step 5400/15000, Loss: 0.0050, LR: 0.000540
Step 5500/15000, Loss: 0.0045, LR: 0.000501
Step 5600/15000, Loss: 0.0026, LR: 0.000461
Step 5700/15000, Loss: 0.0033, LR: 0.000422
Step 5800/15000, Loss: 0.0045, LR: 0.000384
Step 5900/15000, Loss: 0.0058, LR: 0.000346
Step 6000/15000, Loss: 0.0003, LR: 0.000309
Step 6100/15000, Loss: 0.0018, LR: 0.000274
Step 6200/15000, Loss: 0.0038, LR: 0.000240
Step 6300/15000, Loss: 0.0020, LR: 0.000207
Step 6400/15000, Loss: 0.0012, LR: 0.000176
Step 6500/15000, Loss: 0.0003, LR: 0.000147
Step 6600/15000, Loss: 0.0008, LR: 0.000121
Step 6700/15000, Loss: 0.0030, LR: 0.000096
Step 6800/15000, Loss: 0.0011, LR: 0.000075
Step 6900/15000, Loss: 0.0007, LR: 0.000055
Step 7000/15000, Loss: 0.0008, LR: 0.000039
Step 7100/15000, Loss: 0.0002, LR: 0.000025
Step 7200/15000, Loss: 0.0005, LR: 0.000015
Step 7300/15000, Loss: 0.0003, LR: 0.000007
Step 7400/15000, Loss: 0.0003, LR: 0.000003
Step 7500/15000, Loss: 0.0013, LR: 0.001000
Step 7600/15000, Loss: 0.0320, LR: 0.001000
Step 7700/15000, Loss: 0.0187, LR: 0.000998
Step 7800/15000, Loss: 0.0189, LR: 0.000997
Step 7900/15000, Loss: 0.0211, LR: 0.000994
Step 8000/15000, Loss: 0.0201, LR: 0.000990
Step 8100/15000, Loss: 0.0139, LR: 0.000986
Step 8200/15000, Loss: 0.0352, LR: 0.000981
Step 8300/15000, Loss: 0.0177, LR: 0.000976
Step 8400/15000, Loss: 0.0094, LR: 0.000969
Step 8500/15000, Loss: 0.0204, LR: 0.000962
Step 8600/15000, Loss: 0.0141, LR: 0.000954
Step 8700/15000, Loss: 0.0098, LR: 0.000946
Step 8800/15000, Loss: 0.0132, LR: 0.000936
Step 8900/15000, Loss: 0.0073, LR: 0.000926
Step 9000/15000, Loss: 0.0140, LR: 0.000916
Step 9100/15000, Loss: 0.0105, LR: 0.000905
Step 9200/15000, Loss: 0.0239, LR: 0.000893
Step 9300/15000, Loss: 0.0069, LR: 0.000880
Step 9400/15000, Loss: 0.0071, LR: 0.000867
Step 9500/15000, Loss: 0.0131, LR: 0.000854
Step 9600/15000, Loss: 0.0063, LR: 0.000840
Step 9700/15000, Loss: 0.0093, LR: 0.000825
Step 9800/15000, Loss: 0.0185, LR: 0.000810
Step 9900/15000, Loss: 0.0114, LR: 0.000794
Step 10000/15000, Loss: 0.0091, LR: 0.000778
Step 10100/15000, Loss: 0.0054, LR: 0.000761
Step 10200/15000, Loss: 0.0037, LR: 0.000745
Step 10300/15000, Loss: 0.0106, LR: 0.000727
Step 10400/15000, Loss: 0.0021, LR: 0.000710
Step 10500/15000, Loss: 0.0046, LR: 0.000692
Step 10600/15000, Loss: 0.0016, LR: 0.000673
Step 10700/15000, Loss: 0.0037, LR: 0.000655
Step 10800/15000, Loss: 0.0022, LR: 0.000636
Step 10900/15000, Loss: 0.0031, LR: 0.000617
Step 11000/15000, Loss: 0.0012, LR: 0.000598
Step 11100/15000, Loss: 0.0025, LR: 0.000579
Step 11200/15000, Loss: 0.0019, LR: 0.000559
Step 11300/15000, Loss: 0.0023, LR: 0.000540
Step 11400/15000, Loss: 0.0060, LR: 0.000520
Step 11500/15000, Loss: 0.0052, LR: 0.000501
Step 11600/15000, Loss: 0.0033, LR: 0.000481
Step 11700/15000, Loss: 0.0049, LR: 0.000461
Step 11800/15000, Loss: 0.0027, LR: 0.000442
Step 11900/15000, Loss: 0.0069, LR: 0.000422
Step 12000/15000, Loss: 0.0010, LR: 0.000403
Step 12100/15000, Loss: 0.0002, LR: 0.000384
Step 12200/15000, Loss: 0.0011, LR: 0.000365
Step 12300/15000, Loss: 0.0004, LR: 0.000346
Step 12400/15000, Loss: 0.0031, LR: 0.000328
Step 12500/15000, Loss: 0.0007, LR: 0.000309
Step 12600/15000, Loss: 0.0006, LR: 0.000291
Step 12700/15000, Loss: 0.0018, LR: 0.000274
Step 12800/15000, Loss: 0.0012, LR: 0.000256
Step 12900/15000, Loss: 0.0009, LR: 0.000240
Step 13000/15000, Loss: 0.0001, LR: 0.000223
Step 13100/15000, Loss: 0.0001, LR: 0.000207
Step 13200/15000, Loss: 0.0001, LR: 0.000191
Step 13300/15000, Loss: 0.0001, LR: 0.000176
Step 13400/15000, Loss: 0.0001, LR: 0.000161
Step 13500/15000, Loss: 0.0005, LR: 0.000147
Step 13600/15000, Loss: 0.0001, LR: 0.000134
Step 13700/15000, Loss: 0.0001, LR: 0.000121
Step 13800/15000, Loss: 0.0002, LR: 0.000108
Step 13900/15000, Loss: 0.0001, LR: 0.000096
Step 14000/15000, Loss: 0.0001, LR: 0.000085
Step 14100/15000, Loss: 0.0002, LR: 0.000075
Step 14200/15000, Loss: 0.0000, LR: 0.000065
Step 14300/15000, Loss: 0.0001, LR: 0.000055
Step 14400/15000, Loss: 0.0002, LR: 0.000047
Step 14500/15000, Loss: 0.0000, LR: 0.000039
Step 14600/15000, Loss: 0.0001, LR: 0.000032
Step 14700/15000, Loss: 0.0001, LR: 0.000025
Step 14800/15000, Loss: 0.0000, LR: 0.000020
Step 14900/15000, Loss: 0.0005, LR: 0.000015
Step 15000/15000, Loss: 0.0000, LR: 0.000011
Training time: 51.73s (3.45ms per step)
Evaluating model...
Hard Accuracy: 1.0
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
5000
Axiom (anonymity) Satisfaction Rate: 1.0
4623
Axiom (neutrality) Satisfaction Rate: 0.9246
2688
Axiom (condorcet) Satisfaction Rate: 0.5376
260
Axiom (pareto) Satisfaction Rate: 0.052
Measuring inference time...
Average inference time (single sample): 0.2366ms

ONEHOT Results (PLURALITY):
  Hard Accuracy: 1.0000
  Soft Accuracy: 1.0000
  Training Time: 51.73s
  Inference Time: 0.2366ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

################################################################################
# Testing with COPELAND voting method
################################################################################

--------------------------------------------------------------------------------
Dataset size: 1,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 0.80s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.04s (2.86ms per step)
Evaluating model...
Hard Accuracy: 0.0
Soft Accuracy: 0.0
Evaluation time: 0.22s
Checking axiom satisfaction...
0
Axiom (anonymity) Satisfaction Rate: 0.0
0
Axiom (neutrality) Satisfaction Rate: 0.0
3986
Axiom (condorcet) Satisfaction Rate: 0.7972
234
Axiom (pareto) Satisfaction Rate: 0.0468
Measuring inference time...
Average inference time (single sample): 0.2370ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.0000
  Soft Accuracy: 0.0000
  Training Time: 0.04s
  Inference Time: 0.2370ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 1,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 0.42s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.05s (3.60ms per step)
Evaluating model...
Hard Accuracy: 0.3968
Soft Accuracy: 0.5082
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.3968
Axiom (neutrality) Satisfaction Rate: 0.394
Axiom (condorcet) Satisfaction Rate: 0.812
Axiom (pareto) Satisfaction Rate: 0.051
Measuring inference time...
Average inference time (single sample): 0.2753ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.3968
  Soft Accuracy: 0.5082
  Training Time: 0.05s
  Inference Time: 0.2753ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 1,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 0.34s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 15 steps (5 steps/epoch)
Training time: 0.05s (3.61ms per step)
Evaluating model...
Hard Accuracy: 0.0
Soft Accuracy: 0.0
Evaluation time: 0.22s
Checking axiom satisfaction...
0
Axiom (anonymity) Satisfaction Rate: 0.0
0
Axiom (neutrality) Satisfaction Rate: 0.0
4007
Axiom (condorcet) Satisfaction Rate: 0.8014
219
Axiom (pareto) Satisfaction Rate: 0.0438
Measuring inference time...
Average inference time (single sample): 0.2339ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.0000
  Soft Accuracy: 0.0000
  Training Time: 0.05s
  Inference Time: 0.2339ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 5,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 3.93s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.19s (2.55ms per step)
Evaluating model...
Hard Accuracy: 0.7034
Soft Accuracy: 0.8114
Evaluation time: 0.22s
Checking axiom satisfaction...
3517
Axiom (anonymity) Satisfaction Rate: 0.7034
3501
Axiom (neutrality) Satisfaction Rate: 0.7002
4071
Axiom (condorcet) Satisfaction Rate: 0.8142
262
Axiom (pareto) Satisfaction Rate: 0.0524
Measuring inference time...
Average inference time (single sample): 0.2370ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.7034
  Soft Accuracy: 0.8114
  Training Time: 0.19s
  Inference Time: 0.2370ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 5,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 2.14s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.25s (3.29ms per step)
Evaluating model...
Hard Accuracy: 0.7452
Soft Accuracy: 0.8972
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.7452
Axiom (neutrality) Satisfaction Rate: 0.7368
Axiom (condorcet) Satisfaction Rate: 0.809
Axiom (pareto) Satisfaction Rate: 0.052
Measuring inference time...
Average inference time (single sample): 0.2728ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.7452
  Soft Accuracy: 0.8972
  Training Time: 0.25s
  Inference Time: 0.2728ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 5,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 1.74s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 75 steps (25 steps/epoch)
Training time: 0.27s (3.59ms per step)
Evaluating model...
Hard Accuracy: 0.4788
Soft Accuracy: 0.5458
Evaluation time: 0.46s
Checking axiom satisfaction...
2394
Axiom (anonymity) Satisfaction Rate: 0.4788
2391
Axiom (neutrality) Satisfaction Rate: 0.4782
4034
Axiom (condorcet) Satisfaction Rate: 0.8068
216
Axiom (pareto) Satisfaction Rate: 0.0432
Measuring inference time...
Average inference time (single sample): 0.2328ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.4788
  Soft Accuracy: 0.5458
  Training Time: 0.27s
  Inference Time: 0.2328ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 15,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 11.71s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.1901, LR: 0.000905
Step 200/225, Loss: 0.1554, LR: 0.000655
Training time: 0.56s (2.51ms per step)
Evaluating model...
Hard Accuracy: 0.8164
Soft Accuracy: 0.9664
Evaluation time: 0.22s
Checking axiom satisfaction...
4082
Axiom (anonymity) Satisfaction Rate: 0.8164
4036
Axiom (neutrality) Satisfaction Rate: 0.8072
3993
Axiom (condorcet) Satisfaction Rate: 0.7986
250
Axiom (pareto) Satisfaction Rate: 0.05
Measuring inference time...
Average inference time (single sample): 0.2326ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.8164
  Soft Accuracy: 0.9664
  Training Time: 0.56s
  Inference Time: 0.2326ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 15,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 6.51s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.1298, LR: 0.000905
Step 200/225, Loss: 0.0665, LR: 0.000655
Training time: 0.65s (2.88ms per step)
Evaluating model...
Hard Accuracy: 0.8066
Soft Accuracy: 0.9494
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8066
Axiom (neutrality) Satisfaction Rate: 0.7914
Axiom (condorcet) Satisfaction Rate: 0.8178
Axiom (pareto) Satisfaction Rate: 0.0478
Measuring inference time...
Average inference time (single sample): 0.2724ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.8066
  Soft Accuracy: 0.9494
  Training Time: 0.65s
  Inference Time: 0.2724ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 15,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 5.42s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 225 steps (75 steps/epoch)
Step 100/225, Loss: 0.2454, LR: 0.000905
Step 200/225, Loss: 0.1673, LR: 0.000655
Training time: 0.79s (3.49ms per step)
Evaluating model...
Hard Accuracy: 0.7366
Soft Accuracy: 0.8506
Evaluation time: 0.22s
Checking axiom satisfaction...
3683
Axiom (anonymity) Satisfaction Rate: 0.7366
3625
Axiom (neutrality) Satisfaction Rate: 0.725
4014
Axiom (condorcet) Satisfaction Rate: 0.8028
260
Axiom (pareto) Satisfaction Rate: 0.052
Measuring inference time...
Average inference time (single sample): 0.2326ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.7366
  Soft Accuracy: 0.8506
  Training Time: 0.79s
  Inference Time: 0.2326ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 50,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 39.24s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.1969, LR: 0.000905
Step 200/750, Loss: 0.1395, LR: 0.000655
Step 300/750, Loss: 0.1152, LR: 0.000346
Step 400/750, Loss: 0.1422, LR: 0.000096
Step 500/750, Loss: 0.1259, LR: 0.001000
Step 600/750, Loss: 0.1190, LR: 0.000976
Step 700/750, Loss: 0.0684, LR: 0.000905
Training time: 1.91s (2.54ms per step)
Evaluating model...
Hard Accuracy: 0.868
Soft Accuracy: 0.9884
Evaluation time: 0.22s
Checking axiom satisfaction...
4340
Axiom (anonymity) Satisfaction Rate: 0.868
4280
Axiom (neutrality) Satisfaction Rate: 0.856
4010
Axiom (condorcet) Satisfaction Rate: 0.802
217
Axiom (pareto) Satisfaction Rate: 0.0434
Measuring inference time...
Average inference time (single sample): 0.2364ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.8680
  Soft Accuracy: 0.9884
  Training Time: 1.91s
  Inference Time: 0.2364ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 50,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 21.36s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.1468, LR: 0.000905
Step 200/750, Loss: 0.1255, LR: 0.000655
Step 300/750, Loss: 0.1151, LR: 0.000346
Step 400/750, Loss: 0.1070, LR: 0.000096
Step 500/750, Loss: 0.0669, LR: 0.001000
Step 600/750, Loss: 0.1080, LR: 0.000976
Step 700/750, Loss: 0.1327, LR: 0.000905
Training time: 2.18s (2.91ms per step)
Evaluating model...
Hard Accuracy: 0.8078
Soft Accuracy: 0.9562
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.8078
Axiom (neutrality) Satisfaction Rate: 0.7904
Axiom (condorcet) Satisfaction Rate: 0.8032
Axiom (pareto) Satisfaction Rate: 0.0512
Measuring inference time...
Average inference time (single sample): 0.2756ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.8078
  Soft Accuracy: 0.9562
  Training Time: 2.18s
  Inference Time: 0.2756ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 50,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 18.10s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 750 steps (250 steps/epoch)
Step 100/750, Loss: 0.2768, LR: 0.000905
Step 200/750, Loss: 0.2142, LR: 0.000655
Step 300/750, Loss: 0.1347, LR: 0.000346
Step 400/750, Loss: 0.1742, LR: 0.000096
Step 500/750, Loss: 0.1512, LR: 0.001000
Step 600/750, Loss: 0.1363, LR: 0.000976
Step 700/750, Loss: 0.1427, LR: 0.000905
Training time: 4.00s (5.34ms per step)
Evaluating model...
Hard Accuracy: 0.8164
Soft Accuracy: 0.934
Evaluation time: 0.40s
Checking axiom satisfaction...
4082
Axiom (anonymity) Satisfaction Rate: 0.8164
4012
Axiom (neutrality) Satisfaction Rate: 0.8024
4079
Axiom (condorcet) Satisfaction Rate: 0.8158
233
Axiom (pareto) Satisfaction Rate: 0.0466
Measuring inference time...
Average inference time (single sample): 0.2348ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8164
  Soft Accuracy: 0.9340
  Training Time: 4.00s
  Inference Time: 0.2348ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 150,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 119.12s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.2031, LR: 0.000905
Step 200/2250, Loss: 0.1282, LR: 0.000655
Step 300/2250, Loss: 0.1087, LR: 0.000346
Step 400/2250, Loss: 0.1119, LR: 0.000096
Step 500/2250, Loss: 0.1121, LR: 0.001000
Step 600/2250, Loss: 0.1333, LR: 0.000976
Step 700/2250, Loss: 0.0892, LR: 0.000905
Step 800/2250, Loss: 0.0709, LR: 0.000794
Step 900/2250, Loss: 0.0713, LR: 0.000655
Step 1000/2250, Loss: 0.0889, LR: 0.000501
Step 1100/2250, Loss: 0.0752, LR: 0.000346
Step 1200/2250, Loss: 0.1006, LR: 0.000207
Step 1300/2250, Loss: 0.0591, LR: 0.000096
Step 1400/2250, Loss: 0.0685, LR: 0.000025
Step 1500/2250, Loss: 0.0619, LR: 0.001000
Step 1600/2250, Loss: 0.0565, LR: 0.000994
Step 1700/2250, Loss: 0.0756, LR: 0.000976
Step 1800/2250, Loss: 0.0653, LR: 0.000946
Step 1900/2250, Loss: 0.0944, LR: 0.000905
Step 2000/2250, Loss: 0.0703, LR: 0.000854
Step 2100/2250, Loss: 0.0640, LR: 0.000794
Step 2200/2250, Loss: 0.0656, LR: 0.000727
Training time: 5.74s (2.55ms per step)
Evaluating model...
Hard Accuracy: 0.909
Soft Accuracy: 0.9982
Evaluation time: 0.23s
Checking axiom satisfaction...
4545
Axiom (anonymity) Satisfaction Rate: 0.909
4399
Axiom (neutrality) Satisfaction Rate: 0.8798
4016
Axiom (condorcet) Satisfaction Rate: 0.8032
241
Axiom (pareto) Satisfaction Rate: 0.0482
Measuring inference time...
Average inference time (single sample): 0.2390ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9090
  Soft Accuracy: 0.9982
  Training Time: 5.74s
  Inference Time: 0.2390ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 150,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 64.65s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.1564, LR: 0.000905
Step 200/2250, Loss: 0.1141, LR: 0.000655
Step 300/2250, Loss: 0.1411, LR: 0.000346
Step 400/2250, Loss: 0.1159, LR: 0.000096
Step 500/2250, Loss: 0.1093, LR: 0.001000
Step 600/2250, Loss: 0.1120, LR: 0.000976
Step 700/2250, Loss: 0.0981, LR: 0.000905
Step 800/2250, Loss: 0.1140, LR: 0.000794
Step 900/2250, Loss: 0.0979, LR: 0.000655
Step 1000/2250, Loss: 0.0956, LR: 0.000501
Step 1100/2250, Loss: 0.0637, LR: 0.000346
Step 1200/2250, Loss: 0.0610, LR: 0.000207
Step 1300/2250, Loss: 0.0821, LR: 0.000096
Step 1400/2250, Loss: 0.0868, LR: 0.000025
Step 1500/2250, Loss: 0.0958, LR: 0.001000
Step 1600/2250, Loss: 0.0797, LR: 0.000994
Step 1700/2250, Loss: 0.0879, LR: 0.000976
Step 1800/2250, Loss: 0.0828, LR: 0.000946
Step 1900/2250, Loss: 0.0913, LR: 0.000905
Step 2000/2250, Loss: 0.1010, LR: 0.000854
Step 2100/2250, Loss: 0.1014, LR: 0.000794
Step 2200/2250, Loss: 0.0862, LR: 0.000727
Training time: 6.46s (2.87ms per step)
Evaluating model...
Hard Accuracy: 0.875
Soft Accuracy: 0.9808
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.875
Axiom (neutrality) Satisfaction Rate: 0.8432
Axiom (condorcet) Satisfaction Rate: 0.798
Axiom (pareto) Satisfaction Rate: 0.0446
Measuring inference time...
Average inference time (single sample): 0.2693ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.8750
  Soft Accuracy: 0.9808
  Training Time: 6.46s
  Inference Time: 0.2693ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 150,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 53.72s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 2,250 steps (750 steps/epoch)
Step 100/2250, Loss: 0.3194, LR: 0.000905
Step 200/2250, Loss: 0.1944, LR: 0.000655
Step 300/2250, Loss: 0.1809, LR: 0.000346
Step 400/2250, Loss: 0.1842, LR: 0.000096
Step 500/2250, Loss: 0.1262, LR: 0.001000
Step 600/2250, Loss: 0.1540, LR: 0.000976
Step 700/2250, Loss: 0.0935, LR: 0.000905
Step 800/2250, Loss: 0.1228, LR: 0.000794
Step 900/2250, Loss: 0.1279, LR: 0.000655
Step 1000/2250, Loss: 0.1256, LR: 0.000501
Step 1100/2250, Loss: 0.1057, LR: 0.000346
Step 1200/2250, Loss: 0.1163, LR: 0.000207
Step 1300/2250, Loss: 0.1049, LR: 0.000096
Step 1400/2250, Loss: 0.0896, LR: 0.000025
Step 1500/2250, Loss: 0.0815, LR: 0.001000
Step 1600/2250, Loss: 0.1299, LR: 0.000994
Step 1700/2250, Loss: 0.1244, LR: 0.000976
Step 1800/2250, Loss: 0.1350, LR: 0.000946
Step 1900/2250, Loss: 0.0812, LR: 0.000905
Step 2000/2250, Loss: 0.0931, LR: 0.000854
Step 2100/2250, Loss: 0.0940, LR: 0.000794
Step 2200/2250, Loss: 0.1019, LR: 0.000727
Training time: 6.61s (2.94ms per step)
Evaluating model...
Hard Accuracy: 0.8292
Soft Accuracy: 0.961
Evaluation time: 0.22s
Checking axiom satisfaction...
4146
Axiom (anonymity) Satisfaction Rate: 0.8292
4041
Axiom (neutrality) Satisfaction Rate: 0.8082
4044
Axiom (condorcet) Satisfaction Rate: 0.8088
251
Axiom (pareto) Satisfaction Rate: 0.0502
Measuring inference time...
Average inference time (single sample): 0.2346ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8292
  Soft Accuracy: 0.9610
  Training Time: 6.61s
  Inference Time: 0.2346ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 500,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 395.33s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.1828, LR: 0.000905
Step 200/7500, Loss: 0.1276, LR: 0.000655
Step 300/7500, Loss: 0.1458, LR: 0.000346
Step 400/7500, Loss: 0.1095, LR: 0.000096
Step 500/7500, Loss: 0.1242, LR: 0.001000
Step 600/7500, Loss: 0.1142, LR: 0.000976
Step 700/7500, Loss: 0.1179, LR: 0.000905
Step 800/7500, Loss: 0.0967, LR: 0.000794
Step 900/7500, Loss: 0.0968, LR: 0.000655
Step 1000/7500, Loss: 0.0549, LR: 0.000501
Step 1100/7500, Loss: 0.0714, LR: 0.000346
Step 1200/7500, Loss: 0.0693, LR: 0.000207
Step 1300/7500, Loss: 0.0537, LR: 0.000096
Step 1400/7500, Loss: 0.0851, LR: 0.000025
Step 1500/7500, Loss: 0.0839, LR: 0.001000
Step 1600/7500, Loss: 0.0880, LR: 0.000994
Step 1700/7500, Loss: 0.0913, LR: 0.000976
Step 1800/7500, Loss: 0.0751, LR: 0.000946
Step 1900/7500, Loss: 0.0616, LR: 0.000905
Step 2000/7500, Loss: 0.0776, LR: 0.000854
Step 2100/7500, Loss: 0.0675, LR: 0.000794
Step 2200/7500, Loss: 0.0689, LR: 0.000727
Step 2300/7500, Loss: 0.0534, LR: 0.000655
Step 2400/7500, Loss: 0.0718, LR: 0.000579
Step 2500/7500, Loss: 0.0500, LR: 0.000501
Step 2600/7500, Loss: 0.0572, LR: 0.000422
Step 2700/7500, Loss: 0.0556, LR: 0.000346
Step 2800/7500, Loss: 0.0717, LR: 0.000274
Step 2900/7500, Loss: 0.0497, LR: 0.000207
Step 3000/7500, Loss: 0.0397, LR: 0.000147
Step 3100/7500, Loss: 0.0447, LR: 0.000096
Step 3200/7500, Loss: 0.0665, LR: 0.000055
Step 3300/7500, Loss: 0.0510, LR: 0.000025
Step 3400/7500, Loss: 0.0599, LR: 0.000007
Step 3500/7500, Loss: 0.0538, LR: 0.001000
Step 3600/7500, Loss: 0.0498, LR: 0.000998
Step 3700/7500, Loss: 0.0599, LR: 0.000994
Step 3800/7500, Loss: 0.0595, LR: 0.000986
Step 3900/7500, Loss: 0.0497, LR: 0.000976
Step 4000/7500, Loss: 0.0486, LR: 0.000962
Step 4100/7500, Loss: 0.0447, LR: 0.000946
Step 4200/7500, Loss: 0.0566, LR: 0.000926
Step 4300/7500, Loss: 0.0690, LR: 0.000905
Step 4400/7500, Loss: 0.0370, LR: 0.000880
Step 4500/7500, Loss: 0.0314, LR: 0.000854
Step 4600/7500, Loss: 0.0550, LR: 0.000825
Step 4700/7500, Loss: 0.0409, LR: 0.000794
Step 4800/7500, Loss: 0.0243, LR: 0.000761
Step 4900/7500, Loss: 0.0501, LR: 0.000727
Step 5000/7500, Loss: 0.0192, LR: 0.000692
Step 5100/7500, Loss: 0.0416, LR: 0.000655
Step 5200/7500, Loss: 0.0489, LR: 0.000617
Step 5300/7500, Loss: 0.0329, LR: 0.000579
Step 5400/7500, Loss: 0.0307, LR: 0.000540
Step 5500/7500, Loss: 0.0325, LR: 0.000501
Step 5600/7500, Loss: 0.0343, LR: 0.000461
Step 5700/7500, Loss: 0.0324, LR: 0.000422
Step 5800/7500, Loss: 0.0291, LR: 0.000384
Step 5900/7500, Loss: 0.0393, LR: 0.000346
Step 6000/7500, Loss: 0.0244, LR: 0.000309
Step 6100/7500, Loss: 0.0389, LR: 0.000274
Step 6200/7500, Loss: 0.0403, LR: 0.000240
Step 6300/7500, Loss: 0.0162, LR: 0.000207
Step 6400/7500, Loss: 0.0255, LR: 0.000176
Step 6500/7500, Loss: 0.0329, LR: 0.000147
Step 6600/7500, Loss: 0.0223, LR: 0.000121
Step 6700/7500, Loss: 0.0283, LR: 0.000096
Step 6800/7500, Loss: 0.0283, LR: 0.000075
Step 6900/7500, Loss: 0.0480, LR: 0.000055
Step 7000/7500, Loss: 0.0412, LR: 0.000039
Step 7100/7500, Loss: 0.0360, LR: 0.000025
Step 7200/7500, Loss: 0.0206, LR: 0.000015
Step 7300/7500, Loss: 0.0222, LR: 0.000007
Step 7400/7500, Loss: 0.0344, LR: 0.000003
Step 7500/7500, Loss: 0.0246, LR: 0.001000
Training time: 20.32s (2.71ms per step)
Evaluating model...
Hard Accuracy: 0.9604
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4802
Axiom (anonymity) Satisfaction Rate: 0.9604
4553
Axiom (neutrality) Satisfaction Rate: 0.9106
4012
Axiom (condorcet) Satisfaction Rate: 0.8024
244
Axiom (pareto) Satisfaction Rate: 0.0488
Measuring inference time...
Average inference time (single sample): 0.2358ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9604
  Soft Accuracy: 1.0000
  Training Time: 20.32s
  Inference Time: 0.2358ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 500,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 216.22s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.1560, LR: 0.000905
Step 200/7500, Loss: 0.1182, LR: 0.000655
Step 300/7500, Loss: 0.1231, LR: 0.000346
Step 400/7500, Loss: 0.1292, LR: 0.000096
Step 500/7500, Loss: 0.1178, LR: 0.001000
Step 600/7500, Loss: 0.1361, LR: 0.000976
Step 700/7500, Loss: 0.1291, LR: 0.000905
Step 800/7500, Loss: 0.1090, LR: 0.000794
Step 900/7500, Loss: 0.0965, LR: 0.000655
Step 1000/7500, Loss: 0.1121, LR: 0.000501
Step 1100/7500, Loss: 0.0955, LR: 0.000346
Step 1200/7500, Loss: 0.0965, LR: 0.000207
Step 1300/7500, Loss: 0.0830, LR: 0.000096
Step 1400/7500, Loss: 0.0774, LR: 0.000025
Step 1500/7500, Loss: 0.0980, LR: 0.001000
Step 1600/7500, Loss: 0.1151, LR: 0.000994
Step 1700/7500, Loss: 0.1107, LR: 0.000976
Step 1800/7500, Loss: 0.0761, LR: 0.000946
Step 1900/7500, Loss: 0.1004, LR: 0.000905
Step 2000/7500, Loss: 0.0848, LR: 0.000854
Step 2100/7500, Loss: 0.0729, LR: 0.000794
Step 2200/7500, Loss: 0.0696, LR: 0.000727
Step 2300/7500, Loss: 0.0818, LR: 0.000655
Step 2400/7500, Loss: 0.0807, LR: 0.000579
Step 2500/7500, Loss: 0.0781, LR: 0.000501
Step 2600/7500, Loss: 0.0584, LR: 0.000422
Step 2700/7500, Loss: 0.0440, LR: 0.000346
Step 2800/7500, Loss: 0.0599, LR: 0.000274
Step 2900/7500, Loss: 0.0570, LR: 0.000207
Step 3000/7500, Loss: 0.0563, LR: 0.000147
Step 3100/7500, Loss: 0.0417, LR: 0.000096
Step 3200/7500, Loss: 0.0498, LR: 0.000055
Step 3300/7500, Loss: 0.0551, LR: 0.000025
Step 3400/7500, Loss: 0.0438, LR: 0.000007
Step 3500/7500, Loss: 0.0749, LR: 0.001000
Step 3600/7500, Loss: 0.0938, LR: 0.000998
Step 3700/7500, Loss: 0.0696, LR: 0.000994
Step 3800/7500, Loss: 0.0601, LR: 0.000986
Step 3900/7500, Loss: 0.1003, LR: 0.000976
Step 4000/7500, Loss: 0.0659, LR: 0.000962
Step 4100/7500, Loss: 0.0462, LR: 0.000946
Step 4200/7500, Loss: 0.0614, LR: 0.000926
Step 4300/7500, Loss: 0.0405, LR: 0.000905
Step 4400/7500, Loss: 0.0609, LR: 0.000880
Step 4500/7500, Loss: 0.0564, LR: 0.000854
Step 4600/7500, Loss: 0.0797, LR: 0.000825
Step 4700/7500, Loss: 0.0613, LR: 0.000794
Step 4800/7500, Loss: 0.0781, LR: 0.000761
Step 4900/7500, Loss: 0.0599, LR: 0.000727
Step 5000/7500, Loss: 0.0659, LR: 0.000692
Step 5100/7500, Loss: 0.0473, LR: 0.000655
Step 5200/7500, Loss: 0.0432, LR: 0.000617
Step 5300/7500, Loss: 0.0376, LR: 0.000579
Step 5400/7500, Loss: 0.0471, LR: 0.000540
Step 5500/7500, Loss: 0.0319, LR: 0.000501
Step 5600/7500, Loss: 0.0519, LR: 0.000461
Step 5700/7500, Loss: 0.0536, LR: 0.000422
Step 5800/7500, Loss: 0.0538, LR: 0.000384
Step 5900/7500, Loss: 0.0318, LR: 0.000346
Step 6000/7500, Loss: 0.0347, LR: 0.000309
Step 6100/7500, Loss: 0.0299, LR: 0.000274
Step 6200/7500, Loss: 0.0207, LR: 0.000240
Step 6300/7500, Loss: 0.0220, LR: 0.000207
Step 6400/7500, Loss: 0.0365, LR: 0.000176
Step 6500/7500, Loss: 0.0268, LR: 0.000147
Step 6600/7500, Loss: 0.0321, LR: 0.000121
Step 6700/7500, Loss: 0.0150, LR: 0.000096
Step 6800/7500, Loss: 0.0311, LR: 0.000075
Step 6900/7500, Loss: 0.0293, LR: 0.000055
Step 7000/7500, Loss: 0.0312, LR: 0.000039
Step 7100/7500, Loss: 0.0318, LR: 0.000025
Step 7200/7500, Loss: 0.0266, LR: 0.000015
Step 7300/7500, Loss: 0.0326, LR: 0.000007
Step 7400/7500, Loss: 0.0300, LR: 0.000003
Step 7500/7500, Loss: 0.0175, LR: 0.001000
Training time: 26.13s (3.48ms per step)
Evaluating model...
Hard Accuracy: 0.9528
Soft Accuracy: 0.998
Evaluation time: 0.22s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9528
Axiom (neutrality) Satisfaction Rate: 0.9046
Axiom (condorcet) Satisfaction Rate: 0.8118
Axiom (pareto) Satisfaction Rate: 0.04
Measuring inference time...
Average inference time (single sample): 0.2726ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.9528
  Soft Accuracy: 0.9980
  Training Time: 26.13s
  Inference Time: 0.2726ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 500,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 178.76s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 7,500 steps (2500 steps/epoch)
Step 100/7500, Loss: 0.3110, LR: 0.000905
Step 200/7500, Loss: 0.2332, LR: 0.000655
Step 300/7500, Loss: 0.1693, LR: 0.000346
Step 400/7500, Loss: 0.1407, LR: 0.000096
Step 500/7500, Loss: 0.1857, LR: 0.001000
Step 600/7500, Loss: 0.1607, LR: 0.000976
Step 700/7500, Loss: 0.1532, LR: 0.000905
Step 800/7500, Loss: 0.1504, LR: 0.000794
Step 900/7500, Loss: 0.1166, LR: 0.000655
Step 1000/7500, Loss: 0.0888, LR: 0.000501
Step 1100/7500, Loss: 0.1152, LR: 0.000346
Step 1200/7500, Loss: 0.1196, LR: 0.000207
Step 1300/7500, Loss: 0.1096, LR: 0.000096
Step 1400/7500, Loss: 0.1104, LR: 0.000025
Step 1500/7500, Loss: 0.0848, LR: 0.001000
Step 1600/7500, Loss: 0.1391, LR: 0.000994
Step 1700/7500, Loss: 0.1205, LR: 0.000976
Step 1800/7500, Loss: 0.1351, LR: 0.000946
Step 1900/7500, Loss: 0.1017, LR: 0.000905
Step 2000/7500, Loss: 0.1154, LR: 0.000854
Step 2100/7500, Loss: 0.1028, LR: 0.000794
Step 2200/7500, Loss: 0.1184, LR: 0.000727
Step 2300/7500, Loss: 0.1027, LR: 0.000655
Step 2400/7500, Loss: 0.1029, LR: 0.000579
Step 2500/7500, Loss: 0.0772, LR: 0.000501
Step 2600/7500, Loss: 0.0958, LR: 0.000422
Step 2700/7500, Loss: 0.1289, LR: 0.000346
Step 2800/7500, Loss: 0.0961, LR: 0.000274
Step 2900/7500, Loss: 0.0838, LR: 0.000207
Step 3000/7500, Loss: 0.0821, LR: 0.000147
Step 3100/7500, Loss: 0.0996, LR: 0.000096
Step 3200/7500, Loss: 0.0898, LR: 0.000055
Step 3300/7500, Loss: 0.0915, LR: 0.000025
Step 3400/7500, Loss: 0.0967, LR: 0.000007
Step 3500/7500, Loss: 0.1055, LR: 0.001000
Step 3600/7500, Loss: 0.1217, LR: 0.000998
Step 3700/7500, Loss: 0.1271, LR: 0.000994
Step 3800/7500, Loss: 0.1007, LR: 0.000986
Step 3900/7500, Loss: 0.1266, LR: 0.000976
Step 4000/7500, Loss: 0.1234, LR: 0.000962
Step 4100/7500, Loss: 0.0999, LR: 0.000946
Step 4200/7500, Loss: 0.0736, LR: 0.000926
Step 4300/7500, Loss: 0.1061, LR: 0.000905
Step 4400/7500, Loss: 0.0954, LR: 0.000880
Step 4500/7500, Loss: 0.0887, LR: 0.000854
Step 4600/7500, Loss: 0.1007, LR: 0.000825
Step 4700/7500, Loss: 0.1008, LR: 0.000794
Step 4800/7500, Loss: 0.1069, LR: 0.000761
Step 4900/7500, Loss: 0.1315, LR: 0.000727
Step 5000/7500, Loss: 0.0866, LR: 0.000692
Step 5100/7500, Loss: 0.1037, LR: 0.000655
Step 5200/7500, Loss: 0.1086, LR: 0.000617
Step 5300/7500, Loss: 0.0834, LR: 0.000579
Step 5400/7500, Loss: 0.0956, LR: 0.000540
Step 5500/7500, Loss: 0.1052, LR: 0.000501
Step 5600/7500, Loss: 0.1099, LR: 0.000461
Step 5700/7500, Loss: 0.0885, LR: 0.000422
Step 5800/7500, Loss: 0.0804, LR: 0.000384
Step 5900/7500, Loss: 0.0854, LR: 0.000346
Step 6000/7500, Loss: 0.0884, LR: 0.000309
Step 6100/7500, Loss: 0.1138, LR: 0.000274
Step 6200/7500, Loss: 0.1051, LR: 0.000240
Step 6300/7500, Loss: 0.0689, LR: 0.000207
Step 6400/7500, Loss: 0.0724, LR: 0.000176
Step 6500/7500, Loss: 0.1015, LR: 0.000147
Step 6600/7500, Loss: 0.0892, LR: 0.000121
Step 6700/7500, Loss: 0.1054, LR: 0.000096
Step 6800/7500, Loss: 0.0802, LR: 0.000075
Step 6900/7500, Loss: 0.0640, LR: 0.000055
Step 7000/7500, Loss: 0.0795, LR: 0.000039
Step 7100/7500, Loss: 0.0796, LR: 0.000025
Step 7200/7500, Loss: 0.0988, LR: 0.000015
Step 7300/7500, Loss: 0.0879, LR: 0.000007
Step 7400/7500, Loss: 0.0773, LR: 0.000003
Step 7500/7500, Loss: 0.0756, LR: 0.001000
Training time: 27.00s (3.60ms per step)
Evaluating model...
Hard Accuracy: 0.8498
Soft Accuracy: 0.9706
Evaluation time: 0.23s
Checking axiom satisfaction...
4249
Axiom (anonymity) Satisfaction Rate: 0.8498
4136
Axiom (neutrality) Satisfaction Rate: 0.8272
4062
Axiom (condorcet) Satisfaction Rate: 0.8124
242
Axiom (pareto) Satisfaction Rate: 0.0484
Measuring inference time...
Average inference time (single sample): 0.2335ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8498
  Soft Accuracy: 0.9706
  Training Time: 27.00s
  Inference Time: 0.2335ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

--------------------------------------------------------------------------------
Dataset size: 1,000,000 samples
--------------------------------------------------------------------------------

============================================================
Benchmarking PAIRWISE encoding with COPELAND
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise encoding...
Data creation time: 799.11s
Creating test data with pairwise encoding...
Input size: 10
Creating MLP model with pairwise encoding...
Model moved to cuda
Number of parameters: 236,293
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.1989, LR: 0.000905
Step 200/15000, Loss: 0.1182, LR: 0.000655
Step 300/15000, Loss: 0.1118, LR: 0.000346
Step 400/15000, Loss: 0.0991, LR: 0.000096
Step 500/15000, Loss: 0.1202, LR: 0.001000
Step 600/15000, Loss: 0.1106, LR: 0.000976
Step 700/15000, Loss: 0.0983, LR: 0.000905
Step 800/15000, Loss: 0.0996, LR: 0.000794
Step 900/15000, Loss: 0.0833, LR: 0.000655
Step 1000/15000, Loss: 0.0864, LR: 0.000501
Step 1100/15000, Loss: 0.0567, LR: 0.000346
Step 1200/15000, Loss: 0.0961, LR: 0.000207
Step 1300/15000, Loss: 0.1004, LR: 0.000096
Step 1400/15000, Loss: 0.0743, LR: 0.000025
Step 1500/15000, Loss: 0.0482, LR: 0.001000
Step 1600/15000, Loss: 0.0888, LR: 0.000994
Step 1700/15000, Loss: 0.1009, LR: 0.000976
Step 1800/15000, Loss: 0.0643, LR: 0.000946
Step 1900/15000, Loss: 0.0838, LR: 0.000905
Step 2000/15000, Loss: 0.0450, LR: 0.000854
Step 2100/15000, Loss: 0.0519, LR: 0.000794
Step 2200/15000, Loss: 0.0521, LR: 0.000727
Step 2300/15000, Loss: 0.0767, LR: 0.000655
Step 2400/15000, Loss: 0.0720, LR: 0.000579
Step 2500/15000, Loss: 0.0518, LR: 0.000501
Step 2600/15000, Loss: 0.0596, LR: 0.000422
Step 2700/15000, Loss: 0.0513, LR: 0.000346
Step 2800/15000, Loss: 0.0452, LR: 0.000274
Step 2900/15000, Loss: 0.0344, LR: 0.000207
Step 3000/15000, Loss: 0.0500, LR: 0.000147
Step 3100/15000, Loss: 0.0571, LR: 0.000096
Step 3200/15000, Loss: 0.0415, LR: 0.000055
Step 3300/15000, Loss: 0.0577, LR: 0.000025
Step 3400/15000, Loss: 0.0737, LR: 0.000007
Step 3500/15000, Loss: 0.0475, LR: 0.001000
Step 3600/15000, Loss: 0.0564, LR: 0.000998
Step 3700/15000, Loss: 0.0478, LR: 0.000994
Step 3800/15000, Loss: 0.0833, LR: 0.000986
Step 3900/15000, Loss: 0.0367, LR: 0.000976
Step 4000/15000, Loss: 0.0533, LR: 0.000962
Step 4100/15000, Loss: 0.0382, LR: 0.000946
Step 4200/15000, Loss: 0.0463, LR: 0.000926
Step 4300/15000, Loss: 0.0381, LR: 0.000905
Step 4400/15000, Loss: 0.0413, LR: 0.000880
Step 4500/15000, Loss: 0.0427, LR: 0.000854
Step 4600/15000, Loss: 0.0351, LR: 0.000825
Step 4700/15000, Loss: 0.0383, LR: 0.000794
Step 4800/15000, Loss: 0.0333, LR: 0.000761
Step 4900/15000, Loss: 0.0289, LR: 0.000727
Step 5000/15000, Loss: 0.0297, LR: 0.000692
Step 5100/15000, Loss: 0.0414, LR: 0.000655
Step 5200/15000, Loss: 0.0371, LR: 0.000617
Step 5300/15000, Loss: 0.0346, LR: 0.000579
Step 5400/15000, Loss: 0.0496, LR: 0.000540
Step 5500/15000, Loss: 0.0234, LR: 0.000501
Step 5600/15000, Loss: 0.0664, LR: 0.000461
Step 5700/15000, Loss: 0.0282, LR: 0.000422
Step 5800/15000, Loss: 0.0133, LR: 0.000384
Step 5900/15000, Loss: 0.0320, LR: 0.000346
Step 6000/15000, Loss: 0.0279, LR: 0.000309
Step 6100/15000, Loss: 0.0195, LR: 0.000274
Step 6200/15000, Loss: 0.0326, LR: 0.000240
Step 6300/15000, Loss: 0.0204, LR: 0.000207
Step 6400/15000, Loss: 0.0327, LR: 0.000176
Step 6500/15000, Loss: 0.0175, LR: 0.000147
Step 6600/15000, Loss: 0.0257, LR: 0.000121
Step 6700/15000, Loss: 0.0374, LR: 0.000096
Step 6800/15000, Loss: 0.0192, LR: 0.000075
Step 6900/15000, Loss: 0.0169, LR: 0.000055
Step 7000/15000, Loss: 0.0218, LR: 0.000039
Step 7100/15000, Loss: 0.0216, LR: 0.000025
Step 7200/15000, Loss: 0.0196, LR: 0.000015
Step 7300/15000, Loss: 0.0248, LR: 0.000007
Step 7400/15000, Loss: 0.0128, LR: 0.000003
Step 7500/15000, Loss: 0.0209, LR: 0.001000
Step 7600/15000, Loss: 0.0538, LR: 0.001000
Step 7700/15000, Loss: 0.0424, LR: 0.000998
Step 7800/15000, Loss: 0.0303, LR: 0.000997
Step 7900/15000, Loss: 0.0181, LR: 0.000994
Step 8000/15000, Loss: 0.0377, LR: 0.000990
Step 8100/15000, Loss: 0.0272, LR: 0.000986
Step 8200/15000, Loss: 0.0325, LR: 0.000981
Step 8300/15000, Loss: 0.0434, LR: 0.000976
Step 8400/15000, Loss: 0.0328, LR: 0.000969
Step 8500/15000, Loss: 0.0163, LR: 0.000962
Step 8600/15000, Loss: 0.0230, LR: 0.000954
Step 8700/15000, Loss: 0.0323, LR: 0.000946
Step 8800/15000, Loss: 0.0261, LR: 0.000936
Step 8900/15000, Loss: 0.0232, LR: 0.000926
Step 9000/15000, Loss: 0.0298, LR: 0.000916
Step 9100/15000, Loss: 0.0154, LR: 0.000905
Step 9200/15000, Loss: 0.0430, LR: 0.000893
Step 9300/15000, Loss: 0.0421, LR: 0.000880
Step 9400/15000, Loss: 0.0184, LR: 0.000867
Step 9500/15000, Loss: 0.0423, LR: 0.000854
Step 9600/15000, Loss: 0.0328, LR: 0.000840
Step 9700/15000, Loss: 0.0299, LR: 0.000825
Step 9800/15000, Loss: 0.0261, LR: 0.000810
Step 9900/15000, Loss: 0.0185, LR: 0.000794
Step 10000/15000, Loss: 0.0295, LR: 0.000778
Step 10100/15000, Loss: 0.0139, LR: 0.000761
Step 10200/15000, Loss: 0.0190, LR: 0.000745
Step 10300/15000, Loss: 0.0341, LR: 0.000727
Step 10400/15000, Loss: 0.0370, LR: 0.000710
Step 10500/15000, Loss: 0.0163, LR: 0.000692
Step 10600/15000, Loss: 0.0152, LR: 0.000673
Step 10700/15000, Loss: 0.0209, LR: 0.000655
Step 10800/15000, Loss: 0.0140, LR: 0.000636
Step 10900/15000, Loss: 0.0374, LR: 0.000617
Step 11000/15000, Loss: 0.0337, LR: 0.000598
Step 11100/15000, Loss: 0.0161, LR: 0.000579
Step 11200/15000, Loss: 0.0353, LR: 0.000559
Step 11300/15000, Loss: 0.0106, LR: 0.000540
Step 11400/15000, Loss: 0.0202, LR: 0.000520
Step 11500/15000, Loss: 0.0560, LR: 0.000501
Step 11600/15000, Loss: 0.0243, LR: 0.000481
Step 11700/15000, Loss: 0.0145, LR: 0.000461
Step 11800/15000, Loss: 0.0292, LR: 0.000442
Step 11900/15000, Loss: 0.0261, LR: 0.000422
Step 12000/15000, Loss: 0.0137, LR: 0.000403
Step 12100/15000, Loss: 0.0120, LR: 0.000384
Step 12200/15000, Loss: 0.0222, LR: 0.000365
Step 12300/15000, Loss: 0.0235, LR: 0.000346
Step 12400/15000, Loss: 0.0167, LR: 0.000328
Step 12500/15000, Loss: 0.0228, LR: 0.000309
Step 12600/15000, Loss: 0.0185, LR: 0.000291
Step 12700/15000, Loss: 0.0328, LR: 0.000274
Step 12800/15000, Loss: 0.0218, LR: 0.000256
Step 12900/15000, Loss: 0.0363, LR: 0.000240
Step 13000/15000, Loss: 0.0186, LR: 0.000223
Step 13100/15000, Loss: 0.0152, LR: 0.000207
Step 13200/15000, Loss: 0.0436, LR: 0.000191
Step 13300/15000, Loss: 0.0111, LR: 0.000176
Step 13400/15000, Loss: 0.0291, LR: 0.000161
Step 13500/15000, Loss: 0.0126, LR: 0.000147
Step 13600/15000, Loss: 0.0472, LR: 0.000134
Step 13700/15000, Loss: 0.0208, LR: 0.000121
Step 13800/15000, Loss: 0.0116, LR: 0.000108
Step 13900/15000, Loss: 0.0100, LR: 0.000096
Step 14000/15000, Loss: 0.0106, LR: 0.000085
Step 14100/15000, Loss: 0.0132, LR: 0.000075
Step 14200/15000, Loss: 0.0185, LR: 0.000065
Step 14300/15000, Loss: 0.0139, LR: 0.000055
Step 14400/15000, Loss: 0.0228, LR: 0.000047
Step 14500/15000, Loss: 0.0132, LR: 0.000039
Step 14600/15000, Loss: 0.0202, LR: 0.000032
Step 14700/15000, Loss: 0.0236, LR: 0.000025
Step 14800/15000, Loss: 0.0196, LR: 0.000020
Step 14900/15000, Loss: 0.0157, LR: 0.000015
Step 15000/15000, Loss: 0.0063, LR: 0.000011
Training time: 39.45s (2.63ms per step)
Evaluating model...
Hard Accuracy: 0.9756
Soft Accuracy: 1.0
Evaluation time: 0.22s
Checking axiom satisfaction...
4878
Axiom (anonymity) Satisfaction Rate: 0.9756
4601
Axiom (neutrality) Satisfaction Rate: 0.9202
4028
Axiom (condorcet) Satisfaction Rate: 0.8056
249
Axiom (pareto) Satisfaction Rate: 0.0498
Measuring inference time...
Average inference time (single sample): 0.2357ms

PAIRWISE Results (COPELAND):
  Hard Accuracy: 0.9756
  Soft Accuracy: 1.0000
  Training Time: 39.45s
  Inference Time: 0.2357ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking PAIRWISE_PER_VOTER encoding with COPELAND
Dataset size: 1,000,000 samples
============================================================

Creating training data with pairwise_per_voter encoding...
Data creation time: 428.64s
Creating test data with pairwise_per_voter encoding...
Input size: 550
Creating MLP model with pairwise_per_voter encoding...
Model moved to cuda
Number of parameters: 241,669
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.1419, LR: 0.000905
Step 200/15000, Loss: 0.1344, LR: 0.000655
Step 300/15000, Loss: 0.1146, LR: 0.000346
Step 400/15000, Loss: 0.1287, LR: 0.000096
Step 500/15000, Loss: 0.1106, LR: 0.001000
Step 600/15000, Loss: 0.1266, LR: 0.000976
Step 700/15000, Loss: 0.1374, LR: 0.000905
Step 800/15000, Loss: 0.1253, LR: 0.000794
Step 900/15000, Loss: 0.1165, LR: 0.000655
Step 1000/15000, Loss: 0.1278, LR: 0.000501
Step 1100/15000, Loss: 0.1054, LR: 0.000346
Step 1200/15000, Loss: 0.0915, LR: 0.000207
Step 1300/15000, Loss: 0.0858, LR: 0.000096
Step 1400/15000, Loss: 0.0862, LR: 0.000025
Step 1500/15000, Loss: 0.0934, LR: 0.001000
Step 1600/15000, Loss: 0.1026, LR: 0.000994
Step 1700/15000, Loss: 0.0975, LR: 0.000976
Step 1800/15000, Loss: 0.1025, LR: 0.000946
Step 1900/15000, Loss: 0.0692, LR: 0.000905
Step 2000/15000, Loss: 0.1006, LR: 0.000854
Step 2100/15000, Loss: 0.0946, LR: 0.000794
Step 2200/15000, Loss: 0.0825, LR: 0.000727
Step 2300/15000, Loss: 0.0884, LR: 0.000655
Step 2400/15000, Loss: 0.0832, LR: 0.000579
Step 2500/15000, Loss: 0.0886, LR: 0.000501
Step 2600/15000, Loss: 0.0767, LR: 0.000422
Step 2700/15000, Loss: 0.0616, LR: 0.000346
Step 2800/15000, Loss: 0.0654, LR: 0.000274
Step 2900/15000, Loss: 0.0723, LR: 0.000207
Step 3000/15000, Loss: 0.0591, LR: 0.000147
Step 3100/15000, Loss: 0.0441, LR: 0.000096
Step 3200/15000, Loss: 0.0460, LR: 0.000055
Step 3300/15000, Loss: 0.0704, LR: 0.000025
Step 3400/15000, Loss: 0.0579, LR: 0.000007
Step 3500/15000, Loss: 0.0479, LR: 0.001000
Step 3600/15000, Loss: 0.0698, LR: 0.000998
Step 3700/15000, Loss: 0.0712, LR: 0.000994
Step 3800/15000, Loss: 0.0873, LR: 0.000986
Step 3900/15000, Loss: 0.0614, LR: 0.000976
Step 4000/15000, Loss: 0.0825, LR: 0.000962
Step 4100/15000, Loss: 0.0551, LR: 0.000946
Step 4200/15000, Loss: 0.0711, LR: 0.000926
Step 4300/15000, Loss: 0.0399, LR: 0.000905
Step 4400/15000, Loss: 0.0599, LR: 0.000880
Step 4500/15000, Loss: 0.0732, LR: 0.000854
Step 4600/15000, Loss: 0.0521, LR: 0.000825
Step 4700/15000, Loss: 0.0448, LR: 0.000794
Step 4800/15000, Loss: 0.0644, LR: 0.000761
Step 4900/15000, Loss: 0.0607, LR: 0.000727
Step 5000/15000, Loss: 0.0537, LR: 0.000692
Step 5100/15000, Loss: 0.0289, LR: 0.000655
Step 5200/15000, Loss: 0.0420, LR: 0.000617
Step 5300/15000, Loss: 0.0479, LR: 0.000579
Step 5400/15000, Loss: 0.0447, LR: 0.000540
Step 5500/15000, Loss: 0.0690, LR: 0.000501
Step 5600/15000, Loss: 0.0313, LR: 0.000461
Step 5700/15000, Loss: 0.0267, LR: 0.000422
Step 5800/15000, Loss: 0.0333, LR: 0.000384
Step 5900/15000, Loss: 0.0386, LR: 0.000346
Step 6000/15000, Loss: 0.0327, LR: 0.000309
Step 6100/15000, Loss: 0.0597, LR: 0.000274
Step 6200/15000, Loss: 0.0296, LR: 0.000240
Step 6300/15000, Loss: 0.0316, LR: 0.000207
Step 6400/15000, Loss: 0.0344, LR: 0.000176
Step 6500/15000, Loss: 0.0363, LR: 0.000147
Step 6600/15000, Loss: 0.0247, LR: 0.000121
Step 6700/15000, Loss: 0.0383, LR: 0.000096
Step 6800/15000, Loss: 0.0247, LR: 0.000075
Step 6900/15000, Loss: 0.0298, LR: 0.000055
Step 7000/15000, Loss: 0.0164, LR: 0.000039
Step 7100/15000, Loss: 0.0328, LR: 0.000025
Step 7200/15000, Loss: 0.0202, LR: 0.000015
Step 7300/15000, Loss: 0.0328, LR: 0.000007
Step 7400/15000, Loss: 0.0279, LR: 0.000003
Step 7500/15000, Loss: 0.0179, LR: 0.001000
Step 7600/15000, Loss: 0.0707, LR: 0.001000
Step 7700/15000, Loss: 0.0545, LR: 0.000998
Step 7800/15000, Loss: 0.0601, LR: 0.000997
Step 7900/15000, Loss: 0.0638, LR: 0.000994
Step 8000/15000, Loss: 0.0553, LR: 0.000990
Step 8100/15000, Loss: 0.0553, LR: 0.000986
Step 8200/15000, Loss: 0.0429, LR: 0.000981
Step 8300/15000, Loss: 0.0496, LR: 0.000976
Step 8400/15000, Loss: 0.0590, LR: 0.000969
Step 8500/15000, Loss: 0.0509, LR: 0.000962
Step 8600/15000, Loss: 0.0401, LR: 0.000954
Step 8700/15000, Loss: 0.0442, LR: 0.000946
Step 8800/15000, Loss: 0.0251, LR: 0.000936
Step 8900/15000, Loss: 0.0359, LR: 0.000926
Step 9000/15000, Loss: 0.0257, LR: 0.000916
Step 9100/15000, Loss: 0.0362, LR: 0.000905
Step 9200/15000, Loss: 0.0269, LR: 0.000893
Step 9300/15000, Loss: 0.0469, LR: 0.000880
Step 9400/15000, Loss: 0.0343, LR: 0.000867
Step 9500/15000, Loss: 0.0416, LR: 0.000854
Step 9600/15000, Loss: 0.0396, LR: 0.000840
Step 9700/15000, Loss: 0.0322, LR: 0.000825
Step 9800/15000, Loss: 0.0323, LR: 0.000810
Step 9900/15000, Loss: 0.0444, LR: 0.000794
Step 10000/15000, Loss: 0.0275, LR: 0.000778
Step 10100/15000, Loss: 0.0262, LR: 0.000761
Step 10200/15000, Loss: 0.0265, LR: 0.000745
Step 10300/15000, Loss: 0.0314, LR: 0.000727
Step 10400/15000, Loss: 0.0338, LR: 0.000710
Step 10500/15000, Loss: 0.0269, LR: 0.000692
Step 10600/15000, Loss: 0.0344, LR: 0.000673
Step 10700/15000, Loss: 0.0295, LR: 0.000655
Step 10800/15000, Loss: 0.0325, LR: 0.000636
Step 10900/15000, Loss: 0.0275, LR: 0.000617
Step 11000/15000, Loss: 0.0188, LR: 0.000598
Step 11100/15000, Loss: 0.0235, LR: 0.000579
Step 11200/15000, Loss: 0.0204, LR: 0.000559
Step 11300/15000, Loss: 0.0128, LR: 0.000540
Step 11400/15000, Loss: 0.0274, LR: 0.000520
Step 11500/15000, Loss: 0.0189, LR: 0.000501
Step 11600/15000, Loss: 0.0297, LR: 0.000481
Step 11700/15000, Loss: 0.0148, LR: 0.000461
Step 11800/15000, Loss: 0.0236, LR: 0.000442
Step 11900/15000, Loss: 0.0251, LR: 0.000422
Step 12000/15000, Loss: 0.0173, LR: 0.000403
Step 12100/15000, Loss: 0.0212, LR: 0.000384
Step 12200/15000, Loss: 0.0322, LR: 0.000365
Step 12300/15000, Loss: 0.0256, LR: 0.000346
Step 12400/15000, Loss: 0.0258, LR: 0.000328
Step 12500/15000, Loss: 0.0210, LR: 0.000309
Step 12600/15000, Loss: 0.0134, LR: 0.000291
Step 12700/15000, Loss: 0.0110, LR: 0.000274
Step 12800/15000, Loss: 0.0161, LR: 0.000256
Step 12900/15000, Loss: 0.0114, LR: 0.000240
Step 13000/15000, Loss: 0.0062, LR: 0.000223
Step 13100/15000, Loss: 0.0075, LR: 0.000207
Step 13200/15000, Loss: 0.0194, LR: 0.000191
Step 13300/15000, Loss: 0.0130, LR: 0.000176
Step 13400/15000, Loss: 0.0079, LR: 0.000161
Step 13500/15000, Loss: 0.0146, LR: 0.000147
Step 13600/15000, Loss: 0.0127, LR: 0.000134
Step 13700/15000, Loss: 0.0162, LR: 0.000121
Step 13800/15000, Loss: 0.0163, LR: 0.000108
Step 13900/15000, Loss: 0.0154, LR: 0.000096
Step 14000/15000, Loss: 0.0125, LR: 0.000085
Step 14100/15000, Loss: 0.0116, LR: 0.000075
Step 14200/15000, Loss: 0.0078, LR: 0.000065
Step 14300/15000, Loss: 0.0106, LR: 0.000055
Step 14400/15000, Loss: 0.0074, LR: 0.000047
Step 14500/15000, Loss: 0.0138, LR: 0.000039
Step 14600/15000, Loss: 0.0096, LR: 0.000032
Step 14700/15000, Loss: 0.0173, LR: 0.000025
Step 14800/15000, Loss: 0.0122, LR: 0.000020
Step 14900/15000, Loss: 0.0109, LR: 0.000015
Step 15000/15000, Loss: 0.0081, LR: 0.000011
Training time: 42.98s (2.87ms per step)
Evaluating model...
Hard Accuracy: 0.9788
Soft Accuracy: 0.9992
Evaluation time: 0.23s
Checking axiom satisfaction...
Axiom (anonymity) Satisfaction Rate: 0.9788
Axiom (neutrality) Satisfaction Rate: 0.9234
Axiom (condorcet) Satisfaction Rate: 0.8092
Axiom (pareto) Satisfaction Rate: 0.0472
Measuring inference time...
Average inference time (single sample): 0.2766ms

PAIRWISE_PER_VOTER Results (COPELAND):
  Hard Accuracy: 0.9788
  Soft Accuracy: 0.9992
  Training Time: 42.98s
  Inference Time: 0.2766ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

============================================================
Benchmarking ONEHOT encoding with COPELAND
Dataset size: 1,000,000 samples
============================================================

Creating training data with onehot encoding...
Data creation time: 358.68s
Creating test data with onehot encoding...
Input size: 1375
Creating MLP model with onehot encoding...
Model moved to cuda
Number of parameters: 226,309
Training for 3 epochs = 15,000 steps (5000 steps/epoch)
Step 100/15000, Loss: 0.2487, LR: 0.000905
Step 200/15000, Loss: 0.2029, LR: 0.000655
Step 300/15000, Loss: 0.1649, LR: 0.000346
Step 400/15000, Loss: 0.1576, LR: 0.000096
Step 500/15000, Loss: 0.1354, LR: 0.001000
Step 600/15000, Loss: 0.1285, LR: 0.000976
Step 700/15000, Loss: 0.1306, LR: 0.000905
Step 800/15000, Loss: 0.1552, LR: 0.000794
Step 900/15000, Loss: 0.1473, LR: 0.000655
Step 1000/15000, Loss: 0.1296, LR: 0.000501
Step 1100/15000, Loss: 0.0864, LR: 0.000346
Step 1200/15000, Loss: 0.0971, LR: 0.000207
Step 1300/15000, Loss: 0.0977, LR: 0.000096
Step 1400/15000, Loss: 0.0971, LR: 0.000025
Step 1500/15000, Loss: 0.0969, LR: 0.001000
Step 1600/15000, Loss: 0.1252, LR: 0.000994
Step 1700/15000, Loss: 0.1056, LR: 0.000976
Step 1800/15000, Loss: 0.1309, LR: 0.000946
Step 1900/15000, Loss: 0.1309, LR: 0.000905
Step 2000/15000, Loss: 0.1004, LR: 0.000854
Step 2100/15000, Loss: 0.0910, LR: 0.000794
Step 2200/15000, Loss: 0.1048, LR: 0.000727
Step 2300/15000, Loss: 0.1196, LR: 0.000655
Step 2400/15000, Loss: 0.0974, LR: 0.000579
Step 2500/15000, Loss: 0.1142, LR: 0.000501
Step 2600/15000, Loss: 0.0967, LR: 0.000422
Step 2700/15000, Loss: 0.1000, LR: 0.000346
Step 2800/15000, Loss: 0.1074, LR: 0.000274
Step 2900/15000, Loss: 0.0781, LR: 0.000207
Step 3000/15000, Loss: 0.0784, LR: 0.000147
Step 3100/15000, Loss: 0.0956, LR: 0.000096
Step 3200/15000, Loss: 0.1019, LR: 0.000055
Step 3300/15000, Loss: 0.0803, LR: 0.000025
Step 3400/15000, Loss: 0.1006, LR: 0.000007
Step 3500/15000, Loss: 0.1113, LR: 0.001000
Step 3600/15000, Loss: 0.0886, LR: 0.000998
Step 3700/15000, Loss: 0.0995, LR: 0.000994
Step 3800/15000, Loss: 0.1034, LR: 0.000986
Step 3900/15000, Loss: 0.0907, LR: 0.000976
Step 4000/15000, Loss: 0.1133, LR: 0.000962
Step 4100/15000, Loss: 0.0716, LR: 0.000946
Step 4200/15000, Loss: 0.1079, LR: 0.000926
Step 4300/15000, Loss: 0.0923, LR: 0.000905
Step 4400/15000, Loss: 0.0959, LR: 0.000880
Step 4500/15000, Loss: 0.1249, LR: 0.000854
Step 4600/15000, Loss: 0.0832, LR: 0.000825
Step 4700/15000, Loss: 0.0885, LR: 0.000794
Step 4800/15000, Loss: 0.0977, LR: 0.000761
Step 4900/15000, Loss: 0.0944, LR: 0.000727
Step 5000/15000, Loss: 0.0982, LR: 0.000692
Step 5100/15000, Loss: 0.1046, LR: 0.000655
Step 5200/15000, Loss: 0.1127, LR: 0.000617
Step 5300/15000, Loss: 0.1127, LR: 0.000579
Step 5400/15000, Loss: 0.1263, LR: 0.000540
Step 5500/15000, Loss: 0.0929, LR: 0.000501
Step 5600/15000, Loss: 0.0965, LR: 0.000461
Step 5700/15000, Loss: 0.0921, LR: 0.000422
Step 5800/15000, Loss: 0.1013, LR: 0.000384
Step 5900/15000, Loss: 0.0945, LR: 0.000346
Step 6000/15000, Loss: 0.0860, LR: 0.000309
Step 6100/15000, Loss: 0.1234, LR: 0.000274
Step 6200/15000, Loss: 0.0846, LR: 0.000240
Step 6300/15000, Loss: 0.1021, LR: 0.000207
Step 6400/15000, Loss: 0.0711, LR: 0.000176
Step 6500/15000, Loss: 0.0980, LR: 0.000147
Step 6600/15000, Loss: 0.0795, LR: 0.000121
Step 6700/15000, Loss: 0.0911, LR: 0.000096
Step 6800/15000, Loss: 0.1176, LR: 0.000075
Step 6900/15000, Loss: 0.0848, LR: 0.000055
Step 7000/15000, Loss: 0.0933, LR: 0.000039
Step 7100/15000, Loss: 0.0882, LR: 0.000025
Step 7200/15000, Loss: 0.0835, LR: 0.000015
Step 7300/15000, Loss: 0.0918, LR: 0.000007
Step 7400/15000, Loss: 0.0842, LR: 0.000003
Step 7500/15000, Loss: 0.0892, LR: 0.001000
Step 7600/15000, Loss: 0.0815, LR: 0.001000
Step 7700/15000, Loss: 0.1169, LR: 0.000998
Step 7800/15000, Loss: 0.0886, LR: 0.000997
Step 7900/15000, Loss: 0.1076, LR: 0.000994
Step 8000/15000, Loss: 0.1210, LR: 0.000990
Step 8100/15000, Loss: 0.1023, LR: 0.000986
Step 8200/15000, Loss: 0.0968, LR: 0.000981
Step 8300/15000, Loss: 0.1050, LR: 0.000976
Step 8400/15000, Loss: 0.0967, LR: 0.000969
Step 8500/15000, Loss: 0.1073, LR: 0.000962
Step 8600/15000, Loss: 0.1041, LR: 0.000954
Step 8700/15000, Loss: 0.0861, LR: 0.000946
Step 8800/15000, Loss: 0.0980, LR: 0.000936
Step 8900/15000, Loss: 0.0871, LR: 0.000926
Step 9000/15000, Loss: 0.1006, LR: 0.000916
Step 9100/15000, Loss: 0.1105, LR: 0.000905
Step 9200/15000, Loss: 0.0880, LR: 0.000893
Step 9300/15000, Loss: 0.1163, LR: 0.000880
Step 9400/15000, Loss: 0.1038, LR: 0.000867
Step 9500/15000, Loss: 0.0802, LR: 0.000854
Step 9600/15000, Loss: 0.1219, LR: 0.000840
Step 9700/15000, Loss: 0.0979, LR: 0.000825
Step 9800/15000, Loss: 0.0771, LR: 0.000810
Step 9900/15000, Loss: 0.1115, LR: 0.000794
Step 10000/15000, Loss: 0.0843, LR: 0.000778
Step 10100/15000, Loss: 0.1165, LR: 0.000761
Step 10200/15000, Loss: 0.0778, LR: 0.000745
Step 10300/15000, Loss: 0.1226, LR: 0.000727
Step 10400/15000, Loss: 0.0908, LR: 0.000710
Step 10500/15000, Loss: 0.0817, LR: 0.000692
Step 10600/15000, Loss: 0.1124, LR: 0.000673
Step 10700/15000, Loss: 0.1006, LR: 0.000655
Step 10800/15000, Loss: 0.0671, LR: 0.000636
Step 10900/15000, Loss: 0.0713, LR: 0.000617
Step 11000/15000, Loss: 0.0730, LR: 0.000598
Step 11100/15000, Loss: 0.1187, LR: 0.000579
Step 11200/15000, Loss: 0.0836, LR: 0.000559
Step 11300/15000, Loss: 0.0787, LR: 0.000540
Step 11400/15000, Loss: 0.0945, LR: 0.000520
Step 11500/15000, Loss: 0.0829, LR: 0.000501
Step 11600/15000, Loss: 0.1065, LR: 0.000481
Step 11700/15000, Loss: 0.0926, LR: 0.000461
Step 11800/15000, Loss: 0.0876, LR: 0.000442
Step 11900/15000, Loss: 0.0739, LR: 0.000422
Step 12000/15000, Loss: 0.0822, LR: 0.000403
Step 12100/15000, Loss: 0.0975, LR: 0.000384
Step 12200/15000, Loss: 0.0838, LR: 0.000365
Step 12300/15000, Loss: 0.0823, LR: 0.000346
Step 12400/15000, Loss: 0.0857, LR: 0.000328
Step 12500/15000, Loss: 0.1133, LR: 0.000309
Step 12600/15000, Loss: 0.0834, LR: 0.000291
Step 12700/15000, Loss: 0.1237, LR: 0.000274
Step 12800/15000, Loss: 0.0657, LR: 0.000256
Step 12900/15000, Loss: 0.0992, LR: 0.000240
Step 13000/15000, Loss: 0.0778, LR: 0.000223
Step 13100/15000, Loss: 0.0922, LR: 0.000207
Step 13200/15000, Loss: 0.1074, LR: 0.000191
Step 13300/15000, Loss: 0.0873, LR: 0.000176
Step 13400/15000, Loss: 0.0993, LR: 0.000161
Step 13500/15000, Loss: 0.0861, LR: 0.000147
Step 13600/15000, Loss: 0.0782, LR: 0.000134
Step 13700/15000, Loss: 0.0979, LR: 0.000121
Step 13800/15000, Loss: 0.0846, LR: 0.000108
Step 13900/15000, Loss: 0.1000, LR: 0.000096
Step 14000/15000, Loss: 0.1088, LR: 0.000085
Step 14100/15000, Loss: 0.0970, LR: 0.000075
Step 14200/15000, Loss: 0.0762, LR: 0.000065
Step 14300/15000, Loss: 0.1185, LR: 0.000055
Step 14400/15000, Loss: 0.0782, LR: 0.000047
Step 14500/15000, Loss: 0.0969, LR: 0.000039
Step 14600/15000, Loss: 0.0899, LR: 0.000032
Step 14700/15000, Loss: 0.0849, LR: 0.000025
Step 14800/15000, Loss: 0.0768, LR: 0.000020
Step 14900/15000, Loss: 0.0979, LR: 0.000015
Step 15000/15000, Loss: 0.1073, LR: 0.000011
Training time: 43.68s (2.91ms per step)
Evaluating model...
Hard Accuracy: 0.8444
Soft Accuracy: 0.9714
Evaluation time: 0.22s
Checking axiom satisfaction...
4222
Axiom (anonymity) Satisfaction Rate: 0.8444
4095
Axiom (neutrality) Satisfaction Rate: 0.819
4002
Axiom (condorcet) Satisfaction Rate: 0.8004
225
Axiom (pareto) Satisfaction Rate: 0.045
Measuring inference time...
Average inference time (single sample): 0.2402ms

ONEHOT Results (COPELAND):
  Hard Accuracy: 0.8444
  Soft Accuracy: 0.9714
  Training Time: 43.68s
  Inference Time: 0.2402ms
GPU memory cleared. Allocated: 16.25MB / 45403.19MB

====================================================================================================
COMPREHENSIVE BENCHMARK SUMMARY
====================================================================================================


====================================================================================================
RESULTS FOR BORDA VOTING METHOD
====================================================================================================


====================================================================================================
Dataset Size: 1,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 5,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 15,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 50,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 150,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 500,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 1,000,000 samples
====================================================================================================

Metric                             Pairwise            Pairwise Per Voter  Onehot              
-----------------------------------------------------------------------------------------------
Input Size                         10                  550                 1375                
Number of Parameters               236,293             241,669             226,309             
Hard Accuracy                      0.9820              0.9986              0.9940              
Soft Accuracy                      1.0000              1.0000              0.9998              
Training Time (s)                  47.22               46.40               55.00               
Time per Step (ms)                 3.15                3.09                3.67                
Inference Time (ms)                0.2507              0.2916              0.2361              

Axiom Satisfaction Rates           
-----------------------------------------------------------------------------------------------
Anonymity                          0.9820              0.9986              0.9940              
Neutrality                         0.9558              0.9608              0.9552              
Condorcet                          0.7092              0.7256              0.7180              
Pareto                             0.0496              0.0526              0.0520              

Improvement vs One-hot (%)         
-----------------------------------------------------------------------------------------------
Input Size                                       +99.27              +60.000.00                
Num Parameters                                    -4.41               -6.790.00                
Hard Accuracy                                     -1.21               +0.460.00                
Soft Accuracy                                     +0.02               +0.020.00                
Training Time                                    +14.14              +15.640.00                
Inference Time                                    -6.17              -23.500.00                

====================================================================================================
RESULTS FOR PLURALITY VOTING METHOD
====================================================================================================


====================================================================================================
Dataset Size: 1,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 5,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 15,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 50,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 150,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 500,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 1,000,000 samples
====================================================================================================

Metric                             Pairwise            Pairwise Per Voter  Onehot              
-----------------------------------------------------------------------------------------------
Input Size                         10                  550                 1375                
Number of Parameters               236,293             241,669             226,309             
Hard Accuracy                      0.7310              0.8172              1.0000              
Soft Accuracy                      0.8608              0.9162              1.0000              
Training Time (s)                  44.56               48.51               51.73               
Time per Step (ms)                 2.97                3.23                3.45                
Inference Time (ms)                0.2357              0.2734              0.2366              

Axiom Satisfaction Rates           
-----------------------------------------------------------------------------------------------
Anonymity                          0.7310              0.8172              1.0000              
Neutrality                         0.7232              0.7852              0.9246              
Condorcet                          0.5472              0.5232              0.5376              
Pareto                             0.0448              0.0480              0.0520              

Improvement vs One-hot (%)         
-----------------------------------------------------------------------------------------------
Input Size                                       +99.27              +60.000.00                
Num Parameters                                    -4.41               -6.790.00                
Hard Accuracy                                    -26.90              -18.280.00                
Soft Accuracy                                    -13.92               -8.380.00                
Training Time                                    +13.85               +6.220.00                
Inference Time                                    +0.39              -15.560.00                

====================================================================================================
RESULTS FOR COPELAND VOTING METHOD
====================================================================================================


====================================================================================================
Dataset Size: 1,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 5,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 15,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 50,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 150,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 500,000 samples
====================================================================================================


====================================================================================================
Dataset Size: 1,000,000 samples
====================================================================================================

Metric                             Pairwise            Pairwise Per Voter  Onehot              
-----------------------------------------------------------------------------------------------
Input Size                         10                  550                 1375                
Number of Parameters               236,293             241,669             226,309             
Hard Accuracy                      0.9756              0.9788              0.8444              
Soft Accuracy                      1.0000              0.9992              0.9714              
Training Time (s)                  39.45               42.98               43.68               
Time per Step (ms)                 2.63                2.87                2.91                
Inference Time (ms)                0.2357              0.2766              0.2402              

Axiom Satisfaction Rates           
-----------------------------------------------------------------------------------------------
Anonymity                          0.9756              0.9788              0.8444              
Neutrality                         0.9202              0.9234              0.8190              
Condorcet                          0.8056              0.8092              0.8004              
Pareto                             0.0498              0.0472              0.0450              

Improvement vs One-hot (%)         
-----------------------------------------------------------------------------------------------
Input Size                                       +99.27              +60.000.00                
Num Parameters                                    -4.41               -6.790.00                
Hard Accuracy                                    +15.54              +15.920.00                
Soft Accuracy                                     +2.94               +2.860.00                
Training Time                                     +9.69               +1.600.00                
Inference Time                                    +1.90              -15.120.00                


====================================================================================================
OVERALL COMPARISON - HARD ACCURACY BY DATASET SIZE
====================================================================================================


BORDA Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.0000              0.3942              0.0000              
5,000               0.7994              0.7954              0.4580              
15,000              0.9340              0.8896              0.7580              
50,000              0.9574              0.9074              0.8928              
150,000             0.9728              0.9502              0.9422              
500,000             0.9776              0.9942              0.9866              
1,000,000           0.9820              0.9986              0.9940              

PLURALITY Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.4266              0.4116              0.0000              
5,000               0.6510              0.6760              0.5302              
15,000              0.7202              0.6850              0.8152              
50,000              0.7212              0.7284              0.9190              
150,000             0.7134              0.7432              0.9704              
500,000             0.7196              0.7960              0.9986              
1,000,000           0.7310              0.8172              1.0000              

COPELAND Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.0000              0.3968              0.0000              
5,000               0.7034              0.7452              0.4788              
15,000              0.8164              0.8066              0.7366              
50,000              0.8680              0.8078              0.8164              
150,000             0.9090              0.8750              0.8292              
500,000             0.9604              0.9528              0.8498              
1,000,000           0.9756              0.9788              0.8444              


====================================================================================================
OVERALL COMPARISON - SOFT ACCURACY BY DATASET SIZE
====================================================================================================


BORDA Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.0000              0.5478              0.0000              
5,000               0.8862              0.8940              0.4892              
15,000              0.9930              0.9592              0.8082              
50,000              0.9996              0.9752              0.9556              
150,000             1.0000              0.9936              0.9934              
500,000             1.0000              1.0000              0.9994              
1,000,000           1.0000              1.0000              0.9998              

PLURALITY Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.5036              0.4684              0.0000              
5,000               0.7744              0.8206              0.6096              
15,000              0.8576              0.8398              0.9224              
50,000              0.8592              0.8482              0.9920              
150,000             0.8476              0.8628              0.9992              
500,000             0.8536              0.9044              1.0000              
1,000,000           0.8608              0.9162              1.0000              

COPELAND Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.0000              0.5082              0.0000              
5,000               0.8114              0.8972              0.5458              
15,000              0.9664              0.9494              0.8506              
50,000              0.9884              0.9562              0.9340              
150,000             0.9982              0.9808              0.9610              
500,000             1.0000              0.9980              0.9706              
1,000,000           1.0000              0.9992              0.9714              


====================================================================================================
OVERALL COMPARISON - TRAINING TIME (s) BY DATASET SIZE
====================================================================================================


BORDA Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.20                0.08                0.09                
5,000               0.48                0.39                0.25                
15,000              0.67                0.85                1.03                
50,000              2.23                2.28                3.47                
150,000             6.79                7.33                7.61                
500,000             19.66               27.19               27.84               
1,000,000           47.22               46.40               55.00               

PLURALITY Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.04                0.04                0.04                
5,000               0.26                0.22                0.25                
15,000              0.57                1.63                0.70                
50,000              1.89                2.70                2.81                
150,000             6.26                8.00                10.25               
500,000             19.63               24.82               27.64               
1,000,000           44.56               48.51               51.73               

COPELAND Voting Method:
Dataset Size        Pairwise            Pairwise Per Voter  Onehot              
----------------------------------------------------------------------------------------------------
1,000               0.04                0.05                0.05                
5,000               0.19                0.25                0.27                
15,000              0.56                0.65                0.79                
50,000              1.91                2.18                4.00                
150,000             5.74                6.46                6.61                
500,000             20.32               26.13               27.00               
1,000,000           39.45               42.98               43.68               


Detailed results saved to benchmark_results_full.json

Creating summary CSV files...
Summary CSV files created: benchmark_hard_accuracy.csv, benchmark_training_time.csv
